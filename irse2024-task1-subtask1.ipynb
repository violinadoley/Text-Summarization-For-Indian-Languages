{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9612357,"sourceType":"datasetVersion","datasetId":5865462}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:48:08.309289Z","iopub.execute_input":"2024-10-13T05:48:08.310174Z","iopub.status.idle":"2024-10-13T05:48:08.314303Z","shell.execute_reply.started":"2024-10-13T05:48:08.310132Z","shell.execute_reply":"2024-10-13T05:48:08.313371Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df= pd.read_csv(\"/kaggle/input/irse-task1-dataset/Task1.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:48:10.878426Z","iopub.execute_input":"2024-10-13T05:48:10.878800Z","iopub.status.idle":"2024-10-13T05:48:10.995561Z","shell.execute_reply.started":"2024-10-13T05:48:10.878762Z","shell.execute_reply":"2024-10-13T05:48:10.994793Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:48:12.909964Z","iopub.execute_input":"2024-10-13T05:48:12.910810Z","iopub.status.idle":"2024-10-13T05:48:12.928821Z","shell.execute_reply.started":"2024-10-13T05:48:12.910769Z","shell.execute_reply":"2024-10-13T05:48:12.927925Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                            Comments  \\\n0                                       /*test 529*/   \n1                                       /*test 525*/   \n2                                           /*done*/   \n3                                       /*test 529*/   \n4                                       /*test 525*/   \n5  /*argv1 = URL\\n * argv2 = proxy\\n * argv3 = no...   \n6                                         /*unused*/   \n7                                         /*unused*/   \n8                                         /*unused*/   \n9                                         /*unused*/   \n\n                            Surrounding Code Context       Class  \n0  -10.   int res = 0;\\n-9.   CURL *curl = NULL;\\...  Not Useful  \n1  -2.     fprintf(stderr, \"Usage: lib529 [url] [...  Not Useful  \n2  -10.   multi_add_handle(m, curl);\\n-9.   for(;...  Not Useful  \n3  -10.   int res = 0;\\n-9.   CURL *curl = NULL;\\...  Not Useful  \n4  -2.     fprintf(stderr, \"Usage: lib529 [url] [...  Not Useful  \n5                                             #NAME?  Not Useful  \n6  -5. #define RUN_FOR_SECONDS 7\\n-4. static pthr...  Not Useful  \n7  -5. #define RUN_FOR_SECONDS 7\\n-4. static pthr...  Not Useful  \n8  -5. #define RUN_FOR_SECONDS 7\\n-4. static pthr...  Not Useful  \n9  -5. #define RUN_FOR_SECONDS 7\\n-4. static pthr...  Not Useful  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comments</th>\n      <th>Surrounding Code Context</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/*test 529*/</td>\n      <td>-10.   int res = 0;\\n-9.   CURL *curl = NULL;\\...</td>\n      <td>Not Useful</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/*test 525*/</td>\n      <td>-2.     fprintf(stderr, \"Usage: lib529 [url] [...</td>\n      <td>Not Useful</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/*done*/</td>\n      <td>-10.   multi_add_handle(m, curl);\\n-9.   for(;...</td>\n      <td>Not Useful</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/*test 529*/</td>\n      <td>-10.   int res = 0;\\n-9.   CURL *curl = NULL;\\...</td>\n      <td>Not Useful</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/*test 525*/</td>\n      <td>-2.     fprintf(stderr, \"Usage: lib529 [url] [...</td>\n      <td>Not Useful</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>/*argv1 = URL\\n * argv2 = proxy\\n * argv3 = no...</td>\n      <td>#NAME?</td>\n      <td>Not Useful</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>/*unused*/</td>\n      <td>-5. #define RUN_FOR_SECONDS 7\\n-4. static pthr...</td>\n      <td>Not Useful</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>/*unused*/</td>\n      <td>-5. #define RUN_FOR_SECONDS 7\\n-4. static pthr...</td>\n      <td>Not Useful</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>/*unused*/</td>\n      <td>-5. #define RUN_FOR_SECONDS 7\\n-4. static pthr...</td>\n      <td>Not Useful</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>/*unused*/</td>\n      <td>-5. #define RUN_FOR_SECONDS 7\\n-4. static pthr...</td>\n      <td>Not Useful</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"counter = 0\nfor index, row in df.iterrows():\n    if row[\"Class\"] == \"Useful\":\n        print(row)\n        counter += 1\n    if counter > 10:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-10-13T04:45:38.371902Z","iopub.execute_input":"2024-10-13T04:45:38.372260Z","iopub.status.idle":"2024-10-13T04:45:38.584156Z","shell.execute_reply.started":"2024-10-13T04:45:38.372227Z","shell.execute_reply":"2024-10-13T04:45:38.583222Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Comments                              /*get the file size of the local file*/\nSurrounding Code Context    -10. #endif\\n-9.     return TEST_ERR_USAGE;\\n-...\nClass                                                                  Useful\nName: 3670, dtype: object\nComments                                        /*can't open file, bail out*/\nSurrounding Code Context    -2.   hd = fstat(fileno(hd_src), &file_info);\\...\nClass                                                                  Useful\nName: 3671, dtype: object\nComments                                                 /*enable uploading*/\nSurrounding Code Context    -10.     fprintf(stderr, \"ERROR: cannot open f...\nClass                                                                  Useful\nName: 3672, dtype: object\nComments                                                   /*specify target*/\nSurrounding Code Context    -1.   easy_setopt(curl, CURLOPT_UPLOAD, 1L);\\n...\nClass                                                                  Useful\nName: 3673, dtype: object\nComments                                                   /*use active FTP*/\nSurrounding Code Context    -1.   easy_setopt(curl, CURLOPT_VERBOSE, 1L);\\...\nClass                                                                  Useful\nName: 3674, dtype: object\nComments                                 /*now specify which file to upload*/\nSurrounding Code Context    -1.   easy_setopt(curl, CURLOPT_FTPPORT, \"-\");...\nClass                                                                  Useful\nName: 3675, dtype: object\nComments                    /*NOTE: if you want this code to work on Windo...\nSurrounding Code Context    -1.   easy_setopt(curl, CURLOPT_READDATA, hd_s...\nClass                                                                  Useful\nName: 3676, dtype: object\nComments                    /*Set the size of the file to upload (optional...\nSurrounding Code Context    -5.   easy_setopt(curl, CURLOPT_READDATA, hd_s...\nClass                                                                  Useful\nName: 3677, dtype: object\nComments                    /*Based on Alex Fishman's bug report on Septem...\nSurrounding Code Context                                               #NAME?\nClass                                                                  Useful\nName: 3678, dtype: object\nComments                                /*a huge number of file descriptors*/\nSurrounding Code Context    -3.     num_open.rlim_max = rl.rlim_cur - SAFE...\nClass                                                                  Useful\nName: 3679, dtype: object\nComments                    /*verify that we won't overflow size_t in mall...\nSurrounding Code Context    -6.     for(nitems = i = 1; nitems <= i; i *= ...\nClass                                                                  Useful\nName: 3680, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-13T04:44:06.855391Z","iopub.execute_input":"2024-10-13T04:44:06.855760Z","iopub.status.idle":"2024-10-13T04:44:06.861900Z","shell.execute_reply.started":"2024-10-13T04:44:06.855726Z","shell.execute_reply":"2024-10-13T04:44:06.860936Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(11452, 3)"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T04:44:10.491823Z","iopub.execute_input":"2024-10-13T04:44:10.492189Z","iopub.status.idle":"2024-10-13T04:44:10.518888Z","shell.execute_reply.started":"2024-10-13T04:44:10.492153Z","shell.execute_reply":"2024-10-13T04:44:10.517982Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 11452 entries, 0 to 11451\nData columns (total 3 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   Comments                  11452 non-null  object\n 1   Surrounding Code Context  11452 non-null  object\n 2   Class                     11452 non-null  object\ndtypes: object(3)\nmemory usage: 268.5+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"print(df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-10-13T04:44:15.593404Z","iopub.execute_input":"2024-10-13T04:44:15.593787Z","iopub.status.idle":"2024-10-13T04:44:15.603470Z","shell.execute_reply.started":"2024-10-13T04:44:15.593751Z","shell.execute_reply":"2024-10-13T04:44:15.602732Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Comments                    0\nSurrounding Code Context    0\nClass                       0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T04:44:19.213461Z","iopub.execute_input":"2024-10-13T04:44:19.213836Z","iopub.status.idle":"2024-10-13T04:44:19.247941Z","shell.execute_reply.started":"2024-10-13T04:44:19.213800Z","shell.execute_reply":"2024-10-13T04:44:19.247061Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"                                                 Comments  \\\ncount                                               11452   \nunique                                               7107   \ntop     /*********************************************...   \nfreq                                                  144   \n\n       Surrounding Code Context   Class  \ncount                     11452   11452  \nunique                     7791       2  \ntop                      #NAME?  Useful  \nfreq                         72    7063  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comments</th>\n      <th>Surrounding Code Context</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>11452</td>\n      <td>11452</td>\n      <td>11452</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>7107</td>\n      <td>7791</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>/*********************************************...</td>\n      <td>#NAME?</td>\n      <td>Useful</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>144</td>\n      <td>72</td>\n      <td>7063</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(df.dtypes)","metadata":{},"execution_count":22,"outputs":[{"name":"stdout","output_type":"stream","text":"Comments                    object\n\nSurrounding Code Context    object\n\nClass                       object\n\ndtype: object\n"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndf['Class'].value_counts().plot(kind='bar')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T04:45:17.261441Z","iopub.execute_input":"2024-10-13T04:45:17.261797Z","iopub.status.idle":"2024-10-13T04:45:17.478077Z","shell.execute_reply.started":"2024-10-13T04:45:17.261765Z","shell.execute_reply":"2024-10-13T04:45:17.477148Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"from openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[\n    {\n      \"role\": \"system\",\n      \"content\": \"You will be provided with statements, and your task is to convert them to standard English.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"She no went to the market.\"\n    }\n  ],\n  temperature=0.7,\n  max_tokens=64,\n  top_p=1\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:40:56.032296Z","iopub.execute_input":"2024-10-13T05:40:56.032801Z","iopub.status.idle":"2024-10-13T05:41:08.733089Z","shell.execute_reply.started":"2024-10-13T05:40:56.032761Z","shell.execute_reply":"2024-10-13T05:41:08.731969Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# Load the CodeT5 tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Salesforce/codet5-base\", num_labels=2)  # 2 labels for useful and not useful\n\n# Set the model to evaluation mode (you can skip this during training)\nmodel.eval()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:41:28.579236Z","iopub.execute_input":"2024-10-13T05:41:28.580105Z","iopub.status.idle":"2024-10-13T05:41:42.717358Z","shell.execute_reply.started":"2024-10-13T05:41:28.580059Z","shell.execute_reply":"2024-10-13T05:41:42.716502Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea813a877f6f45b49704eb4afe25b0bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/703k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"314f328b39db42dabaa689db340cbed1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/294k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48d4ae7cb2f34513a346bfbae065831f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"455e4253842744bd91d17b95d0a29a5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/12.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9352eff5d5243cda32f8ea2d1e581b1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"644f67d4b5b945dd83f800eb2715b8a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf0b8c7ea8a04f28b078b63366c6c13f"}},"metadata":{}},{"name":"stderr","text":"Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at Salesforce/codet5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"T5ForSequenceClassification(\n  (transformer): T5Model(\n    (shared): Embedding(32100, 768)\n    (encoder): T5Stack(\n      (embed_tokens): Embedding(32100, 768)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n                (relative_attention_bias): Embedding(32, 12)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=768, out_features=3072, bias=False)\n                (wo): Linear(in_features=3072, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-11): 11 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=768, out_features=3072, bias=False)\n                (wo): Linear(in_features=3072, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (decoder): T5Stack(\n      (embed_tokens): Embedding(32100, 768)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n                (relative_attention_bias): Embedding(32, 12)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=768, out_features=3072, bias=False)\n                (wo): Linear(in_features=3072, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-11): 11 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=768, out_features=3072, bias=False)\n                (wo): Linear(in_features=3072, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (classification_head): T5ClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.0, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Example code-comment pair\ncode_snippet = \"def add(a, b): return a + b\"\ncomment = \"This function subtracts two numbers\"\n\n# Preprocess the input using the tokenizer\ninputs = tokenizer(f\"code: {code_snippet} comment: {comment}\", return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n\n# Check the tokenized input\nprint(inputs)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:43:39.504002Z","iopub.execute_input":"2024-10-13T05:43:39.504385Z","iopub.status.idle":"2024-10-13T05:43:39.517157Z","shell.execute_reply.started":"2024-10-13T05:43:39.504347Z","shell.execute_reply":"2024-10-13T05:43:39.516203Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[    1,   710,    30,  1652,   527,    12,    69,    16,   324,  4672,\n           327,   279,   397,   324,  2879,    30,  1220,   445, 10418,    87,\n          2795,  5600,     2,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Perform the forward pass to get logits (raw model output)\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Get the logits from the model output\nlogits = outputs.logits\n\n# Apply softmax to get probabilities\nprobs = torch.softmax(logits, dim=1)\n\n# Get the predicted label (0 or 1)\npredicted_label = torch.argmax(probs, dim=1).item()\n\n# Map predicted label to the corresponding class\nlabel_map = {0: \"Not Useful\", 1: \"Useful\"}\npredicted_class = label_map[predicted_label]\n\nprint(f\"Predicted Class: {predicted_class}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:43:43.843952Z","iopub.execute_input":"2024-10-13T05:43:43.844330Z","iopub.status.idle":"2024-10-13T05:43:45.754565Z","shell.execute_reply.started":"2024-10-13T05:43:43.844292Z","shell.execute_reply":"2024-10-13T05:43:45.753671Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Predicted Class: Useful\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/irse-task1-dataset/Task1.csv\")\n\n# Clean and convert the 'Class' column to binary labels (1 for Useful, 0 for Not Useful)\n# Clean and convert the 'Class' column to binary labels (1 for Useful, 0 for Not Useful)\ndf['Class'] = df['Class'].apply(lambda x: 1 if x == 'Useful' else 0)\n\n# Load the tokenizer and model (for example, CodeT5 for code-related tasks)\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Salesforce/codet5-base\", num_labels=2)  # Binary classification\n\n# Function to preprocess each example\ndef preprocess_function(examples):\n    return tokenizer(f\"code: {examples['Surrounding Code Context']} comment: {examples['Comments']}\", \n                     truncation=True, \n                     padding=\"max_length\", \n                     max_length=512)\n\n# Preprocessing the dataset\nencoded_inputs = df.apply(lambda x: preprocess_function({'Surrounding Code Context': x['Surrounding Code Context'], \n                                                         'Comments': x['Comments']}), axis=1)\n\n# Convert the tokenized inputs and labels into tensors for PyTorch\ninput_ids = torch.tensor([ex['input_ids'] for ex in encoded_inputs])\nattention_mask = torch.tensor([ex['attention_mask'] for ex in encoded_inputs])\nlabels = torch.tensor(df['Class'].values)\n\n# # Create a PyTorch dataset for training\n# train_dataset = torch.utils.data.TensorDataset(input_ids, attention_mask, labels)\n\n# # Use a data collator to handle padding\n# data_collator = DataCollatorWithPadding(tokenizer)\n\n# # Define training arguments (standard fine-tuning configuration)\n# training_args = TrainingArguments(\n#     output_dir=\"./results\",               # output directory for model checkpoints\n#     learning_rate=2e-5,                   # learning rate\n#     per_device_train_batch_size=8,        # batch size per device\n#     num_train_epochs=3,                   # number of epochs\n#     weight_decay=0.01,                    # weight decay\n#     logging_dir=\"./logs\",                 # directory for logging (optional)\n#     logging_steps=10,                     # log every 10 steps (optional)\n#     save_steps=500,                       # save checkpoints every 500 steps (optional)\n#     save_total_limit=2,                   # only keep the last 2 checkpoints (optional)\n# )\n\n# # Initialize the Trainer with the model, arguments, and dataset\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     data_collator=data_collator,\n# )\n\n# # Fine-tune the model\n# trainer.train()\n\n\n# Create a PyTorch dataset and DataLoader\ntrain_dataset = TensorDataset(input_ids, attention_mask, labels)\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n# Set up optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Training loop\nepochs = 3\nmodel.train()\n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    \n    for batch in train_dataloader:\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n\n        # Forward pass\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        \n        # Backward pass\n        loss.backward()\n        \n        # Optimize the model\n        optimizer.step()\n        optimizer.zero_grad()\n\n        print(f\"Batch loss: {loss.item()}\")\n\nprint(\"Training completed.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T07:27:45.315628Z","iopub.execute_input":"2024-10-13T07:27:45.316093Z","iopub.status.idle":"2024-10-13T07:28:17.412009Z","shell.execute_reply.started":"2024-10-13T07:27:45.316045Z","shell.execute_reply":"2024-10-13T07:28:17.410431Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecec506022eb477c94de9520610773da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/703k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d721356b7b840bc9bfd0dfe35d5e1fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/294k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5acd8064ff924792a1377108b3dcde01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef0f2e6488374c699b1e7736e822dc54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/12.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"402c2e2ff9c846ff9f2d8a835590cd18"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f43ae95265e423682c069fa2f19085f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e30a2bcd6dd7450cb9b852cc91c3ed30"}},"metadata":{}},{"name":"stderr","text":"Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at Salesforce/codet5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:2052\u001b[0m, in \u001b[0;36mT5ForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2045\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2046\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf no `decoder_input_ids` or `decoder_inputs_embeds` are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2047\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed, `input_ids` cannot be `None`. Please pass either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2048\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2049\u001b[0m         )\n\u001b[1;32m   2050\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shift_right(input_ids)\n\u001b[0;32m-> 2052\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2061\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2068\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2070\u001b[0m eos_mask \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meos_token_id)\u001b[38;5;241m.\u001b[39mto(sequence_output\u001b[38;5;241m.\u001b[39mdevice)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1515\u001b[0m, in \u001b[0;36mT5Model.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1512\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1515\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1107\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1093\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1094\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         output_attentions,\n\u001b[1;32m   1105\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:747\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    744\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    746\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 747\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:336\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    335\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 336\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDenseReluDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:283\u001b[0m, in \u001b[0;36mT5DenseActDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    281\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwi(hidden_states)\n\u001b[1;32m    282\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[0;32m--> 283\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8\n\u001b[1;32m    288\u001b[0m ):\n\u001b[1;32m    289\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 44.12 MiB is free. Process 3929 has 14.70 GiB memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 167.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 44.12 MiB is free. Process 3929 has 14.70 GiB memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 167.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom torch.cuda.amp import GradScaler, autocast\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/irse-task1-dataset/Task1.csv\")\n\n# Clean and convert the 'Class' column to binary labels (1 for Useful, 0 for Not Useful)\ndf['Class'] = df['Class'].apply(lambda x: 1 if x == 'Useful' else 0)\n\n# Load the tokenizer and model (e.g., CodeT5 for code-related tasks)\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Salesforce/codet5-base\", num_labels=2)  # Binary classification\n\n# Function to preprocess each example\ndef preprocess_function(examples):\n    return tokenizer(\n        f\"code: {examples['Surrounding Code Context']} comment: {examples['Comments']}\", \n        truncation=True, \n        padding=\"max_length\", \n        max_length=256  # Reduced from 512\n    )\n\n# Preprocessing the dataset\nencoded_inputs = df.apply(\n    lambda x: preprocess_function({\n        'Surrounding Code Context': x['Surrounding Code Context'], \n        'Comments': x['Comments']\n    }), \n    axis=1\n)\n\n# Convert the tokenized inputs and labels into tensors for PyTorch\ninput_ids = torch.tensor([ex['input_ids'] for ex in encoded_inputs])\nattention_mask = torch.tensor([ex['attention_mask'] for ex in encoded_inputs])\nlabels = torch.tensor(df['Class'].values)\n\ninput_ids_train, input_ids_test, attention_mask_train, attention_mask_test, labels_train, labels_test = train_test_split(\n    input_ids, attention_mask, labels, test_size=0.3, random_state=42\n)\n\n# Create PyTorch TensorDatasets for train and test sets\ntrain_dataset = TensorDataset(input_ids_train, attention_mask_train, labels_train)\ntest_dataset = TensorDataset(input_ids_test, attention_mask_test, labels_test)\n\n# Create DataLoaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Reduced batch size for example\ntest_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n# Set up optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Initialize the gradient scaler for mixed precision\nscaler = GradScaler()\n\n# Define gradient accumulation steps (optional)\naccumulation_steps = 10  # Set to >1 if using gradient accumulation\n\n# Training loop with mixed precision and optional gradient accumulation\nepochs = 7\nmodel.train()\n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    \n    for step, batch in enumerate(train_dataloader):\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n\n        optimizer.zero_grad()\n        \n        with autocast():\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            loss = loss / accumulation_steps  # Normalize loss if using accumulation\n\n        scaler.scale(loss).backward()\n        \n        if (step + 1) % accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n        print(f\"Batch loss: {loss.item() * accumulation_steps}\")\n\n    # Clear cache at the end of each epoch\n    torch.cuda.empty_cache()\n\nprint(\"Training completed.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:53:03.571800Z","iopub.execute_input":"2024-10-13T09:53:03.572206Z","iopub.status.idle":"2024-10-13T10:34:35.103919Z","shell.execute_reply.started":"2024-10-13T09:53:03.572171Z","shell.execute_reply":"2024-10-13T10:34:35.102937Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of T5ForSequenceClassification were not initialized from the model checkpoint at Salesforce/codet5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/tmp/ipykernel_30/750344061.py:59: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/tmp/ipykernel_30/750344061.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/7\nBatch loss: 0.8737792819738388\nBatch loss: 0.7476196438074112\nBatch loss: 0.7188873738050461\nBatch loss: 0.7958526909351349\nBatch loss: 0.6571655720472336\nBatch loss: 0.4599761962890625\nBatch loss: 0.8472748100757599\nBatch loss: 0.6558533012866974\nBatch loss: 1.1119995266199112\nBatch loss: 0.7671203464269638\nBatch loss: 0.561981201171875\nBatch loss: 0.5241546779870987\nBatch loss: 0.7235107570886612\nBatch loss: 1.0980530083179474\nBatch loss: 0.6794510036706924\nBatch loss: 0.3585357591509819\nBatch loss: 0.5751876905560493\nBatch loss: 0.9009323269128799\nBatch loss: 0.8054047077894211\nBatch loss: 0.9793549031019211\nBatch loss: 1.32537841796875\nBatch loss: 0.8147735893726349\nBatch loss: 0.7208557426929474\nBatch loss: 0.4441986232995987\nBatch loss: 0.6198730692267418\nBatch loss: 0.6303711235523224\nBatch loss: 0.7070465385913849\nBatch loss: 0.6348495930433273\nBatch loss: 0.9415894001722336\nBatch loss: 0.7645111531019211\nBatch loss: 0.5320587381720543\nBatch loss: 0.8695831149816513\nBatch loss: 0.7333679497241974\nBatch loss: 0.5118484422564507\nBatch loss: 1.0189514607191086\nBatch loss: 0.7018585503101349\nBatch loss: 1.0442505031824112\nBatch loss: 1.3211670517921448\nBatch loss: 0.7174148410558701\nBatch loss: 0.6814270466566086\nBatch loss: 1.0098876804113388\nBatch loss: 0.6767616420984268\nBatch loss: 0.7160263508558273\nBatch loss: 0.7893981784582138\nBatch loss: 0.7621154934167862\nBatch loss: 0.4480285570025444\nBatch loss: 0.7817840576171875\nBatch loss: 0.6552200764417648\nBatch loss: 1.141204833984375\nBatch loss: 0.6020660325884819\nBatch loss: 0.4322357103228569\nBatch loss: 0.6075591966509819\nBatch loss: 0.6269378960132599\nBatch loss: 0.7786102592945099\nBatch loss: 0.6669311970472336\nBatch loss: 0.7545166462659836\nBatch loss: 0.2532348595559597\nBatch loss: 0.3862762451171875\nBatch loss: 0.7619476318359375\nBatch loss: 0.5208587646484375\nBatch loss: 0.3785247728228569\nBatch loss: 0.6009368970990181\nBatch loss: 0.8243103325366974\nBatch loss: 0.47803115099668503\nBatch loss: 0.5810546875\nBatch loss: 0.3741493448615074\nBatch loss: 0.6810607761144638\nBatch loss: 0.5338440090417862\nBatch loss: 0.6287536770105362\nBatch loss: 0.8600463718175888\nBatch loss: 0.8244476467370987\nBatch loss: 0.9542236477136612\nBatch loss: 0.685882568359375\nBatch loss: 0.821990966796875\nBatch loss: 0.7846985012292862\nBatch loss: 0.7232894748449326\nBatch loss: 0.5798263475298882\nBatch loss: 0.6985779106616974\nBatch loss: 0.5903930589556694\nBatch loss: 0.7779846340417862\nBatch loss: 0.587715171277523\nBatch loss: 0.8669281005859375\nBatch loss: 0.3382263332605362\nBatch loss: 0.8094292134046555\nBatch loss: 1.1287689208984375\nBatch loss: 0.5426178127527237\nBatch loss: 0.4893665388226509\nBatch loss: 0.705844908952713\nBatch loss: 0.8997688442468643\nBatch loss: 0.6444626301527023\nBatch loss: 0.7701034843921661\nBatch loss: 0.5780563503503799\nBatch loss: 0.8568420261144638\nBatch loss: 0.7925567775964737\nBatch loss: 0.7526931911706924\nBatch loss: 0.651828795671463\nBatch loss: 1.2151336669921875\nBatch loss: 0.3839225694537163\nBatch loss: 0.3402061387896538\nBatch loss: 0.6027679517865181\nBatch loss: 0.5069351196289062\nBatch loss: 0.6456756591796875\nBatch loss: 0.6987380981445312\nBatch loss: 0.8177872002124786\nBatch loss: 0.5793838575482368\nBatch loss: 0.5932846292853355\nBatch loss: 0.7506904751062393\nBatch loss: 0.6876068562269211\nBatch loss: 0.4040241241455078\nBatch loss: 0.5261459574103355\nBatch loss: 0.7601623982191086\nBatch loss: 0.6865806877613068\nBatch loss: 0.8259201049804688\nBatch loss: 0.47320175915956497\nBatch loss: 1.0351257771253586\nBatch loss: 0.43379783630371094\nBatch loss: 0.5206070095300674\nBatch loss: 0.5813293531537056\nBatch loss: 0.5969372019171715\nBatch loss: 0.7934685051441193\nBatch loss: 0.8069000393152237\nBatch loss: 0.47542955726385117\nBatch loss: 0.502571128308773\nBatch loss: 0.9010467678308487\nBatch loss: 0.7009582966566086\nBatch loss: 0.5868396908044815\nBatch loss: 0.42198944836854935\nBatch loss: 1.1385688930749893\nBatch loss: 0.4597015306353569\nBatch loss: 1.17919921875\nBatch loss: 0.7148285210132599\nBatch loss: 1.3255004584789276\nBatch loss: 0.7344742119312286\nBatch loss: 0.38960646837949753\nBatch loss: 0.5474510416388512\nBatch loss: 0.9001769870519638\nBatch loss: 1.5022888779640198\nBatch loss: 0.9336509555578232\nBatch loss: 0.3718261793255806\nBatch loss: 0.5076904222369194\nBatch loss: 0.96099853515625\nBatch loss: 0.9771423786878586\nBatch loss: 0.5416126176714897\nBatch loss: 0.3653259202837944\nBatch loss: 0.6034431606531143\nBatch loss: 0.8384781330823898\nBatch loss: 1.2804412841796875\nBatch loss: 0.6827240437269211\nBatch loss: 0.8640747517347336\nBatch loss: 0.7299347221851349\nBatch loss: 0.38269806653261185\nBatch loss: 0.8055801689624786\nBatch loss: 0.6104583665728569\nBatch loss: 1.0144386440515518\nBatch loss: 0.7294197380542755\nBatch loss: 0.6174449995160103\nBatch loss: 0.8659286797046661\nBatch loss: 0.7749100029468536\nBatch loss: 0.4982910305261612\nBatch loss: 0.5764312669634819\nBatch loss: 0.7128353416919708\nBatch loss: 1.0064201802015305\nBatch loss: 0.6959228962659836\nBatch loss: 0.3547554090619087\nBatch loss: 0.5396289750933647\nBatch loss: 0.3704986721277237\nBatch loss: 0.8660908043384552\nBatch loss: 0.5045337602496147\nBatch loss: 0.4456482082605362\nBatch loss: 0.3947753831744194\nBatch loss: 0.5343666300177574\nBatch loss: 0.6759567558765411\nBatch loss: 0.6567078083753586\nBatch loss: 0.6909789890050888\nBatch loss: 0.5762176588177681\nBatch loss: 0.6655273586511612\nBatch loss: 0.328432098031044\nBatch loss: 0.8129806816577911\nBatch loss: 0.625457763671875\nBatch loss: 0.7346496731042862\nBatch loss: 0.46878814697265625\nBatch loss: 0.7449646294116974\nBatch loss: 0.6891327351331711\nBatch loss: 0.8367309719324112\nBatch loss: 0.5420532450079918\nBatch loss: 1.1640243977308273\nBatch loss: 0.4489288479089737\nBatch loss: 0.642547607421875\nBatch loss: 0.6191101297736168\nBatch loss: 0.4505615308880806\nBatch loss: 0.5477046966552734\nBatch loss: 0.5575714260339737\nBatch loss: 0.6740341335535049\nBatch loss: 0.30016327276825905\nBatch loss: 0.7899971306324005\nBatch loss: 0.574897788465023\nBatch loss: 0.5325813218951225\nBatch loss: 0.3699016571044922\nBatch loss: 0.8320464938879013\nBatch loss: 0.5563812330365181\nBatch loss: 0.6201858446002007\nBatch loss: 0.6297912448644638\nBatch loss: 0.5374221876263618\nBatch loss: 0.32326508313417435\nBatch loss: 0.7282104343175888\nBatch loss: 0.8337860554456711\nBatch loss: 0.6414375454187393\nBatch loss: 0.42531587183475494\nBatch loss: 0.3647613525390625\nBatch loss: 0.5705642700195312\nBatch loss: 0.4839324951171875\nBatch loss: 0.3797149658203125\nBatch loss: 0.3694610670208931\nBatch loss: 0.3113861195743084\nBatch loss: 0.4071197658777237\nBatch loss: 0.37995148450136185\nBatch loss: 0.5387115478515625\nBatch loss: 0.5732421949505806\nBatch loss: 0.8307800441980362\nBatch loss: 0.5697936937212944\nBatch loss: 0.5988616868853569\nBatch loss: 0.6584320217370987\nBatch loss: 0.5983429029583931\nBatch loss: 0.5940628051757812\nBatch loss: 0.7111968845129013\nBatch loss: 0.5469055101275444\nBatch loss: 0.6100311502814293\nBatch loss: 0.9453888237476349\nBatch loss: 0.8076363056898117\nBatch loss: 0.7954788208007812\nBatch loss: 0.2571830712258816\nBatch loss: 0.7398682087659836\nBatch loss: 0.6751137226819992\nBatch loss: 0.5673675611615181\nBatch loss: 0.6318359822034836\nBatch loss: 0.4940032958984375\nBatch loss: 0.4544639587402344\nBatch loss: 0.46714019030332565\nBatch loss: 0.5574798583984375\nBatch loss: 0.4051857069134712\nBatch loss: 0.46715546399354935\nBatch loss: 0.4708404466509819\nBatch loss: 1.094207763671875\nBatch loss: 0.5692138895392418\nBatch loss: 0.4024658352136612\nBatch loss: 0.6344451755285263\nBatch loss: 0.31256865710020065\nBatch loss: 0.6214294582605362\nBatch loss: 0.6416511535644531\nBatch loss: 0.6801910698413849\nBatch loss: 0.5223694071173668\nBatch loss: 0.3577995300292969\nBatch loss: 0.3551940992474556\nBatch loss: 0.5365142971277237\nBatch loss: 0.3378906473517418\nBatch loss: 0.3916473314166069\nBatch loss: 0.548553466796875\nBatch loss: 0.8167725056409836\nBatch loss: 0.7100830227136612\nBatch loss: 0.42840100824832916\nBatch loss: 0.7578124850988388\nBatch loss: 0.43851472437381744\nBatch loss: 0.32697487622499466\nBatch loss: 0.49605559557676315\nBatch loss: 0.6801529228687286\nBatch loss: 0.5333147197961807\nBatch loss: 0.5933075025677681\nBatch loss: 0.6196670606732368\nBatch loss: 0.7062835991382599\nBatch loss: 0.5236587673425674\nBatch loss: 0.44184114784002304\nBatch loss: 0.666656494140625\nBatch loss: 0.6533356010913849\nBatch loss: 0.6728363037109375\nBatch loss: 0.6477051228284836\nBatch loss: 0.29657745733857155\nBatch loss: 0.3148765489459038\nBatch loss: 0.7642059773206711\nBatch loss: 0.5370178446173668\nBatch loss: 0.7891235500574112\nBatch loss: 0.6196289137005806\nBatch loss: 0.4864654690027237\nBatch loss: 0.6917267292737961\nBatch loss: 0.4755096510052681\nBatch loss: 0.5673370510339737\nBatch loss: 0.42164992541074753\nBatch loss: 0.6543426960706711\nBatch loss: 0.4161834716796875\nBatch loss: 1.2230072170495987\nBatch loss: 0.6841659545898438\nBatch loss: 0.9899292141199112\nBatch loss: 0.5003891140222549\nBatch loss: 0.5431365966796875\nBatch loss: 0.4998016357421875\nBatch loss: 0.38129426538944244\nBatch loss: 0.28307342901825905\nBatch loss: 0.8707656711339951\nBatch loss: 0.5117492750287056\nBatch loss: 0.7105255126953125\nBatch loss: 0.4855194315314293\nBatch loss: 0.25887681171298027\nBatch loss: 0.7437896728515625\nBatch loss: 0.6435852497816086\nBatch loss: 0.7904434204101562\nBatch loss: 0.3673095628619194\nBatch loss: 0.5788803100585938\nBatch loss: 0.3536262735724449\nBatch loss: 0.6824417412281036\nBatch loss: 0.4002685472369194\nBatch loss: 0.24449540302157402\nBatch loss: 0.3855590894818306\nBatch loss: 0.9105682373046875\nBatch loss: 0.5084533616900444\nBatch loss: 0.3524933010339737\nBatch loss: 0.38720130920410156\nBatch loss: 0.4671764373779297\nBatch loss: 0.8211670070886612\nBatch loss: 0.35596083849668503\nBatch loss: 0.39251234382390976\nBatch loss: 0.504608154296875\nBatch loss: 0.3363037109375\nBatch loss: 0.49603845924139023\nBatch loss: 0.2500343322753906\nBatch loss: 0.544954314827919\nBatch loss: 0.3657989576458931\nBatch loss: 0.8276062458753586\nBatch loss: 0.2361450158059597\nBatch loss: 0.7108459621667862\nBatch loss: 0.48572540283203125\nBatch loss: 0.8787231892347336\nBatch loss: 0.5024528503417969\nBatch loss: 0.7952880859375\nBatch loss: 0.8594818413257599\nBatch loss: 0.3636474534869194\nBatch loss: 1.1840210109949112\nBatch loss: 0.7181091606616974\nBatch loss: 0.9435196220874786\nBatch loss: 0.39217185229063034\nBatch loss: 0.4177207872271538\nBatch loss: 0.6097602844238281\nBatch loss: 0.7430382072925568\nBatch loss: 0.6787719577550888\nBatch loss: 0.3933868557214737\nBatch loss: 0.6900787353515625\nBatch loss: 0.2734680287539959\nBatch loss: 0.38321685045957565\nBatch loss: 0.7601013034582138\nBatch loss: 0.636749267578125\nBatch loss: 0.8910217136144638\nBatch loss: 0.19726181402802467\nBatch loss: 0.9457244724035263\nBatch loss: 0.6141433864831924\nBatch loss: 1.0464706271886826\nBatch loss: 0.4316291958093643\nBatch loss: 0.9089507907629013\nBatch loss: 0.7555618137121201\nBatch loss: 0.24816131219267845\nBatch loss: 0.3257942199707031\nBatch loss: 0.5914001539349556\nBatch loss: 0.837223082780838\nBatch loss: 0.7541809231042862\nBatch loss: 0.49648284912109375\nBatch loss: 0.5695886537432671\nBatch loss: 0.6607818603515625\nBatch loss: 0.43781664222478867\nBatch loss: 0.7849540561437607\nBatch loss: 0.4463005065917969\nBatch loss: 0.32053377479314804\nBatch loss: 0.39883043617010117\nBatch loss: 0.4345903545618057\nBatch loss: 0.6693420559167862\nBatch loss: 0.32080840319395065\nBatch loss: 0.3569602966308594\nBatch loss: 1.0570984333753586\nBatch loss: 0.4893035814166069\nBatch loss: 0.3910866007208824\nBatch loss: 0.4373359680175781\nBatch loss: 0.6605911254882812\nBatch loss: 0.4228201135993004\nBatch loss: 0.7626600563526154\nBatch loss: 0.5542850494384766\nBatch loss: 0.5834236368536949\nBatch loss: 0.9665413200855255\nBatch loss: 1.2052011489868164\nBatch loss: 0.622011199593544\nBatch loss: 0.34897424280643463\nBatch loss: 0.837644562125206\nBatch loss: 0.6447868794202805\nBatch loss: 0.6725921481847763\nBatch loss: 0.5241775512695312\nBatch loss: 0.6127738952636719\nBatch loss: 0.5164947733283043\nBatch loss: 0.55208969861269\nBatch loss: 0.8706207573413849\nBatch loss: 0.9165497124195099\nBatch loss: 0.9459457546472549\nBatch loss: 0.5100374296307564\nBatch loss: 0.9153709560632706\nBatch loss: 0.5200309678912163\nBatch loss: 0.28281403705477715\nBatch loss: 0.6694374233484268\nBatch loss: 0.31812097877264023\nBatch loss: 0.9198303520679474\nBatch loss: 0.7339382171630859\nBatch loss: 0.21213149651885033\nBatch loss: 0.4932517930865288\nBatch loss: 0.6349029392004013\nBatch loss: 0.7165222615003586\nBatch loss: 0.5203781276941299\nBatch loss: 0.57009506970644\nBatch loss: 0.6223297119140625\nBatch loss: 0.7914810627698898\nBatch loss: 0.2029857598245144\nBatch loss: 0.6487579643726349\nBatch loss: 0.5163459852337837\nBatch loss: 0.2872276306152344\nBatch loss: 1.2457199394702911\nBatch loss: 0.6868839263916016\nBatch loss: 0.9273529052734375\nBatch loss: 0.5696258693933487\nBatch loss: 0.7326278835535049\nBatch loss: 0.4989499971270561\nBatch loss: 0.3992309793829918\nBatch loss: 1.0224723815917969\nBatch loss: 0.740177184343338\nBatch loss: 0.12434768490493298\nBatch loss: 0.7769718021154404\nBatch loss: 0.3250741958618164\nBatch loss: 0.4331665113568306\nBatch loss: 0.8251571655273438\nBatch loss: 0.6835594028234482\nBatch loss: 0.9608383476734161\nBatch loss: 1.1084061115980148\nBatch loss: 0.402679443359375\nBatch loss: 0.39818573743104935\nBatch loss: 0.3540840372443199\nBatch loss: 0.6646118313074112\nBatch loss: 0.5965080484747887\nBatch loss: 0.47549057751893997\nBatch loss: 0.5114517360925674\nBatch loss: 0.6443214416503906\nBatch loss: 0.511295311152935\nBatch loss: 0.615692138671875\nBatch loss: 0.8648528903722763\nBatch loss: 0.1893463172018528\nBatch loss: 0.3806610032916069\nBatch loss: 0.47496605664491653\nBatch loss: 0.16223764047026634\nBatch loss: 1.276191771030426\nBatch loss: 0.4627075418829918\nBatch loss: 0.664251372218132\nBatch loss: 0.7774887233972549\nBatch loss: 0.5628089979290962\nBatch loss: 0.8134613186120987\nBatch loss: 0.6836395710706711\nBatch loss: 0.3725557401776314\nBatch loss: 0.5379180982708931\nBatch loss: 0.7759246975183487\nBatch loss: 0.8659382164478302\nBatch loss: 0.5331554636359215\nBatch loss: 0.3487701341509819\nBatch loss: 0.5567045137286186\nBatch loss: 0.48717308789491653\nBatch loss: 0.9316902607679367\nBatch loss: 0.9134292602539062\nBatch loss: 0.9945755451917648\nBatch loss: 0.6919069588184357\nBatch loss: 0.8108367770910263\nBatch loss: 0.3482360765337944\nBatch loss: 0.8906402438879013\nBatch loss: 0.6926422566175461\nBatch loss: 0.38813021034002304\nBatch loss: 0.4629068449139595\nBatch loss: 0.25852395221590996\nBatch loss: 0.6020679697394371\nBatch loss: 0.30828094109892845\nBatch loss: 1.2374420464038849\nBatch loss: 0.21875381469726562\nBatch loss: 0.32854463905096054\nBatch loss: 0.21329497918486595\nBatch loss: 0.7750282436609268\nBatch loss: 0.47151949256658554\nBatch loss: 0.250091552734375\nBatch loss: 0.41541386395692825\nBatch loss: 0.4573211818933487\nBatch loss: 0.7135448604822159\nBatch loss: 0.43618012219667435\nBatch loss: 0.3173408657312393\nBatch loss: 0.6543350219726562\nBatch loss: 0.4567260667681694\nBatch loss: 0.7609939575195312\nBatch loss: 0.5509471893310547\nBatch loss: 0.8645935356616974\nBatch loss: 0.4732093960046768\nBatch loss: 0.6537704914808273\nBatch loss: 0.8192320168018341\nBatch loss: 0.5460205301642418\nBatch loss: 0.4716014862060547\nBatch loss: 0.5045089870691299\nBatch loss: 0.4945678636431694\nBatch loss: 0.3707466274499893\nBatch loss: 0.580143928527832\nBatch loss: 0.5171890184283257\nBatch loss: 0.2681560628116131\nBatch loss: 0.7660599052906036\nBatch loss: 0.5045929178595543\nBatch loss: 0.589384101331234\nBatch loss: 0.5532703548669815\nBatch loss: 0.3041038475930691\nBatch loss: 0.9154510498046875\nBatch loss: 0.20474625751376152\nBatch loss: 0.3141326829791069\nBatch loss: 0.23369407281279564\nBatch loss: 0.32017897814512253\nBatch loss: 0.5359191820025444\nBatch loss: 0.5713234096765518\nBatch loss: 0.7113724201917648\nBatch loss: 1.0895156860351562\nBatch loss: 0.4922065883874893\nBatch loss: 0.6399764865636826\nBatch loss: 0.2314319647848606\nBatch loss: 1.2598954141139984\nBatch loss: 0.9964904934167862\nBatch loss: 0.9850025177001953\nBatch loss: 0.5906171724200249\nBatch loss: 0.7839889824390411\nBatch loss: 0.23234272375702858\nBatch loss: 0.4239768907427788\nBatch loss: 0.2632331848144531\nBatch loss: 0.33463288098573685\nBatch loss: 0.9003124386072159\nBatch loss: 0.35549163818359375\nBatch loss: 0.6672954559326172\nBatch loss: 0.6908893585205078\nBatch loss: 0.7752761989831924\nBatch loss: 1.092906966805458\nBatch loss: 0.47814179211854935\nBatch loss: 0.6445503234863281\nBatch loss: 0.9271698445081711\nBatch loss: 0.523250587284565\nBatch loss: 0.9227447956800461\nBatch loss: 0.20626068115234375\nBatch loss: 0.47545623034238815\nBatch loss: 0.8543968200683594\nBatch loss: 0.43424226343631744\nBatch loss: 0.36663055419921875\nBatch loss: 0.5759048461914062\nBatch loss: 0.34431077539920807\nBatch loss: 0.6200866773724556\nBatch loss: 0.7395553588867188\nBatch loss: 0.6329345703125\nBatch loss: 0.6435661762952805\nBatch loss: 0.6745453178882599\nBatch loss: 0.7674159854650497\nBatch loss: 0.7345046848058701\nBatch loss: 0.5398178100585938\nBatch loss: 0.41788388043642044\nBatch loss: 0.4303741455078125\nBatch loss: 0.2733459509909153\nBatch loss: 0.6356735527515411\nBatch loss: 0.5641040951013565\nBatch loss: 0.4298291355371475\nBatch loss: 0.8535309135913849\nBatch loss: 0.34259796142578125\nBatch loss: 0.5528221279382706\nBatch loss: 0.6330566853284836\nBatch loss: 0.4056091234087944\nBatch loss: 0.4391021654009819\nBatch loss: 0.8646545559167862\nBatch loss: 0.6055908277630806\nBatch loss: 0.48148345202207565\nBatch loss: 0.38914110511541367\nBatch loss: 0.5721740797162056\nBatch loss: 0.8067627251148224\nBatch loss: 0.5943221971392632\nBatch loss: 0.4923858866095543\nBatch loss: 0.5242538452148438\nBatch loss: 0.8214721828699112\nBatch loss: 0.8953704684972763\nBatch loss: 0.9118347615003586\nBatch loss: 0.7906647026538849\nBatch loss: 0.4445495828986168\nBatch loss: 0.5101928859949112\nBatch loss: 0.7321872562170029\nBatch loss: 0.7792816311120987\nBatch loss: 0.6083374097943306\nBatch loss: 0.72509765625\nBatch loss: 0.5688629299402237\nBatch loss: 0.6579742580652237\nBatch loss: 0.3798484802246094\nBatch loss: 0.6287994235754013\nBatch loss: 0.5915222316980362\nBatch loss: 0.5019378662109375\nBatch loss: 0.47871779650449753\nBatch loss: 0.2541770972311497\nBatch loss: 0.5239028856158257\nBatch loss: 0.28878403827548027\nBatch loss: 0.5032806470990181\nBatch loss: 0.7329864799976349\nBatch loss: 0.33751677721738815\nBatch loss: 0.6512451171875\nBatch loss: 0.36556627601385117\nBatch loss: 0.7741012424230576\nBatch loss: 0.29131699353456497\nBatch loss: 0.4522857815027237\nBatch loss: 0.3295593336224556\nBatch loss: 0.6318550556898117\nBatch loss: 0.49576569348573685\nBatch loss: 0.6622924655675888\nBatch loss: 0.4862251505255699\nBatch loss: 0.43413545936346054\nBatch loss: 0.7664718478918076\nBatch loss: 0.53314208984375\nBatch loss: 0.4849090799689293\nBatch loss: 0.4743042215704918\nBatch loss: 0.5285415798425674\nBatch loss: 0.331663154065609\nBatch loss: 0.6058120727539062\nBatch loss: 0.38494493812322617\nBatch loss: 0.2594146691262722\nBatch loss: 0.3433075174689293\nBatch loss: 0.47057535499334335\nBatch loss: 0.7642974704504013\nBatch loss: 0.4900817945599556\nBatch loss: 0.5071096494793892\nBatch loss: 0.5449638515710831\nBatch loss: 0.49660492688417435\nBatch loss: 0.5095977708697319\nBatch loss: 0.3885650634765625\nBatch loss: 0.7795410603284836\nBatch loss: 0.4395141825079918\nBatch loss: 0.5740890651941299\nBatch loss: 0.4981536790728569\nBatch loss: 0.49346923828125\nBatch loss: 0.5286598205566406\nBatch loss: 0.8350219577550888\nBatch loss: 0.9317626804113388\nBatch loss: 0.7763137668371201\nBatch loss: 0.5621414259076118\nBatch loss: 0.5681762844324112\nBatch loss: 0.47909166663885117\nBatch loss: 0.48087310045957565\nBatch loss: 0.6159668043255806\nBatch loss: 0.3836212307214737\nBatch loss: 0.37811279296875\nBatch loss: 0.5585498735308647\nBatch loss: 0.40926363319158554\nBatch loss: 0.625637099146843\nBatch loss: 0.7706909626722336\nBatch loss: 0.4839172586798668\nBatch loss: 0.7137604057788849\nBatch loss: 0.7576751708984375\nBatch loss: 0.36185454577207565\nBatch loss: 0.32994844019412994\nBatch loss: 0.45287322252988815\nBatch loss: 0.9184799343347549\nBatch loss: 0.31743239611387253\nBatch loss: 0.9616126865148544\nBatch loss: 0.6202583387494087\nBatch loss: 0.4262657091021538\nBatch loss: 0.2507324330508709\nBatch loss: 0.3356209024786949\nBatch loss: 0.4049377515912056\nBatch loss: 0.3899688646197319\nBatch loss: 0.4230194166302681\nBatch loss: 0.8929596096277237\nBatch loss: 0.6714057922363281\nBatch loss: 0.3349609300494194\nBatch loss: 0.5534572526812553\nBatch loss: 0.4803771898150444\nBatch loss: 0.4463806375861168\nBatch loss: 0.516456626355648\nBatch loss: 0.3275756910443306\nBatch loss: 0.635223388671875\nBatch loss: 0.38475800305604935\nBatch loss: 0.48604201525449753\nBatch loss: 0.7887192070484161\nBatch loss: 0.4100990295410156\nBatch loss: 0.28144074603915215\nBatch loss: 0.6599006801843643\nBatch loss: 0.8987960964441299\nBatch loss: 0.5895919725298882\nBatch loss: 0.7330550998449326\nBatch loss: 0.40665436536073685\nBatch loss: 0.7391967624425888\nBatch loss: 0.9274597465991974\nBatch loss: 0.5937957763671875\nBatch loss: 0.7180862873792648\nBatch loss: 0.6729278713464737\nBatch loss: 0.8322601765394211\nBatch loss: 0.41229248046875\nBatch loss: 0.527748130261898\nBatch loss: 0.4136810451745987\nBatch loss: 0.688018798828125\nBatch loss: 0.5831985548138618\nBatch loss: 0.24858856573700905\nBatch loss: 0.5765485763549805\nBatch loss: 0.41391756385564804\nBatch loss: 0.636138916015625\nBatch loss: 0.41236497461795807\nBatch loss: 0.5792236328125\nBatch loss: 0.6167755275964737\nBatch loss: 0.5391235277056694\nBatch loss: 0.2709503285586834\nBatch loss: 0.6143493577837944\nBatch loss: 0.2532653883099556\nBatch loss: 0.9878158569335938\nBatch loss: 0.5475616455078125\nBatch loss: 0.8864212036132812\nBatch loss: 0.4087219387292862\nBatch loss: 0.8531951904296875\nBatch loss: 0.6268959492444992\nBatch loss: 0.5454101786017418\nBatch loss: 0.6906433403491974\nBatch loss: 0.6605453789234161\nBatch loss: 0.3035125695168972\nBatch loss: 0.4334869608283043\nBatch loss: 0.618896484375\nBatch loss: 0.6094741821289062\nBatch loss: 0.5270843580365181\nBatch loss: 0.9739303588867188\nBatch loss: 0.3827667236328125\nBatch loss: 0.58807373046875\nBatch loss: 0.7024192810058594\nBatch loss: 0.38541413843631744\nBatch loss: 0.2277679555118084\nBatch loss: 0.4219360277056694\nBatch loss: 0.4645843431353569\nBatch loss: 0.4763031005859375\nBatch loss: 0.5253448709845543\nBatch loss: 0.8287811279296875\nBatch loss: 0.4985160753130913\nBatch loss: 0.3403015062212944\nBatch loss: 0.42877960950136185\nBatch loss: 0.8543091267347336\nBatch loss: 0.47260668128728867\nBatch loss: 0.43625641614198685\nBatch loss: 0.7713623344898224\nBatch loss: 0.6717834621667862\nBatch loss: 0.2837982214987278\nBatch loss: 0.6951446831226349\nBatch loss: 0.5943451076745987\nBatch loss: 0.3550262376666069\nBatch loss: 0.6606902927160263\nBatch loss: 0.34095000475645065\nBatch loss: 0.4545288160443306\nBatch loss: 0.7939243316650391\nBatch loss: 0.40709685534238815\nBatch loss: 0.6272583454847336\nBatch loss: 0.3893737867474556\nBatch loss: 0.5886077880859375\nBatch loss: 0.923309326171875\nBatch loss: 0.5565948411822319\nBatch loss: 0.9917602688074112\nBatch loss: 0.3616943582892418\nBatch loss: 0.7049637287855148\nBatch loss: 1.0924682766199112\nBatch loss: 0.47739412635564804\nBatch loss: 0.6544265896081924\nBatch loss: 0.36277007311582565\nBatch loss: 0.6036987528204918\nBatch loss: 0.53286362439394\nBatch loss: 1.1407470703125\nBatch loss: 0.7197647541761398\nBatch loss: 0.6373824924230576\nBatch loss: 0.4823455959558487\nBatch loss: 0.6422080844640732\nBatch loss: 0.5514564737677574\nBatch loss: 0.25102997198700905\nBatch loss: 0.5198326334357262\nBatch loss: 0.6000404432415962\nBatch loss: 0.6475601345300674\nBatch loss: 0.7062911987304688\nBatch loss: 0.5623626708984375\nBatch loss: 0.5365142971277237\nBatch loss: 0.5361557006835938\nBatch loss: 0.4385509714484215\nBatch loss: 0.7460518181324005\nBatch loss: 0.34932326525449753\nBatch loss: 0.4523773118853569\nBatch loss: 0.7071914523839951\nBatch loss: 0.7239379733800888\nBatch loss: 0.5477142333984375\nBatch loss: 0.3497162088751793\nBatch loss: 0.2924499474465847\nBatch loss: 0.39548493921756744\nBatch loss: 0.4738769680261612\nBatch loss: 0.2599182166159153\nBatch loss: 0.23976897820830345\nBatch loss: 0.5723381042480469\nBatch loss: 0.20612334832549095\nBatch loss: 0.34270476549863815\nBatch loss: 1.4105224609375\nBatch loss: 0.2713623084127903\nBatch loss: 0.9659557789564133\nBatch loss: 0.4642791673541069\nBatch loss: 0.8013458549976349\nBatch loss: 0.42956162244081497\nBatch loss: 0.16814423725008965\nBatch loss: 0.6460037082433701\nBatch loss: 0.7916164398193359\nBatch loss: 0.6476517021656036\nBatch loss: 0.5272255092859268\nBatch loss: 0.25894928723573685\nBatch loss: 0.6694183498620987\nBatch loss: 0.7994613796472549\nBatch loss: 0.6798501312732697\nBatch loss: 0.7000694423913956\nBatch loss: 1.312120407819748\nBatch loss: 0.47672271728515625\nBatch loss: 0.4639415815472603\nBatch loss: 0.25084113702178\nBatch loss: 0.5736999586224556\nBatch loss: 0.716579481959343\nBatch loss: 0.7578048855066299\nBatch loss: 0.78155517578125\nBatch loss: 0.5627136304974556\nBatch loss: 0.7565536350011826\nBatch loss: 0.7766914367675781\nBatch loss: 0.6903572380542755\nBatch loss: 0.1485157012939453\nBatch loss: 1.0340118408203125\nBatch loss: 0.5074195936322212\nBatch loss: 0.32921601086854935\nBatch loss: 0.511537566781044\nBatch loss: 0.6543999165296555\nBatch loss: 0.5317211151123047\nBatch loss: 1.1605072021484375\nEpoch 2/7\nBatch loss: 0.6872520595788956\nBatch loss: 0.22983169183135033\nBatch loss: 0.9303360432386398\nBatch loss: 0.29479026794433594\nBatch loss: 0.38222886621952057\nBatch loss: 0.5297870561480522\nBatch loss: 0.7974453270435333\nBatch loss: 1.3519592583179474\nBatch loss: 0.5290107801556587\nBatch loss: 0.24152947589755058\nBatch loss: 0.8583440631628036\nBatch loss: 0.3782501444220543\nBatch loss: 0.7514915615320206\nBatch loss: 0.5338172987103462\nBatch loss: 0.30252838507294655\nBatch loss: 0.29139328747987747\nBatch loss: 0.9509105980396271\nBatch loss: 0.45730210840702057\nBatch loss: 0.6090622022747993\nBatch loss: 0.5484542995691299\nBatch loss: 0.5655365064740181\nBatch loss: 0.8701057732105255\nBatch loss: 0.6867523491382599\nBatch loss: 0.6695976108312607\nBatch loss: 0.893671065568924\nBatch loss: 0.6870384514331818\nBatch loss: 0.3674469143152237\nBatch loss: 0.8038683235645294\nBatch loss: 0.46221543103456497\nBatch loss: 0.6187744066119194\nBatch loss: 0.3902130201458931\nBatch loss: 0.31497955322265625\nBatch loss: 0.6589183956384659\nBatch loss: 0.5959949642419815\nBatch loss: 0.7964020222425461\nBatch loss: 0.7433777302503586\nBatch loss: 0.5237884446978569\nBatch loss: 0.7380809634923935\nBatch loss: 0.3011741675436497\nBatch loss: 0.3259887918829918\nBatch loss: 0.5841522291302681\nBatch loss: 0.5147018656134605\nBatch loss: 0.273284912109375\nBatch loss: 0.2943229675292969\nBatch loss: 0.4306640848517418\nBatch loss: 0.5602989345788956\nBatch loss: 0.2657318115234375\nBatch loss: 0.6318054348230362\nBatch loss: 0.6778564304113388\nBatch loss: 0.4305877909064293\nBatch loss: 0.5059509351849556\nBatch loss: 0.6014442443847656\nBatch loss: 0.7791747897863388\nBatch loss: 0.6547241657972336\nBatch loss: 0.2579498291015625\nBatch loss: 0.3970460966229439\nBatch loss: 0.87127685546875\nBatch loss: 0.48987578600645065\nBatch loss: 0.520782470703125\nBatch loss: 0.5387001112103462\nBatch loss: 0.8879623562097549\nBatch loss: 0.11943817138671875\nBatch loss: 0.3479881212115288\nBatch loss: 0.5420570448040962\nBatch loss: 1.0917510837316513\nBatch loss: 0.38060761988162994\nBatch loss: 0.4959259182214737\nBatch loss: 0.7580261677503586\nBatch loss: 0.5016632005572319\nBatch loss: 0.5107498168945312\nBatch loss: 0.4175262525677681\nBatch loss: 0.2888526953756809\nBatch loss: 0.26925278827548027\nBatch loss: 0.3812842443585396\nBatch loss: 0.5143585428595543\nBatch loss: 0.48943329602479935\nBatch loss: 0.5860977247357368\nBatch loss: 0.46942900866270065\nBatch loss: 0.7275314629077911\nBatch loss: 0.6431350857019424\nBatch loss: 0.5024166032671928\nBatch loss: 1.1193656921386719\nBatch loss: 0.6741736084222794\nBatch loss: 0.34743499010801315\nBatch loss: 0.45162487775087357\nBatch loss: 0.38703061640262604\nBatch loss: 0.7277164608240128\nBatch loss: 0.9625854343175888\nBatch loss: 0.6657753139734268\nBatch loss: 0.41976548731327057\nBatch loss: 0.4942970350384712\nBatch loss: 0.740177184343338\nBatch loss: 0.68145751953125\nBatch loss: 0.4715538024902344\nBatch loss: 0.6343159824609756\nBatch loss: 0.33634185791015625\nBatch loss: 0.6566467136144638\nBatch loss: 1.002601608633995\nBatch loss: 0.6985550373792648\nBatch loss: 0.4665832594037056\nBatch loss: 0.9766922146081924\nBatch loss: 0.6006526947021484\nBatch loss: 0.6586046516895294\nBatch loss: 0.6617508083581924\nBatch loss: 0.2615385130047798\nBatch loss: 1.26068115234375\nBatch loss: 0.2731471136212349\nBatch loss: 1.0257873684167862\nBatch loss: 0.5801696702837944\nBatch loss: 0.3008232079446316\nBatch loss: 0.3500976786017418\nBatch loss: 0.6764841079711914\nBatch loss: 0.7115459442138672\nBatch loss: 0.7686767727136612\nBatch loss: 0.7763671875\nBatch loss: 0.6190872192382812\nBatch loss: 0.5742492899298668\nBatch loss: 0.8053131401538849\nBatch loss: 1.1002855747938156\nBatch loss: 0.7229766994714737\nBatch loss: 0.7142334431409836\nBatch loss: 0.4860496520996094\nBatch loss: 0.2931938134133816\nBatch loss: 0.5403251573443413\nBatch loss: 0.8527612686157227\nBatch loss: 0.5554504320025444\nBatch loss: 0.3303069993853569\nBatch loss: 0.6339111179113388\nBatch loss: 0.5263786390423775\nBatch loss: 0.41912078857421875\nBatch loss: 0.54046630859375\nBatch loss: 0.2812185324728489\nBatch loss: 0.2186431922018528\nBatch loss: 0.5084800720214844\nBatch loss: 0.9359131008386612\nBatch loss: 1.1235465854406357\nBatch loss: 0.4807129129767418\nBatch loss: 1.063636764883995\nBatch loss: 0.42712021619081497\nBatch loss: 0.6118622049689293\nBatch loss: 0.42709924280643463\nBatch loss: 0.28653716668486595\nBatch loss: 0.28824901208281517\nBatch loss: 0.5974883958697319\nBatch loss: 0.3490295633673668\nBatch loss: 0.39727210998535156\nBatch loss: 0.5854034423828125\nBatch loss: 0.47470856457948685\nBatch loss: 0.8133087307214737\nBatch loss: 0.6385345757007599\nBatch loss: 0.6616821140050888\nBatch loss: 0.8168792724609375\nBatch loss: 0.37925340235233307\nBatch loss: 0.2586975134909153\nBatch loss: 0.3726501390337944\nBatch loss: 0.8984442055225372\nBatch loss: 0.7198029011487961\nBatch loss: 0.3844909742474556\nBatch loss: 0.4496154934167862\nBatch loss: 0.2992095984518528\nBatch loss: 0.2846565283834934\nBatch loss: 0.4690227657556534\nBatch loss: 0.4831848293542862\nBatch loss: 0.36643028259277344\nBatch loss: 0.4565734788775444\nBatch loss: 0.7124252617359161\nBatch loss: 0.7471161335706711\nBatch loss: 0.8303432911634445\nBatch loss: 0.6553249806165695\nBatch loss: 0.8221664279699326\nBatch loss: 0.45282840728759766\nBatch loss: 0.8175277709960938\nBatch loss: 0.30057525262236595\nBatch loss: 0.6570740044116974\nBatch loss: 0.3552436828613281\nBatch loss: 0.6862717121839523\nBatch loss: 0.4984626919031143\nBatch loss: 0.7114677876234055\nBatch loss: 0.20803069695830345\nBatch loss: 0.7401122897863388\nBatch loss: 0.4591217264533043\nBatch loss: 0.3234577178955078\nBatch loss: 0.7294692844152451\nBatch loss: 0.6597290188074112\nBatch loss: 0.4416218027472496\nBatch loss: 0.792999267578125\nBatch loss: 0.6928329914808273\nBatch loss: 0.29234504327178\nBatch loss: 0.28666306287050247\nBatch loss: 0.3207864984869957\nBatch loss: 0.7424698024988174\nBatch loss: 0.9962387382984161\nBatch loss: 0.6553135067224503\nBatch loss: 0.7210864871740341\nBatch loss: 0.808962807059288\nBatch loss: 0.3782663494348526\nBatch loss: 0.2064685896039009\nBatch loss: 0.6569748371839523\nBatch loss: 0.2471017837524414\nBatch loss: 0.7769317924976349\nBatch loss: 0.5752677842974663\nBatch loss: 0.1289691962301731\nBatch loss: 0.3586120530962944\nBatch loss: 0.23790407925844193\nBatch loss: 0.29424095526337624\nBatch loss: 0.39808083325624466\nBatch loss: 0.8548984676599503\nBatch loss: 0.675012618303299\nBatch loss: 0.22303033620119095\nBatch loss: 0.07537460420280695\nBatch loss: 0.33582307398319244\nBatch loss: 0.6442756950855255\nBatch loss: 0.623599998652935\nBatch loss: 0.2930774725973606\nBatch loss: 0.9864959865808487\nBatch loss: 0.9377937763929367\nBatch loss: 0.5535125732421875\nBatch loss: 1.015642210841179\nBatch loss: 0.6599235534667969\nBatch loss: 0.6311941146850586\nBatch loss: 0.10476303286850452\nBatch loss: 1.1224555969238281\nBatch loss: 0.2652549743652344\nBatch loss: 1.2587432563304901\nBatch loss: 1.5045471489429474\nBatch loss: 0.34597110003232956\nBatch loss: 0.21424388512969017\nBatch loss: 0.433712974190712\nBatch loss: 0.32293129712343216\nBatch loss: 0.8405303955078125\nBatch loss: 0.09354925714433193\nBatch loss: 0.9276094287633896\nBatch loss: 0.6737690418958664\nBatch loss: 0.7066192477941513\nBatch loss: 0.1963367499411106\nBatch loss: 0.38019564002752304\nBatch loss: 0.659027099609375\nBatch loss: 0.9417381137609482\nBatch loss: 0.509587787091732\nBatch loss: 0.36626625806093216\nBatch loss: 0.5101661756634712\nBatch loss: 0.7246933132410049\nBatch loss: 0.8265800774097443\nBatch loss: 0.8455496281385422\nBatch loss: 0.7301654666662216\nBatch loss: 0.318111889064312\nBatch loss: 0.14538384042680264\nBatch loss: 0.3104248084127903\nBatch loss: 0.6658401340246201\nBatch loss: 0.5874805524945259\nBatch loss: 0.2571563795208931\nBatch loss: 0.2817411534488201\nBatch loss: 1.0161743313074112\nBatch loss: 0.8787184208631516\nBatch loss: 1.6254711151123047\nBatch loss: 1.3029460608959198\nBatch loss: 0.06000709719955921\nBatch loss: 0.20682334899902344\nBatch loss: 1.2189941853284836\nBatch loss: 0.5221352726221085\nBatch loss: 0.5751972272992134\nBatch loss: 0.5768594890832901\nBatch loss: 0.5614414438605309\nBatch loss: 1.8145890533924103\nBatch loss: 0.7766151428222656\nBatch loss: 0.5128584057092667\nBatch loss: 0.13563252054154873\nBatch loss: 0.5141181871294975\nBatch loss: 0.34856226295232773\nBatch loss: 0.3814573213458061\nBatch loss: 0.49986839294433594\nBatch loss: 0.20690251141786575\nBatch loss: 1.1015873402357101\nBatch loss: 0.6757736206054688\nBatch loss: 0.6630440056324005\nBatch loss: 0.34192275255918503\nBatch loss: 0.2772684209048748\nBatch loss: 1.1260052025318146\nBatch loss: 0.9401054680347443\nBatch loss: 0.3220539167523384\nBatch loss: 0.4634275659918785\nBatch loss: 0.6661806255578995\nBatch loss: 0.2517280541360378\nBatch loss: 0.7175598293542862\nBatch loss: 0.6796379387378693\nBatch loss: 0.49430467188358307\nBatch loss: 0.6648197025060654\nBatch loss: 0.4030590131878853\nBatch loss: 0.5652542039752007\nBatch loss: 0.19439984112977982\nBatch loss: 0.5642395094037056\nBatch loss: 0.4374236986041069\nBatch loss: 0.90301513671875\nBatch loss: 0.5607795715332031\nBatch loss: 0.2992973290383816\nBatch loss: 0.45176316052675247\nBatch loss: 0.9361114352941513\nBatch loss: 0.6907129287719727\nBatch loss: 0.20916176959872246\nBatch loss: 0.1524148043245077\nBatch loss: 0.8668556064367294\nBatch loss: 0.6570625305175781\nBatch loss: 0.5232343822717667\nBatch loss: 0.5556373670697212\nBatch loss: 0.7768707722425461\nBatch loss: 0.7824649661779404\nBatch loss: 0.5690613016486168\nBatch loss: 0.5587053298950195\nBatch loss: 0.30126189813017845\nBatch loss: 0.2998838387429714\nBatch loss: 0.4688110575079918\nBatch loss: 0.5321388319134712\nBatch loss: 0.4352455213665962\nBatch loss: 0.5231037363409996\nBatch loss: 0.4502296447753906\nBatch loss: 0.2670612372457981\nBatch loss: 0.5679884180426598\nBatch loss: 0.9549140930175781\nBatch loss: 1.0764370113611221\nBatch loss: 0.6282806396484375\nBatch loss: 0.30824853107333183\nBatch loss: 0.39756011217832565\nBatch loss: 0.5564498901367188\nBatch loss: 0.8751554787158966\nBatch loss: 1.2383003532886505\nBatch loss: 0.6411819905042648\nBatch loss: 0.12514591217041016\nBatch loss: 1.1905288696289062\nBatch loss: 0.15888214111328125\nBatch loss: 0.33416748046875\nBatch loss: 0.5355262756347656\nBatch loss: 0.30881309881806374\nBatch loss: 0.8612918853759766\nBatch loss: 0.41213609278202057\nBatch loss: 0.39148714393377304\nBatch loss: 0.6943684071302414\nBatch loss: 0.5583152920007706\nBatch loss: 0.4452209547162056\nBatch loss: 0.4355163499712944\nBatch loss: 0.9563751518726349\nBatch loss: 0.24009419605135918\nBatch loss: 0.30089760199189186\nBatch loss: 0.6035137176513672\nBatch loss: 0.4830188676714897\nBatch loss: 0.5353584513068199\nBatch loss: 0.5282364040613174\nBatch loss: 0.4144010692834854\nBatch loss: 0.7682495564222336\nBatch loss: 0.4773712158203125\nBatch loss: 0.31750869005918503\nBatch loss: 0.37933923304080963\nBatch loss: 0.41104506701231003\nBatch loss: 0.5886688455939293\nBatch loss: 0.35619355738162994\nBatch loss: 0.5756759643554688\nBatch loss: 0.2003936842083931\nBatch loss: 1.0027885437011719\nBatch loss: 0.6121959909796715\nBatch loss: 0.19845008850097656\nBatch loss: 0.9926681965589523\nBatch loss: 0.8735046535730362\nBatch loss: 0.3006744384765625\nBatch loss: 0.21467780694365501\nBatch loss: 0.5559539794921875\nBatch loss: 0.9566650539636612\nBatch loss: 0.5860061571002007\nBatch loss: 0.38357924669981003\nBatch loss: 0.5451488494873047\nBatch loss: 0.4165821149945259\nBatch loss: 0.34470368176698685\nBatch loss: 0.5770263820886612\nBatch loss: 0.44019319117069244\nBatch loss: 0.27383042499423027\nBatch loss: 0.43407250195741653\nBatch loss: 0.3768463060259819\nBatch loss: 0.8173675835132599\nBatch loss: 1.0582466423511505\nBatch loss: 0.5636405944824219\nBatch loss: 0.3345794603228569\nBatch loss: 0.4408111795783043\nBatch loss: 0.2743225172162056\nBatch loss: 0.6940612941980362\nBatch loss: 0.4392986372113228\nBatch loss: 0.5151519924402237\nBatch loss: 0.6184310838580132\nBatch loss: 0.7414551079273224\nBatch loss: 0.9011059254407883\nBatch loss: 0.2887535095214844\nBatch loss: 0.4240722581744194\nBatch loss: 0.2935829199850559\nBatch loss: 0.7346916198730469\nBatch loss: 0.23595048114657402\nBatch loss: 1.289510726928711\nBatch loss: 0.6067466735839844\nBatch loss: 0.39016153663396835\nBatch loss: 0.692874938249588\nBatch loss: 0.5628376081585884\nBatch loss: 0.357576385140419\nBatch loss: 0.4917183145880699\nBatch loss: 0.28920842334628105\nBatch loss: 0.14149094000458717\nBatch loss: 0.5572281032800674\nBatch loss: 0.6882324069738388\nBatch loss: 0.4654083400964737\nBatch loss: 0.5894660949707031\nBatch loss: 0.29837800189852715\nBatch loss: 0.9847793728113174\nBatch loss: 0.2871742285788059\nBatch loss: 0.4077472910284996\nBatch loss: 0.6131515651941299\nBatch loss: 0.6056766584515572\nBatch loss: 0.7098197937011719\nBatch loss: 0.5867309495806694\nBatch loss: 0.18301773816347122\nBatch loss: 0.2256469801068306\nBatch loss: 0.5500488355755806\nBatch loss: 0.49674034118652344\nBatch loss: 0.41446875780820847\nBatch loss: 0.1705455780029297\nBatch loss: 0.4210071638226509\nBatch loss: 0.2980632893741131\nBatch loss: 0.4999237135052681\nBatch loss: 0.8772430568933487\nBatch loss: 0.48519134521484375\nBatch loss: 0.43935395777225494\nBatch loss: 0.47522734850645065\nBatch loss: 0.8246555179357529\nBatch loss: 0.295764934271574\nBatch loss: 0.622311607003212\nBatch loss: 0.6184501573443413\nBatch loss: 0.2924156188964844\nBatch loss: 0.35895537585020065\nBatch loss: 0.7337494194507599\nBatch loss: 0.6204834207892418\nBatch loss: 0.48095703125\nBatch loss: 0.44338226318359375\nBatch loss: 0.5920162424445152\nBatch loss: 0.7435226440429688\nBatch loss: 0.5515174940228462\nBatch loss: 0.5717163160443306\nBatch loss: 0.11171722784638405\nBatch loss: 0.7003288716077805\nBatch loss: 0.3244180604815483\nBatch loss: 0.38559723645448685\nBatch loss: 0.3923005983233452\nBatch loss: 0.6172409281134605\nBatch loss: 0.7907714694738388\nBatch loss: 0.8054733276367188\nBatch loss: 0.34905243664979935\nBatch loss: 0.48323821276426315\nBatch loss: 0.64727783203125\nBatch loss: 0.1546726282685995\nBatch loss: 0.4679107666015625\nBatch loss: 0.5588493496179581\nBatch loss: 0.8643341064453125\nBatch loss: 0.6069030985236168\nBatch loss: 0.5125274881720543\nBatch loss: 0.3119545057415962\nBatch loss: 0.604432113468647\nBatch loss: 0.7123412936925888\nBatch loss: 0.6503982841968536\nBatch loss: 0.22298766300082207\nBatch loss: 0.29256630688905716\nBatch loss: 1.0430927574634552\nBatch loss: 0.3048133850097656\nBatch loss: 0.27863694354891777\nBatch loss: 0.34009553492069244\nBatch loss: 0.41941072791814804\nBatch loss: 0.8487138897180557\nBatch loss: 0.414186492562294\nBatch loss: 0.37468530237674713\nBatch loss: 0.24496078491210938\nBatch loss: 0.2196655236184597\nBatch loss: 0.17856597900390625\nBatch loss: 0.5501861497759819\nBatch loss: 0.24672318249940872\nBatch loss: 1.074722334742546\nBatch loss: 0.46262361109256744\nBatch loss: 0.23031806573271751\nBatch loss: 0.3180808946490288\nBatch loss: 0.12050819583237171\nBatch loss: 0.5127878114581108\nBatch loss: 0.4988670349121094\nBatch loss: 0.4532966762781143\nBatch loss: 0.3526649624109268\nBatch loss: 0.6119537353515625\nBatch loss: 0.21892929449677467\nBatch loss: 0.7257652282714844\nBatch loss: 0.35372164100408554\nBatch loss: 0.6574630737304688\nBatch loss: 0.6306534260511398\nBatch loss: 0.43184947222471237\nBatch loss: 0.6714630126953125\nBatch loss: 0.5001449584960938\nBatch loss: 0.43568801134824753\nBatch loss: 0.19486427307128906\nBatch loss: 0.6825142353773117\nBatch loss: 0.5740242078900337\nBatch loss: 0.5770950391888618\nBatch loss: 0.3032112121582031\nBatch loss: 0.39865877479314804\nBatch loss: 0.672149658203125\nBatch loss: 0.29192162677645683\nBatch loss: 0.17612267285585403\nBatch loss: 0.4320049285888672\nBatch loss: 0.30190085992217064\nBatch loss: 0.21216774359345436\nBatch loss: 0.45250702649354935\nBatch loss: 0.7332038879394531\nBatch loss: 0.3490142896771431\nBatch loss: 0.19239425659179688\nBatch loss: 0.47072600573301315\nBatch loss: 0.5743799358606339\nBatch loss: 0.28105927631258965\nBatch loss: 0.3757820278406143\nBatch loss: 0.38838960230350494\nBatch loss: 0.4441986232995987\nBatch loss: 0.5849533155560493\nBatch loss: 0.2594432793557644\nBatch loss: 0.443267822265625\nBatch loss: 0.23477936163544655\nBatch loss: 0.3794097900390625\nBatch loss: 0.2121582068502903\nBatch loss: 0.7424316555261612\nBatch loss: 0.4200134426355362\nBatch loss: 1.024017333984375\nBatch loss: 0.40229033678770065\nBatch loss: 0.12879944406449795\nBatch loss: 0.36753464490175247\nBatch loss: 0.7101517170667648\nBatch loss: 0.27359772473573685\nBatch loss: 0.32511141151189804\nBatch loss: 0.639280304312706\nBatch loss: 0.39031218737363815\nBatch loss: 0.25389863178133965\nBatch loss: 0.27825547382235527\nBatch loss: 0.3750629350543022\nBatch loss: 1.1321449279785156\nBatch loss: 0.3708076477050781\nBatch loss: 0.6565036624670029\nBatch loss: 0.2903595007956028\nBatch loss: 0.6108856201171875\nBatch loss: 0.24641705676913261\nBatch loss: 0.25029754266142845\nBatch loss: 0.31890869140625\nBatch loss: 0.3656310960650444\nBatch loss: 0.5100708082318306\nBatch loss: 0.22687245160341263\nBatch loss: 0.3523597866296768\nBatch loss: 0.34654807299375534\nBatch loss: 0.5624084547162056\nBatch loss: 0.4485616832971573\nBatch loss: 1.147613525390625\nBatch loss: 0.491059310734272\nBatch loss: 0.32164763659238815\nBatch loss: 0.3356952592730522\nBatch loss: 0.14127731323242188\nBatch loss: 0.10859871283173561\nBatch loss: 0.2578468434512615\nBatch loss: 0.4373645782470703\nBatch loss: 0.3980865702033043\nBatch loss: 0.25988006964325905\nBatch loss: 0.4414672777056694\nBatch loss: 0.42385101318359375\nBatch loss: 0.6593628227710724\nBatch loss: 0.7616844028234482\nBatch loss: 0.34081269055604935\nBatch loss: 0.26863861829042435\nBatch loss: 0.16734695062041283\nBatch loss: 0.5188589170575142\nBatch loss: 0.5062999948859215\nBatch loss: 0.6497345119714737\nBatch loss: 0.41617393493652344\nBatch loss: 0.2791481092572212\nBatch loss: 0.6753768771886826\nBatch loss: 0.1902008056640625\nBatch loss: 0.6319313496351242\nBatch loss: 0.33095933496952057\nBatch loss: 0.3055095672607422\nBatch loss: 0.4129066690802574\nBatch loss: 0.21044159308075905\nBatch loss: 0.6758232414722443\nBatch loss: 0.5511932447552681\nBatch loss: 0.2283020131289959\nBatch loss: 0.44796276837587357\nBatch loss: 0.2478942833840847\nBatch loss: 1.1034546047449112\nBatch loss: 0.263214111328125\nBatch loss: 0.9146728366613388\nBatch loss: 0.7088851928710938\nBatch loss: 0.5675773695111275\nBatch loss: 0.32981108874082565\nBatch loss: 0.4181218147277832\nBatch loss: 0.33753015100955963\nBatch loss: 0.5072479322552681\nBatch loss: 0.6020870432257652\nBatch loss: 0.5505247041583061\nBatch loss: 0.537872314453125\nBatch loss: 0.19894981756806374\nBatch loss: 0.7462692260742188\nBatch loss: 1.0094757378101349\nBatch loss: 0.227813720703125\nBatch loss: 0.5062256008386612\nBatch loss: 0.14202690683305264\nBatch loss: 0.17892075702548027\nBatch loss: 0.7952957600355148\nBatch loss: 0.781518965959549\nBatch loss: 0.5642929300665855\nBatch loss: 0.16884613782167435\nBatch loss: 0.1280899066478014\nBatch loss: 0.19860459491610527\nBatch loss: 0.2389240264892578\nBatch loss: 0.3439006954431534\nBatch loss: 0.6906156986951828\nBatch loss: 0.9026527404785156\nBatch loss: 0.44420626014471054\nBatch loss: 0.3387890011072159\nBatch loss: 0.29650498181581497\nBatch loss: 0.3389129787683487\nBatch loss: 0.14742279425263405\nBatch loss: 0.3949851915240288\nBatch loss: 0.37213515490293503\nBatch loss: 0.5435257032513618\nBatch loss: 0.39942361414432526\nBatch loss: 0.6894321739673615\nBatch loss: 0.7071381062269211\nBatch loss: 0.774105116724968\nBatch loss: 0.684904083609581\nBatch loss: 0.3006553649902344\nBatch loss: 0.3372812271118164\nBatch loss: 0.13119506649672985\nBatch loss: 0.2911376953125\nBatch loss: 0.3870086744427681\nBatch loss: 0.3599243238568306\nBatch loss: 0.2992858923971653\nBatch loss: 0.3571663051843643\nBatch loss: 0.39248276501893997\nBatch loss: 0.6848154217004776\nBatch loss: 0.39591025561094284\nBatch loss: 0.9152469784021378\nBatch loss: 0.22136593237519264\nBatch loss: 0.465913787484169\nBatch loss: 0.2881774865090847\nBatch loss: 0.56515883654356\nBatch loss: 0.6622200459241867\nBatch loss: 0.3780803829431534\nBatch loss: 0.41389085352420807\nBatch loss: 0.5734510347247124\nBatch loss: 0.9046325832605362\nBatch loss: 0.31467437744140625\nBatch loss: 0.18149947747588158\nBatch loss: 0.33033277839422226\nBatch loss: 0.7125987857580185\nBatch loss: 0.19836807623505592\nBatch loss: 0.5543842539191246\nBatch loss: 0.5070438608527184\nBatch loss: 1.0466670989990234\nBatch loss: 0.5188865587115288\nBatch loss: 0.8910446614027023\nBatch loss: 0.39911460131406784\nBatch loss: 0.38836289197206497\nBatch loss: 0.21739769726991653\nBatch loss: 0.4542083665728569\nBatch loss: 0.34245681017637253\nBatch loss: 0.39720822125673294\nBatch loss: 0.3316841274499893\nBatch loss: 0.6569500267505646\nBatch loss: 0.3597870096564293\nBatch loss: 0.37525177001953125\nBatch loss: 0.43975066393613815\nBatch loss: 0.44181060045957565\nBatch loss: 0.41756439954042435\nBatch loss: 0.50356674939394\nBatch loss: 0.45158959925174713\nBatch loss: 0.6042098999023438\nBatch loss: 0.8526688069105148\nBatch loss: 0.8391571044921875\nBatch loss: 0.5806884914636612\nBatch loss: 0.35125162452459335\nBatch loss: 0.7095032185316086\nBatch loss: 0.21200751885771751\nBatch loss: 0.16357040032744408\nBatch loss: 1.0450591892004013\nBatch loss: 0.46290017664432526\nBatch loss: 0.5785238742828369\nBatch loss: 0.45496847480535507\nBatch loss: 0.23812485858798027\nBatch loss: 0.17399216070771217\nBatch loss: 0.41598226875066757\nBatch loss: 0.668003112077713\nBatch loss: 0.6062927469611168\nBatch loss: 0.21610260009765625\nBatch loss: 0.33406831324100494\nBatch loss: 0.25300407782197\nBatch loss: 0.7577448338270187\nBatch loss: 0.32503511756658554\nBatch loss: 0.4974403604865074\nBatch loss: 0.5276565626263618\nBatch loss: 0.3283844143152237\nBatch loss: 0.3453082963824272\nBatch loss: 1.3398896157741547\nBatch loss: 0.32535936683416367\nBatch loss: 0.3059234656393528\nBatch loss: 0.2644043043255806\nBatch loss: 0.3950614854693413\nBatch loss: 0.49312688410282135\nBatch loss: 0.42393971234560013\nBatch loss: 0.5800514295697212\nBatch loss: 0.49886513501405716\nBatch loss: 0.45960426330566406\nBatch loss: 0.5735340341925621\nBatch loss: 0.48266220837831497\nBatch loss: 0.3470725938677788\nBatch loss: 0.8072433620691299\nBatch loss: 0.5930404737591743\nBatch loss: 0.540107749402523\nBatch loss: 0.3944244608283043\nBatch loss: 0.34260939806699753\nBatch loss: 0.4863929748535156\nBatch loss: 0.28078651055693626\nBatch loss: 0.3684864193201065\nBatch loss: 0.5032653734087944\nBatch loss: 0.5666399002075195\nBatch loss: 0.5583420023322105\nBatch loss: 0.736846923828125\nBatch loss: 0.409088134765625\nBatch loss: 0.2735328674316406\nBatch loss: 0.3029518201947212\nBatch loss: 0.2769165113568306\nBatch loss: 0.4158611223101616\nBatch loss: 0.6286316365003586\nBatch loss: 0.6057090684771538\nBatch loss: 0.3389778360724449\nBatch loss: 0.7239494472742081\nBatch loss: 0.3636036068201065\nBatch loss: 0.7043609768152237\nBatch loss: 0.2722053602337837\nBatch loss: 0.3568458557128906\nBatch loss: 0.39823152124881744\nBatch loss: 0.4451293870806694\nBatch loss: 0.3650856018066406\nBatch loss: 0.4404449462890625\nBatch loss: 0.6495094299316406\nBatch loss: 0.5551376566290855\nBatch loss: 0.6462860107421875\nBatch loss: 0.3303647041320801\nBatch loss: 0.6655426323413849\nBatch loss: 0.5899124220013618\nBatch loss: 0.35100556910037994\nBatch loss: 0.3647899627685547\nBatch loss: 0.8036193996667862\nBatch loss: 0.5266838148236275\nBatch loss: 0.7538451999425888\nBatch loss: 0.6576309353113174\nBatch loss: 0.9357833862304688\nBatch loss: 0.31591035425662994\nBatch loss: 0.43976593762636185\nBatch loss: 0.27303313836455345\nBatch loss: 0.45694734901189804\nBatch loss: 0.29887009412050247\nBatch loss: 0.5957336351275444\nBatch loss: 0.6522446125745773\nBatch loss: 0.4576568678021431\nBatch loss: 0.2721252478659153\nBatch loss: 0.7975311577320099\nBatch loss: 0.5034485086798668\nBatch loss: 0.28584957122802734\nBatch loss: 0.3641548380255699\nBatch loss: 0.3342895582318306\nBatch loss: 0.2825584448873997\nBatch loss: 0.5979976803064346\nBatch loss: 0.5723190307617188\nBatch loss: 0.22334670647978783\nBatch loss: 0.19144058227539062\nBatch loss: 0.5711841583251953\nBatch loss: 0.4710054397583008\nBatch loss: 0.7084598392248154\nBatch loss: 0.7367324829101562\nBatch loss: 0.47291185706853867\nBatch loss: 0.4887256771326065\nBatch loss: 0.6688842922449112\nBatch loss: 0.4855918884277344\nBatch loss: 0.6687946617603302\nBatch loss: 0.5601349100470543\nBatch loss: 0.5211105570197105\nBatch loss: 0.5228424072265625\nBatch loss: 0.6702270358800888\nBatch loss: 0.51116943359375\nBatch loss: 0.8248367160558701\nBatch loss: 0.20351791754364967\nBatch loss: 0.6157827377319336\nBatch loss: 0.24547386914491653\nBatch loss: 0.4534492641687393\nBatch loss: 0.4869117960333824\nBatch loss: 0.4787445068359375\nBatch loss: 0.5398712307214737\nBatch loss: 0.5030899122357368\nBatch loss: 0.3251037746667862\nBatch loss: 0.4328003153204918\nBatch loss: 0.4475860670208931\nBatch loss: 0.30539704486727715\nBatch loss: 0.44109344482421875\nBatch loss: 0.38446810096502304\nBatch loss: 0.45627977699041367\nBatch loss: 0.5502242967486382\nBatch loss: 0.3114948235452175\nBatch loss: 0.5082511901855469\nBatch loss: 0.886993408203125\nBatch loss: 0.16802215948700905\nBatch loss: 0.29346466064453125\nBatch loss: 0.43799590319395065\nBatch loss: 0.37558745592832565\nBatch loss: 0.547737143933773\nBatch loss: 0.4115600511431694\nBatch loss: 0.8511886745691299\nBatch loss: 0.27880096808075905\nBatch loss: 0.26606369763612747\nBatch loss: 0.2656097523868084\nBatch loss: 0.3767585754394531\nBatch loss: 0.592830665409565\nBatch loss: 0.42109299451112747\nBatch loss: 0.2101440541446209\nBatch loss: 0.7579574733972549\nBatch loss: 0.21285248920321465\nBatch loss: 0.6162719801068306\nBatch loss: 0.16918564215302467\nBatch loss: 0.36066439002752304\nBatch loss: 0.5617256090044975\nBatch loss: 0.7072849571704865\nBatch loss: 0.1912364922463894\nBatch loss: 0.26622772216796875\nBatch loss: 0.42346954345703125\nBatch loss: 0.5335388332605362\nBatch loss: 0.5425834655761719\nBatch loss: 0.4199514538049698\nBatch loss: 0.8256454765796661\nBatch loss: 0.10308265686035156\nBatch loss: 0.14582062140107155\nBatch loss: 0.7975921779870987\nBatch loss: 0.4215049743652344\nBatch loss: 0.3640556335449219\nBatch loss: 0.22388076409697533\nBatch loss: 0.4979419708251953\nBatch loss: 0.4350585862994194\nBatch loss: 0.7797107845544815\nBatch loss: 0.6279449909925461\nBatch loss: 0.2527961693704128\nBatch loss: 0.3812256082892418\nBatch loss: 0.8102741092443466\nBatch loss: 0.4629240185022354\nBatch loss: 0.17738724127411842\nBatch loss: 0.39655305445194244\nBatch loss: 0.33966064453125\nBatch loss: 0.3710613399744034\nBatch loss: 0.12846278958022594\nBatch loss: 0.27971649542450905\nBatch loss: 0.37056397646665573\nBatch loss: 0.5432167276740074\nBatch loss: 0.5642471462488174\nBatch loss: 0.5064163357019424\nBatch loss: 0.4438171535730362\nBatch loss: 0.53680419921875\nBatch loss: 0.5446319654583931\nBatch loss: 0.6226348876953125\nBatch loss: 0.5264968797564507\nBatch loss: 0.3014793433248997\nBatch loss: 0.33321764320135117\nBatch loss: 0.3548431396484375\nBatch loss: 0.2105712890625\nBatch loss: 0.26102447882294655\nBatch loss: 0.520172119140625\nBatch loss: 0.20033741369843483\nBatch loss: 0.4960327222943306\nBatch loss: 0.6003379821777344\nBatch loss: 0.6593360751867294\nBatch loss: 0.6289710849523544\nBatch loss: 0.6354675441980362\nBatch loss: 0.6513290852308273\nBatch loss: 0.297290813177824\nBatch loss: 0.36548424512147903\nBatch loss: 0.2974414825439453\nBatch loss: 1.2599335610866547\nBatch loss: 0.7210006564855576\nBatch loss: 0.16064835712313652\nBatch loss: 0.4933471605181694\nBatch loss: 0.8807831257581711\nBatch loss: 0.7114829868078232\nBatch loss: 0.33080291002988815\nBatch loss: 0.5136613920331001\nBatch loss: 0.1428775768727064\nBatch loss: 0.13995218090713024\nBatch loss: 0.5335884168744087\nBatch loss: 0.7367515563964844\nBatch loss: 1.105804443359375\nBatch loss: 0.4828796535730362\nBatch loss: 0.22305965423583984\nBatch loss: 0.3531622886657715\nBatch loss: 0.48057176172733307\nBatch loss: 0.7717609405517578\nBatch loss: 0.2746315114200115\nBatch loss: 0.34343626350164413\nBatch loss: 0.4983215406537056\nBatch loss: 0.8869399875402451\nBatch loss: 0.20583344623446465\nBatch loss: 0.20186996087431908\nBatch loss: 0.5601882934570312\nBatch loss: 0.3283100202679634\nBatch loss: 0.895359069108963\nBatch loss: 0.23928070440888405\nBatch loss: 0.18273545429110527\nBatch loss: 0.9750747680664062\nBatch loss: 0.35802461206912994\nBatch loss: 0.3476428985595703\nBatch loss: 0.5593566969037056\nBatch loss: 0.8984623104333878\nBatch loss: 0.4489250108599663\nBatch loss: 0.43801307678222656\nBatch loss: 0.37022780627012253\nBatch loss: 0.7573395222425461\nBatch loss: 0.40801621973514557\nBatch loss: 0.6191062927246094\nBatch loss: 0.18302727490663528\nBatch loss: 0.2885170094668865\nBatch loss: 0.507606528699398\nBatch loss: 0.2434234693646431\nBatch loss: 0.7634048908948898\nBatch loss: 0.8706016838550568\nBatch loss: 0.40146637707948685\nBatch loss: 0.3532257303595543\nBatch loss: 1.040390059351921\nBatch loss: 0.6438102573156357\nBatch loss: 0.35497285425662994\nBatch loss: 0.7320385426282883\nBatch loss: 0.4939880594611168\nBatch loss: 0.47774888575077057\nBatch loss: 0.7825393974781036\nBatch loss: 0.2501087263226509\nBatch loss: 0.2503681182861328\nBatch loss: 0.42873669415712357\nBatch loss: 0.4230079799890518\nBatch loss: 0.30511094257235527\nBatch loss: 0.37290193140506744\nBatch loss: 0.4076986387372017\nBatch loss: 0.4598121717572212\nBatch loss: 0.6543274223804474\nBatch loss: 0.44637300074100494\nBatch loss: 0.3233451768755913\nBatch loss: 0.41446685791015625\nBatch loss: 0.4015464708209038\nBatch loss: 0.3461914137005806\nBatch loss: 0.7741623371839523\nBatch loss: 0.18081093207001686\nBatch loss: 0.19484711810946465\nBatch loss: 0.23896027356386185\nBatch loss: 0.39464186877012253\nBatch loss: 0.2286052703857422\nBatch loss: 0.17910003662109375\nBatch loss: 0.3648719936609268\nBatch loss: 0.5875644832849503\nBatch loss: 0.2699127234518528\nBatch loss: 0.24580955505371094\nBatch loss: 0.18213843926787376\nBatch loss: 0.2867546118795872\nBatch loss: 0.5245933681726456\nBatch loss: 0.21991539746522903\nBatch loss: 0.3937111049890518\nBatch loss: 0.24174118414521217\nBatch loss: 0.4660797119140625\nBatch loss: 0.43903540819883347\nBatch loss: 0.3411712870001793\nBatch loss: 0.2866020239889622\nBatch loss: 0.5287208780646324\nBatch loss: 0.7897033542394638\nBatch loss: 0.5704651027917862\nBatch loss: 0.39995577186346054\nBatch loss: 0.5981826782226562\nBatch loss: 0.753910094499588\nBatch loss: 0.6701965630054474\nBatch loss: 0.2675781212747097\nBatch loss: 0.3732757642865181\nBatch loss: 0.6271534413099289\nBatch loss: 0.4736804962158203\nBatch loss: 0.14659595675766468\nBatch loss: 0.51028061658144\nBatch loss: 0.4920654371380806\nBatch loss: 0.5663986131548882\nBatch loss: 0.4721374437212944\nBatch loss: 0.3018817864358425\nBatch loss: 0.6273289024829865\nBatch loss: 0.4302816465497017\nBatch loss: 0.5391998216509819\nBatch loss: 0.47862816601991653\nBatch loss: 0.6575804203748703\nBatch loss: 0.5184464529156685\nBatch loss: 0.1873455010354519\nBatch loss: 0.2434692345559597\nBatch loss: 0.42135145515203476\nBatch loss: 0.8608360588550568\nBatch loss: 0.6925735622644424\nBatch loss: 0.39913561195135117\nBatch loss: 0.29615020379424095\nBatch loss: 0.34076690673828125\nEpoch 3/7\nBatch loss: 0.08335876278579235\nBatch loss: 0.3914489969611168\nBatch loss: 0.8201513439416885\nBatch loss: 1.003345474600792\nBatch loss: 0.46935465186834335\nBatch loss: 0.4256286844611168\nBatch loss: 0.2864265441894531\nBatch loss: 0.39527129381895065\nBatch loss: 0.2567863464355469\nBatch loss: 0.4129009321331978\nBatch loss: 0.32718658447265625\nBatch loss: 0.3337536007165909\nBatch loss: 0.41638948023319244\nBatch loss: 0.5211257934570312\nBatch loss: 0.48588942736387253\nBatch loss: 0.06913280580192804\nBatch loss: 0.29472803696990013\nBatch loss: 0.4534301906824112\nBatch loss: 0.30463123694062233\nBatch loss: 0.3959045559167862\nBatch loss: 0.584205649793148\nBatch loss: 0.43576430529356003\nBatch loss: 0.7095909118652344\nBatch loss: 0.20598126575350761\nBatch loss: 0.6458206474781036\nBatch loss: 0.36255646497011185\nBatch loss: 0.47089386731386185\nBatch loss: 0.3027305565774441\nBatch loss: 0.32114792615175247\nBatch loss: 0.4989776760339737\nBatch loss: 0.5928344652056694\nBatch loss: 0.28717994689941406\nBatch loss: 0.3583526611328125\nBatch loss: 0.4631815105676651\nBatch loss: 0.6375847011804581\nBatch loss: 0.7519721984863281\nBatch loss: 0.44301223009824753\nBatch loss: 0.29123736545443535\nBatch loss: 0.5718803405761719\nBatch loss: 0.9151935577392578\nBatch loss: 0.33621978014707565\nBatch loss: 0.3103809431195259\nBatch loss: 0.5210704728960991\nBatch loss: 0.2734050713479519\nBatch loss: 0.30001068487763405\nBatch loss: 0.2013387717306614\nBatch loss: 0.41127968579530716\nBatch loss: 0.7601337879896164\nBatch loss: 0.48352815210819244\nBatch loss: 0.31745340675115585\nBatch loss: 0.26406098157167435\nBatch loss: 0.6146154552698135\nBatch loss: 0.46813394874334335\nBatch loss: 0.4646625742316246\nBatch loss: 0.30994607135653496\nBatch loss: 0.2299194410443306\nBatch loss: 0.3992138057947159\nBatch loss: 0.44188499450683594\nBatch loss: 0.4105224832892418\nBatch loss: 0.4406433179974556\nBatch loss: 0.2145557478070259\nBatch loss: 0.37533190101385117\nBatch loss: 0.34148789942264557\nBatch loss: 0.42780686169862747\nBatch loss: 0.518619529902935\nBatch loss: 0.2369251288473606\nBatch loss: 0.8374805748462677\nBatch loss: 0.4590015485882759\nBatch loss: 0.2422027662396431\nBatch loss: 0.3690490871667862\nBatch loss: 0.5644188076257706\nBatch loss: 0.28469277545809746\nBatch loss: 0.20335054025053978\nBatch loss: 0.14555740170180798\nBatch loss: 0.2316589467227459\nBatch loss: 0.4316139221191406\nBatch loss: 0.7203502953052521\nBatch loss: 0.32996274530887604\nBatch loss: 0.3101196326315403\nBatch loss: 0.6006088480353355\nBatch loss: 0.8052139729261398\nBatch loss: 0.2205371856689453\nBatch loss: 0.2799644507467747\nBatch loss: 0.33960726112127304\nBatch loss: 0.48352815210819244\nBatch loss: 0.40351152420043945\nBatch loss: 0.1310520153492689\nBatch loss: 0.244324691593647\nBatch loss: 0.39339449256658554\nBatch loss: 0.8574600517749786\nBatch loss: 0.22465897724032402\nBatch loss: 0.5120620876550674\nBatch loss: 0.4123726114630699\nBatch loss: 0.2172870747745037\nBatch loss: 0.7842025905847549\nBatch loss: 0.32076168805360794\nBatch loss: 0.26660919189453125\nBatch loss: 0.3351478651165962\nBatch loss: 0.5183219909667969\nBatch loss: 0.1926136016845703\nBatch loss: 0.7209052890539169\nBatch loss: 0.2032165601849556\nBatch loss: 0.16044998541474342\nBatch loss: 0.5042629316449165\nBatch loss: 0.24069881066679955\nBatch loss: 0.766422301530838\nBatch loss: 0.36284636706113815\nBatch loss: 0.27889443561434746\nBatch loss: 0.24524688720703125\nBatch loss: 0.41701506823301315\nBatch loss: 0.37323426455259323\nBatch loss: 0.5507564544677734\nBatch loss: 0.8907794952392578\nBatch loss: 0.22677231580018997\nBatch loss: 0.32997895032167435\nBatch loss: 0.2574348449707031\nBatch loss: 0.6704635918140411\nBatch loss: 0.7760448753833771\nBatch loss: 0.4015922546386719\nBatch loss: 1.148945838212967\nBatch loss: 0.3600120544433594\nBatch loss: 0.5685539171099663\nBatch loss: 0.12078285217285156\nBatch loss: 0.15671109780669212\nBatch loss: 0.26535987854003906\nBatch loss: 0.38993455469608307\nBatch loss: 0.2379322052001953\nBatch loss: 0.14013290405273438\nBatch loss: 0.3311328962445259\nBatch loss: 0.4917879030108452\nBatch loss: 0.4428711161017418\nBatch loss: 0.4940643534064293\nBatch loss: 0.6838951259851456\nBatch loss: 0.4228248819708824\nBatch loss: 0.5695228651165962\nBatch loss: 0.32755281776189804\nBatch loss: 0.2962322346866131\nBatch loss: 0.4875297471880913\nBatch loss: 0.5177459865808487\nBatch loss: 0.11712837032973766\nBatch loss: 0.3796539455652237\nBatch loss: 0.28937626630067825\nBatch loss: 0.5308876186609268\nBatch loss: 0.6879158318042755\nBatch loss: 0.881679579615593\nBatch loss: 0.4512472078204155\nBatch loss: 0.21830081939697266\nBatch loss: 0.4625816270709038\nBatch loss: 0.18157958984375\nBatch loss: 0.3093566931784153\nBatch loss: 0.5881862714886665\nBatch loss: 0.1106948871165514\nBatch loss: 0.11760330758988857\nBatch loss: 0.7302894443273544\nBatch loss: 1.0464096069335938\nBatch loss: 0.3171348571777344\nBatch loss: 0.1421146374195814\nBatch loss: 0.48682596534490585\nBatch loss: 0.16084862872958183\nBatch loss: 0.42513277381658554\nBatch loss: 0.6547393649816513\nBatch loss: 0.26398468762636185\nBatch loss: 0.2753028832376003\nBatch loss: 0.23448562249541283\nBatch loss: 0.9623565524816513\nBatch loss: 0.1436691265553236\nBatch loss: 0.32764818519353867\nBatch loss: 0.11972618289291859\nBatch loss: 0.5592746660113335\nBatch loss: 0.49770545214414597\nBatch loss: 0.2898435667157173\nBatch loss: 0.16610527411103249\nBatch loss: 0.6728458404541016\nBatch loss: 0.22746849805116653\nBatch loss: 0.5553054809570312\nBatch loss: 0.2495288848876953\nBatch loss: 0.23966217413544655\nBatch loss: 0.24120235815644264\nBatch loss: 0.3261871263384819\nBatch loss: 0.7221259921789169\nBatch loss: 0.19312335178256035\nBatch loss: 0.23375893011689186\nBatch loss: 0.1236496027559042\nBatch loss: 0.08677005767822266\nBatch loss: 0.8049564808607101\nBatch loss: 0.19852925091981888\nBatch loss: 0.361175537109375\nBatch loss: 0.4069213941693306\nBatch loss: 0.7893591374158859\nBatch loss: 0.44582750648260117\nBatch loss: 0.29252244159579277\nBatch loss: 0.41076090186834335\nBatch loss: 0.12003421783447266\nBatch loss: 0.13246155343949795\nBatch loss: 0.2975044213235378\nBatch loss: 0.43967820703983307\nBatch loss: 0.12749291025102139\nBatch loss: 0.7627296447753906\nBatch loss: 0.2699756622314453\nBatch loss: 0.17377376556396484\nBatch loss: 0.3972606733441353\nBatch loss: 0.4468536376953125\nBatch loss: 0.3487272188067436\nBatch loss: 0.34377288073301315\nBatch loss: 0.31765367835760117\nBatch loss: 0.1251907367259264\nBatch loss: 0.7410984486341476\nBatch loss: 0.4957437515258789\nBatch loss: 0.4800090938806534\nBatch loss: 0.5492935329675674\nBatch loss: 0.43828200548887253\nBatch loss: 0.5580520629882812\nBatch loss: 0.10735893622040749\nBatch loss: 0.4517221450805664\nBatch loss: 0.36125946789979935\nBatch loss: 0.1668386533856392\nBatch loss: 0.14654827304184437\nBatch loss: 0.40560532361268997\nBatch loss: 0.7176370918750763\nBatch loss: 0.21377181634306908\nBatch loss: 0.5741748958826065\nBatch loss: 0.5666294321417809\nBatch loss: 0.09959220886230469\nBatch loss: 0.7776012271642685\nBatch loss: 0.37770844995975494\nBatch loss: 0.4327230527997017\nBatch loss: 0.6540212780237198\nBatch loss: 0.6145956739783287\nBatch loss: 0.3185730054974556\nBatch loss: 0.9888610988855362\nBatch loss: 0.37726592272520065\nBatch loss: 0.3239159658551216\nBatch loss: 1.066509261727333\nBatch loss: 0.6190090253949165\nBatch loss: 0.5224905163049698\nBatch loss: 0.37470627576112747\nBatch loss: 0.16369342803955078\nBatch loss: 0.528411865234375\nBatch loss: 0.2004261128604412\nBatch loss: 0.40728379040956497\nBatch loss: 0.43422698974609375\nBatch loss: 0.7403745502233505\nBatch loss: 0.22809600457549095\nBatch loss: 0.5034675821661949\nBatch loss: 0.3324718400835991\nBatch loss: 0.3203468397259712\nBatch loss: 0.47931671142578125\nBatch loss: 0.6913013756275177\nBatch loss: 0.1079559326171875\nBatch loss: 0.49346160143613815\nBatch loss: 0.1914367638528347\nBatch loss: 0.35094834864139557\nBatch loss: 0.5463333055377007\nBatch loss: 0.3040914610028267\nBatch loss: 0.7977676391601562\nBatch loss: 0.22245407104492188\nBatch loss: 0.47457125037908554\nBatch loss: 0.18098831176757812\nBatch loss: 0.19672488793730736\nBatch loss: 0.2960548363626003\nBatch loss: 0.4927825927734375\nBatch loss: 0.13495159335434437\nBatch loss: 0.17737388610839844\nBatch loss: 0.3690981864929199\nBatch loss: 0.297595988959074\nBatch loss: 0.15247344970703125\nBatch loss: 0.2575368992984295\nBatch loss: 0.22284125909209251\nBatch loss: 0.3377838060259819\nBatch loss: 0.13247299008071423\nBatch loss: 0.126793859526515\nBatch loss: 0.22222137078642845\nBatch loss: 0.5134773254394531\nBatch loss: 0.14593697153031826\nBatch loss: 0.5566902086138725\nBatch loss: 0.3930854797363281\nBatch loss: 0.5261116102337837\nBatch loss: 0.0594596890732646\nBatch loss: 0.12492370791733265\nBatch loss: 0.3079643286764622\nBatch loss: 0.48151399940252304\nBatch loss: 0.3665962442755699\nBatch loss: 0.8558159321546555\nBatch loss: 0.3786773607134819\nBatch loss: 0.49567461013793945\nBatch loss: 0.6856651604175568\nBatch loss: 0.25895310565829277\nBatch loss: 0.39251137524843216\nBatch loss: 0.4781046137213707\nBatch loss: 0.4700469970703125\nBatch loss: 0.1781771145761013\nBatch loss: 0.1752319373190403\nBatch loss: 0.2663917653262615\nBatch loss: 0.4050903394818306\nBatch loss: 0.5002222210168839\nBatch loss: 0.4866332933306694\nBatch loss: 0.3499775007367134\nBatch loss: 0.8355560153722763\nBatch loss: 0.36447715014219284\nBatch loss: 0.8161430805921555\nBatch loss: 0.41653823107481003\nBatch loss: 0.23024750873446465\nBatch loss: 0.24454498663544655\nBatch loss: 0.7406120747327805\nBatch loss: 0.661468505859375\nBatch loss: 0.40443994104862213\nBatch loss: 0.19719695672392845\nBatch loss: 0.7146873325109482\nBatch loss: 0.4288787767291069\nBatch loss: 0.9335937350988388\nBatch loss: 0.36814119666814804\nBatch loss: 0.6002655252814293\nBatch loss: 0.3037605248391628\nBatch loss: 0.5304565653204918\nBatch loss: 0.11698531918227673\nBatch loss: 0.23376083001494408\nBatch loss: 0.4646778106689453\nBatch loss: 0.22875403985381126\nBatch loss: 0.5542869493365288\nBatch loss: 0.6257706135511398\nBatch loss: 0.2407531812787056\nBatch loss: 0.5797271803021431\nBatch loss: 0.11855077929794788\nBatch loss: 0.5652008205652237\nBatch loss: 0.2681617811322212\nBatch loss: 0.3742046281695366\nBatch loss: 0.6301040947437286\nBatch loss: 0.35492707043886185\nBatch loss: 0.36319732666015625\nBatch loss: 0.7214508205652237\nBatch loss: 0.3889274597167969\nBatch loss: 0.3476238250732422\nBatch loss: 0.4985246807336807\nBatch loss: 0.44861890375614166\nBatch loss: 0.2242603339254856\nBatch loss: 0.37526320666074753\nBatch loss: 0.19321346655488014\nBatch loss: 0.6964826583862305\nBatch loss: 0.4833069071173668\nBatch loss: 0.27811814099550247\nBatch loss: 0.3630371019244194\nBatch loss: 0.46483613550662994\nBatch loss: 0.3510894998908043\nBatch loss: 1.3288727402687073\nBatch loss: 0.3051922284066677\nBatch loss: 0.4227018356323242\nBatch loss: 0.1798420026898384\nBatch loss: 0.45276641845703125\nBatch loss: 0.36687087267637253\nBatch loss: 0.27820779010653496\nBatch loss: 0.22238923236727715\nBatch loss: 0.31352996826171875\nBatch loss: 0.32041169703006744\nBatch loss: 0.36356355994939804\nBatch loss: 0.5579767376184464\nBatch loss: 0.4540901258587837\nBatch loss: 0.37982940673828125\nBatch loss: 0.4526824876666069\nBatch loss: 0.5200881883502007\nBatch loss: 0.42032625526189804\nBatch loss: 0.3614196926355362\nBatch loss: 0.5857467651367188\nBatch loss: 0.6510696560144424\nBatch loss: 0.26071930304169655\nBatch loss: 0.4484710842370987\nBatch loss: 0.15457344241440296\nBatch loss: 0.3177795559167862\nBatch loss: 0.2021808736026287\nBatch loss: 0.17745209857821465\nBatch loss: 0.2505226247012615\nBatch loss: 0.6639595329761505\nBatch loss: 0.3600196912884712\nBatch loss: 0.3431239351630211\nBatch loss: 0.2937889099121094\nBatch loss: 0.4634857177734375\nBatch loss: 0.4906768724322319\nBatch loss: 0.3305168077349663\nBatch loss: 0.45767784118652344\nBatch loss: 0.3141021728515625\nBatch loss: 0.27336692437529564\nBatch loss: 0.3233318403363228\nBatch loss: 0.4523315653204918\nBatch loss: 0.3829307481646538\nBatch loss: 0.37452317774295807\nBatch loss: 0.9396820515394211\nBatch loss: 0.6790142506361008\nBatch loss: 0.11272883974015713\nBatch loss: 0.3427314758300781\nBatch loss: 0.39083387702703476\nBatch loss: 0.2602844312787056\nBatch loss: 0.4481315612792969\nBatch loss: 0.5271768569946289\nBatch loss: 0.14088344760239124\nBatch loss: 0.14648819342255592\nBatch loss: 0.10010910220444202\nBatch loss: 0.44097900390625\nBatch loss: 0.36318685859441757\nBatch loss: 0.4078865051269531\nBatch loss: 0.2190685272216797\nBatch loss: 0.4337882995605469\nBatch loss: 0.3554077073931694\nBatch loss: 0.4817810282111168\nBatch loss: 0.19684409722685814\nBatch loss: 0.32992076128721237\nBatch loss: 0.5106544494628906\nBatch loss: 0.3960418701171875\nBatch loss: 0.3796691820025444\nBatch loss: 0.2695922926068306\nBatch loss: 0.22646714001893997\nBatch loss: 0.4877624660730362\nBatch loss: 0.9248695522546768\nBatch loss: 0.49561310559511185\nBatch loss: 0.1232223492115736\nBatch loss: 0.6523742526769638\nBatch loss: 0.35700228065252304\nBatch loss: 0.9514961391687393\nBatch loss: 0.19138528034090996\nBatch loss: 0.9510879963636398\nBatch loss: 0.3639869764447212\nBatch loss: 0.39371587336063385\nBatch loss: 0.22969437763094902\nBatch loss: 0.6140785291790962\nBatch loss: 0.7377300411462784\nBatch loss: 0.4393310472369194\nBatch loss: 0.2913513220846653\nBatch loss: 0.6166210398077965\nBatch loss: 0.3184356912970543\nBatch loss: 0.16089439392089844\nBatch loss: 0.32445143908262253\nBatch loss: 0.10629273019731045\nBatch loss: 0.592597983777523\nBatch loss: 0.47927476465702057\nBatch loss: 0.3377990797162056\nBatch loss: 0.5036149173974991\nBatch loss: 0.1428756769746542\nBatch loss: 0.5096969753503799\nBatch loss: 0.30047131702303886\nBatch loss: 0.2907257154583931\nBatch loss: 0.2901153638958931\nBatch loss: 0.18484115600585938\nBatch loss: 0.5815448984503746\nBatch loss: 0.2990570105612278\nBatch loss: 0.1540660858154297\nBatch loss: 0.6187276914715767\nBatch loss: 0.33445168286561966\nBatch loss: 0.33742140978574753\nBatch loss: 0.24323655292391777\nBatch loss: 0.7868995517492294\nBatch loss: 0.28134727850556374\nBatch loss: 0.9155960381031036\nBatch loss: 0.14081859961152077\nBatch loss: 0.5706758424639702\nBatch loss: 0.46345900744199753\nBatch loss: 0.23723412305116653\nBatch loss: 0.5811386182904243\nBatch loss: 0.8856296539306641\nBatch loss: 0.3602771833539009\nBatch loss: 0.5710449442267418\nBatch loss: 0.6203823164105415\nBatch loss: 0.9378375858068466\nBatch loss: 0.07346916478127241\nBatch loss: 0.3563251718878746\nBatch loss: 0.2396698109805584\nBatch loss: 0.7140312343835831\nBatch loss: 0.13521957211196423\nBatch loss: 0.2057027816772461\nBatch loss: 0.3124389611184597\nBatch loss: 0.36049652844667435\nBatch loss: 0.7837428897619247\nBatch loss: 0.14574194326996803\nBatch loss: 0.15206336975097656\nBatch loss: 0.19977379590272903\nBatch loss: 0.3221283107995987\nBatch loss: 0.4955768585205078\nBatch loss: 0.8641748875379562\nBatch loss: 0.31548500061035156\nBatch loss: 0.9627228230237961\nBatch loss: 0.7418899983167648\nBatch loss: 0.3771514818072319\nBatch loss: 0.12932300567626953\nBatch loss: 0.5535411834716797\nBatch loss: 0.15621853061020374\nBatch loss: 0.3620262071490288\nBatch loss: 0.14752578921616077\nBatch loss: 0.5860843881964684\nBatch loss: 0.6163635477423668\nBatch loss: 0.9032364189624786\nBatch loss: 0.3024940565228462\nBatch loss: 0.6724472343921661\nBatch loss: 0.2277851104736328\nBatch loss: 0.4281044006347656\nBatch loss: 0.5827407911419868\nBatch loss: 0.5630245432257652\nBatch loss: 0.6107978895306587\nBatch loss: 0.4190254211425781\nBatch loss: 0.18497085198760033\nBatch loss: 0.5134201049804688\nBatch loss: 0.32364942133426666\nBatch loss: 0.12345314025878906\nBatch loss: 0.3234901651740074\nBatch loss: 0.12608909979462624\nBatch loss: 0.4548540338873863\nBatch loss: 0.7541980594396591\nBatch loss: 0.3263244777917862\nBatch loss: 0.5296402052044868\nBatch loss: 0.4503745958209038\nBatch loss: 0.21338654682040215\nBatch loss: 0.12135124765336514\nBatch loss: 0.66314697265625\nBatch loss: 0.6735115498304367\nBatch loss: 0.3629341349005699\nBatch loss: 0.6018180772662163\nBatch loss: 0.14274025335907936\nBatch loss: 0.5435562133789062\nBatch loss: 0.251935962587595\nBatch loss: 0.34125518053770065\nBatch loss: 0.5292053148150444\nBatch loss: 0.14577102847397327\nBatch loss: 0.2077636681497097\nBatch loss: 0.11931419372558594\nBatch loss: 0.22035980597138405\nBatch loss: 0.5578117445111275\nBatch loss: 0.38251399993896484\nBatch loss: 0.3921356424689293\nBatch loss: 0.37325479090213776\nBatch loss: 0.17850114032626152\nBatch loss: 0.26074791327118874\nBatch loss: 0.5876770243048668\nBatch loss: 0.1753702200949192\nBatch loss: 0.35787202417850494\nBatch loss: 0.3166627883911133\nBatch loss: 0.4400062561035156\nBatch loss: 0.33367156982421875\nBatch loss: 0.5101356655359268\nBatch loss: 0.4999389871954918\nBatch loss: 0.14858245849609375\nBatch loss: 0.3759002685546875\nBatch loss: 0.5666294321417809\nBatch loss: 0.11639595031738281\nBatch loss: 0.30030442401766777\nBatch loss: 0.4055028036236763\nBatch loss: 0.2548856846988201\nBatch loss: 0.38721468299627304\nBatch loss: 0.399690642952919\nBatch loss: 0.2672576904296875\nBatch loss: 0.3058967553079128\nBatch loss: 0.4547119140625\nBatch loss: 0.2772064320743084\nBatch loss: 0.474700927734375\nBatch loss: 0.5150261148810387\nBatch loss: 0.2725229226052761\nBatch loss: 0.560428611934185\nBatch loss: 0.4675750806927681\nBatch loss: 0.5908698961138725\nBatch loss: 0.29424285516142845\nBatch loss: 0.3851776197552681\nBatch loss: 0.5794639512896538\nBatch loss: 0.37734221667051315\nBatch loss: 0.45558929443359375\nBatch loss: 0.3651542589068413\nBatch loss: 0.34220147877931595\nBatch loss: 0.28516579419374466\nBatch loss: 0.35865139216184616\nBatch loss: 0.30840396881103516\nBatch loss: 0.41170503944158554\nBatch loss: 0.5933265760540962\nBatch loss: 0.17726613208651543\nBatch loss: 0.3377990797162056\nBatch loss: 0.7046623528003693\nBatch loss: 0.4616241529583931\nBatch loss: 0.5798034742474556\nBatch loss: 0.5359707027673721\nBatch loss: 0.5059070512652397\nBatch loss: 0.5546111986041069\nBatch loss: 0.0751152029260993\nBatch loss: 0.23037148639559746\nBatch loss: 0.43734360486268997\nBatch loss: 0.27396297082304955\nBatch loss: 0.19914960488677025\nBatch loss: 0.3851194307208061\nBatch loss: 0.1491317804902792\nBatch loss: 0.14257526956498623\nBatch loss: 0.17243577167391777\nBatch loss: 0.4713153839111328\nBatch loss: 0.3471565246582031\nBatch loss: 0.6492538750171661\nBatch loss: 0.2249755896627903\nBatch loss: 0.32886650413274765\nBatch loss: 0.29259681701660156\nBatch loss: 0.3589019924402237\nBatch loss: 0.3379974514245987\nBatch loss: 0.3158607706427574\nBatch loss: 0.2572066895663738\nBatch loss: 0.5826988443732262\nBatch loss: 0.16408538445830345\nBatch loss: 0.30481910333037376\nBatch loss: 0.6118402630090714\nBatch loss: 0.232896339148283\nBatch loss: 0.5353441461920738\nBatch loss: 0.47652244567871094\nBatch loss: 0.45922089368104935\nBatch loss: 0.18210982903838158\nBatch loss: 0.4376225546002388\nBatch loss: 0.421537421643734\nBatch loss: 0.3005218505859375\nBatch loss: 0.3450317308306694\nBatch loss: 0.3471717983484268\nBatch loss: 0.6499481201171875\nBatch loss: 0.327848456799984\nBatch loss: 0.3670353814959526\nBatch loss: 0.3775138780474663\nBatch loss: 0.6323128193616867\nBatch loss: 0.8767242729663849\nBatch loss: 0.5380880832672119\nBatch loss: 0.31377412378787994\nBatch loss: 0.826442763209343\nBatch loss: 0.1549243927001953\nBatch loss: 0.4619751125574112\nBatch loss: 0.2783241309225559\nBatch loss: 0.1617431640625\nBatch loss: 0.17351150512695312\nBatch loss: 0.7615528255701065\nBatch loss: 0.13045310974121094\nBatch loss: 0.1611003838479519\nBatch loss: 0.07069301791489124\nBatch loss: 0.27141381055116653\nBatch loss: 0.30869293957948685\nBatch loss: 0.5742492899298668\nBatch loss: 0.27391720563173294\nBatch loss: 0.1677703857421875\nBatch loss: 0.18379783257842064\nBatch loss: 0.37572480738162994\nBatch loss: 0.24327659979462624\nBatch loss: 0.3105506859719753\nBatch loss: 0.38724519312381744\nBatch loss: 0.18258286640048027\nBatch loss: 0.17766786739230156\nBatch loss: 0.6252918392419815\nBatch loss: 0.39193011820316315\nBatch loss: 0.11937618255615234\nBatch loss: 0.30438899993896484\nBatch loss: 0.39794921875\nBatch loss: 0.20330429077148438\nBatch loss: 0.7422810047864914\nBatch loss: 0.26659775525331497\nBatch loss: 0.2789764478802681\nBatch loss: 0.1921539381146431\nBatch loss: 0.14249133877456188\nBatch loss: 0.09517288766801357\nBatch loss: 0.09533310309052467\nBatch loss: 0.6283283233642578\nBatch loss: 0.6136932596564293\nBatch loss: 0.6135301664471626\nBatch loss: 0.4239768907427788\nBatch loss: 0.23806381970643997\nBatch loss: 0.631076842546463\nBatch loss: 0.24925613775849342\nBatch loss: 0.4904632642865181\nBatch loss: 0.2066192589700222\nBatch loss: 0.583391897380352\nBatch loss: 0.1541824359446764\nBatch loss: 0.10151672177016735\nBatch loss: 0.21870994940400124\nBatch loss: 0.043595791794359684\nBatch loss: 0.8730354160070419\nBatch loss: 0.1698918454349041\nBatch loss: 0.06595802493393421\nBatch loss: 0.34406282007694244\nBatch loss: 0.22610856220126152\nBatch loss: 0.44174958020448685\nBatch loss: 0.48101045191287994\nBatch loss: 0.14051437377929688\nBatch loss: 0.6365527957677841\nBatch loss: 0.370999351143837\nBatch loss: 0.3943166881799698\nBatch loss: 0.163771640509367\nBatch loss: 0.09749985300004482\nBatch loss: 0.5310511589050293\nBatch loss: 0.32154466956853867\nBatch loss: 0.13440608978271484\nBatch loss: 0.16256188973784447\nBatch loss: 0.3695545345544815\nBatch loss: 0.16532136127352715\nBatch loss: 0.4284093528985977\nBatch loss: 0.36672402173280716\nBatch loss: 0.5101356655359268\nBatch loss: 0.46918582171201706\nBatch loss: 0.5598230287432671\nBatch loss: 0.8031616359949112\nBatch loss: 0.1130752544850111\nBatch loss: 0.8110399544239044\nBatch loss: 0.31790733337402344\nBatch loss: 0.3684272989630699\nBatch loss: 0.09425736032426357\nBatch loss: 0.43314363807439804\nBatch loss: 0.350557342171669\nBatch loss: 0.08486366830766201\nBatch loss: 0.5506591871380806\nBatch loss: 0.29071617871522903\nBatch loss: 0.11916542425751686\nBatch loss: 0.9295959770679474\nBatch loss: 0.6586704403162003\nBatch loss: 0.36260034888982773\nBatch loss: 0.35018064081668854\nBatch loss: 0.7989807426929474\nBatch loss: 0.5882244184613228\nBatch loss: 0.24109935387969017\nBatch loss: 0.2931232564151287\nBatch loss: 0.31031036749482155\nBatch loss: 0.4428234323859215\nBatch loss: 0.09448385797441006\nBatch loss: 0.4760437086224556\nBatch loss: 0.3246784210205078\nBatch loss: 0.36709975451231003\nBatch loss: 0.13985157012939453\nBatch loss: 0.24205589666962624\nBatch loss: 0.13184643350541592\nBatch loss: 0.42312242090702057\nBatch loss: 0.25740623474121094\nBatch loss: 0.12248421087861061\nBatch loss: 0.3503227233886719\nBatch loss: 0.14149951748549938\nBatch loss: 0.4288597032427788\nBatch loss: 0.24900246411561966\nBatch loss: 0.7881050556898117\nBatch loss: 0.28314972296357155\nBatch loss: 0.052660941146314144\nBatch loss: 0.44407274574041367\nBatch loss: 0.13376045040786266\nBatch loss: 0.04198122303932905\nBatch loss: 0.4704132303595543\nBatch loss: 0.20853424444794655\nBatch loss: 0.22324562072753906\nBatch loss: 0.45145798474550247\nBatch loss: 0.7283754646778107\nBatch loss: 0.24828720837831497\nBatch loss: 0.16070891171693802\nBatch loss: 0.24410247802734375\nBatch loss: 0.20107079297304153\nBatch loss: 0.14624977484345436\nBatch loss: 0.18463898450136185\nBatch loss: 0.4298827797174454\nBatch loss: 1.0027656704187393\nBatch loss: 0.18915748223662376\nBatch loss: 0.2774086035788059\nBatch loss: 0.46767234802246094\nBatch loss: 0.07507658097893\nBatch loss: 1.0418491810560226\nBatch loss: 0.11742210946977139\nBatch loss: 0.13450956903398037\nBatch loss: 0.17604827880859375\nBatch loss: 0.42946625500917435\nBatch loss: 0.15178585425019264\nBatch loss: 0.10925865732133389\nBatch loss: 0.1472320593893528\nBatch loss: 0.35727690905332565\nBatch loss: 0.09515571407973766\nBatch loss: 0.2675342559814453\nBatch loss: 0.16117239370942116\nBatch loss: 0.2583928219974041\nBatch loss: 0.4329892620444298\nBatch loss: 0.3104858472943306\nBatch loss: 0.21924782544374466\nBatch loss: 0.19898224622011185\nBatch loss: 0.08173370733857155\nBatch loss: 0.44260788708925247\nBatch loss: 0.9114608913660049\nBatch loss: 0.45372962951660156\nBatch loss: 0.48306848853826523\nBatch loss: 0.19559288397431374\nBatch loss: 0.3742504119873047\nBatch loss: 0.35597991198301315\nBatch loss: 0.061314585618674755\nBatch loss: 0.4790334776043892\nBatch loss: 0.09625482372939587\nBatch loss: 0.545196533203125\nBatch loss: 0.37033986300230026\nBatch loss: 0.2720489539206028\nBatch loss: 0.6450624763965607\nBatch loss: 0.09204196743667126\nBatch loss: 0.16786670312285423\nBatch loss: 0.48728562891483307\nBatch loss: 0.28569793328642845\nBatch loss: 0.15913009643554688\nBatch loss: 0.7642994076013565\nBatch loss: 0.37650682032108307\nBatch loss: 0.4424858093261719\nBatch loss: 0.33515166491270065\nBatch loss: 0.18285369500517845\nBatch loss: 0.29864026233553886\nBatch loss: 0.19588375464081764\nBatch loss: 0.3503246232867241\nBatch loss: 0.24730682373046875\nBatch loss: 0.5172863230109215\nBatch loss: 0.483856201171875\nBatch loss: 0.7273998111486435\nBatch loss: 0.4780254513025284\nBatch loss: 0.059820651076734066\nBatch loss: 0.26802515611052513\nBatch loss: 0.6296367943286896\nBatch loss: 0.2814312092959881\nBatch loss: 0.3614330291748047\nBatch loss: 0.41555214673280716\nBatch loss: 0.2671818807721138\nBatch loss: 0.4177870973944664\nBatch loss: 0.11299229227006435\nBatch loss: 0.4274463653564453\nBatch loss: 0.5476589128375053\nBatch loss: 1.0422172397375107\nBatch loss: 0.13631105422973633\nBatch loss: 0.2844705618917942\nBatch loss: 0.4385256767272949\nBatch loss: 0.15976523980498314\nBatch loss: 0.14494228176772594\nBatch loss: 0.17584038898348808\nBatch loss: 0.12981558218598366\nBatch loss: 0.6413650512695312\nBatch loss: 0.35475969314575195\nBatch loss: 0.25025367736816406\nBatch loss: 0.16272354871034622\nBatch loss: 0.23426247760653496\nBatch loss: 0.6811761856079102\nBatch loss: 0.24411488324403763\nBatch loss: 0.41903018951416016\nBatch loss: 0.4082918167114258\nBatch loss: 0.26516009122133255\nBatch loss: 0.19668793305754662\nBatch loss: 0.11150121688842773\nBatch loss: 0.25974273681640625\nBatch loss: 0.5803623422980309\nBatch loss: 0.260296817868948\nBatch loss: 0.10676288977265358\nBatch loss: 0.07568740751594305\nBatch loss: 0.5601677298545837\nBatch loss: 0.16244078055024147\nBatch loss: 0.934077724814415\nBatch loss: 0.17145348712801933\nBatch loss: 0.14525413513183594\nBatch loss: 0.16234684735536575\nBatch loss: 0.05541086196899414\nBatch loss: 0.6500756740570068\nBatch loss: 0.628444105386734\nBatch loss: 0.13043451122939587\nBatch loss: 0.07135891821235418\nBatch loss: 0.37943173199892044\nBatch loss: 0.10937976650893688\nBatch loss: 0.2629661560058594\nBatch loss: 0.736173614859581\nBatch loss: 0.06899070926010609\nBatch loss: 0.09716606698930264\nBatch loss: 0.09588241577148438\nBatch loss: 1.0181207954883575\nBatch loss: 1.8951416015625\nBatch loss: 0.4218459129333496\nBatch loss: 0.1716308668255806\nBatch loss: 0.19970571622252464\nBatch loss: 0.041875364258885384\nBatch loss: 0.09892463684082031\nBatch loss: 0.3290853649377823\nBatch loss: 0.19466495141386986\nBatch loss: 0.17595672979950905\nBatch loss: 0.4487323760986328\nBatch loss: 0.032750130631029606\nBatch loss: 0.10059833526611328\nBatch loss: 0.05358028691262007\nBatch loss: 0.07757759187370539\nBatch loss: 0.1814413070678711\nBatch loss: 0.3475227579474449\nBatch loss: 0.17251968383789062\nBatch loss: 0.4305272176861763\nBatch loss: 0.08441544137895107\nBatch loss: 0.11166906915605068\nBatch loss: 0.6337423622608185\nBatch loss: 0.12087631039321423\nBatch loss: 0.2314014546573162\nBatch loss: 0.3691883012652397\nBatch loss: 0.18550587818026543\nBatch loss: 0.31925585120916367\nBatch loss: 0.06467819213867188\nBatch loss: 0.08625221438705921\nBatch loss: 0.17408324405550957\nBatch loss: 1.3411353528499603\nBatch loss: 0.5766067653894424\nBatch loss: 1.4132098853588104\nBatch loss: 0.46276140958070755\nBatch loss: 0.8701314777135849\nBatch loss: 0.11990833096206188\nBatch loss: 0.3327808529138565\nBatch loss: 0.3451819345355034\nBatch loss: 0.4323098808526993\nBatch loss: 0.4730234295129776\nBatch loss: 0.36660004407167435\nBatch loss: 0.48703480511903763\nBatch loss: 0.2144169807434082\nBatch loss: 0.2362206019461155\nBatch loss: 0.5203089863061905\nBatch loss: 0.23105813190340996\nBatch loss: 0.14614892192184925\nBatch loss: 0.07767486851662397\nBatch loss: 0.7634086906909943\nBatch loss: 0.7207696884870529\nBatch loss: 0.5258536338806152\nBatch loss: 0.5213546752929688\nBatch loss: 0.4394855722784996\nBatch loss: 0.671687126159668\nBatch loss: 0.2341938018798828\nBatch loss: 0.2765190601348877\nBatch loss: 0.674559623003006\nBatch loss: 0.16351794824004173\nBatch loss: 0.06596088409423828\nBatch loss: 0.14462805353105068\nBatch loss: 0.6647362560033798\nBatch loss: 0.6309127807617188\nBatch loss: 0.22655202075839043\nBatch loss: 0.45817233622074127\nBatch loss: 0.35379886627197266\nBatch loss: 0.3325481340289116\nBatch loss: 0.08508014492690563\nBatch loss: 0.6315918266773224\nBatch loss: 0.4878789186477661\nBatch loss: 0.029605389572679996\nBatch loss: 0.463351272046566\nBatch loss: 0.4027710109949112\nBatch loss: 0.04669570829719305\nBatch loss: 0.08311641402542591\nBatch loss: 0.7173629105091095\nBatch loss: 0.5907860025763512\nBatch loss: 0.08881473913788795\nBatch loss: 0.06032896228134632\nBatch loss: 0.7047166675329208\nBatch loss: 0.2122650109231472\nBatch loss: 0.21307755261659622\nBatch loss: 0.06287097930908203\nBatch loss: 0.2058582380414009\nBatch loss: 0.5270347744226456\nBatch loss: 0.9057941287755966\nBatch loss: 0.10804128833115101\nBatch loss: 0.49744416028261185\nBatch loss: 0.5419492721557617\nBatch loss: 0.1794729195535183\nBatch loss: 0.1253032684326172\nBatch loss: 0.226898193359375\nBatch loss: 0.3508796915411949\nBatch loss: 0.2932586707174778\nBatch loss: 0.5825729295611382\nBatch loss: 0.07740259170532227\nBatch loss: 0.0942850112915039\nBatch loss: 0.18596841022372246\nBatch loss: 0.26962852105498314\nBatch loss: 0.39721202105283737\nBatch loss: 0.2760734595358372\nBatch loss: 0.21273447200655937\nBatch loss: 0.18417930230498314\nBatch loss: 0.38121607154607773\nBatch loss: 0.23885393515229225\nBatch loss: 0.2824745140969753\nBatch loss: 0.2527465857565403\nBatch loss: 0.1929311826825142\nBatch loss: 0.27714014053344727\nBatch loss: 0.07696151733398438\nBatch loss: 0.1414270419627428\nBatch loss: 0.6544037163257599\nBatch loss: 0.044586420990526676\nBatch loss: 0.09222030639648438\nBatch loss: 0.15799475833773613\nBatch loss: 0.5155744776129723\nBatch loss: 0.08883428759872913\nBatch loss: 0.33245086669921875\nBatch loss: 0.712185874581337\nBatch loss: 0.5856876447796822\nBatch loss: 0.5426101759076118\nBatch loss: 0.35216689109802246\nBatch loss: 0.265960693359375\nBatch loss: 0.4831085354089737\nBatch loss: 0.12882900424301624\nBatch loss: 0.47566987574100494\nBatch loss: 0.12588310055434704\nBatch loss: 0.24492645636200905\nBatch loss: 0.6702003628015518\nBatch loss: 0.6722436100244522\nBatch loss: 0.2562539651989937\nBatch loss: 0.22950077429413795\nBatch loss: 0.3073711507022381\nBatch loss: 0.05349159240722656\nBatch loss: 0.17255043610930443\nBatch loss: 0.7511396706104279\nBatch loss: 0.6635170429944992\nBatch loss: 0.2717018127441406\nBatch loss: 0.6618652492761612\nBatch loss: 0.44771622866392136\nBatch loss: 0.584842674434185\nBatch loss: 0.5002708360552788\nBatch loss: 0.21638130769133568\nBatch loss: 0.25069331750273705\nBatch loss: 0.43747711926698685\nBatch loss: 0.13877272605895996\nBatch loss: 0.10306263342499733\nBatch loss: 0.2396240271627903\nBatch loss: 0.07323837373405695\nBatch loss: 0.14984369277954102\nBatch loss: 0.32578468322753906\nBatch loss: 0.34481145441532135\nEpoch 4/7\nBatch loss: 0.12198448181152344\nBatch loss: 0.06867122836410999\nBatch loss: 0.08529949001967907\nBatch loss: 0.06958675570785999\nBatch loss: 0.1726207695901394\nBatch loss: 0.6387205421924591\nBatch loss: 0.09689402766525745\nBatch loss: 0.16211463138461113\nBatch loss: 0.21985817700624466\nBatch loss: 0.2410629391670227\nBatch loss: 0.12472915463149548\nBatch loss: 0.6694941967725754\nBatch loss: 0.10636234655976295\nBatch loss: 0.4580679163336754\nBatch loss: 0.43288804590702057\nBatch loss: 0.3541717678308487\nBatch loss: 0.5941333994269371\nBatch loss: 0.09610987268388271\nBatch loss: 0.4615936428308487\nBatch loss: 0.3354683145880699\nBatch loss: 0.4937626048922539\nBatch loss: 0.21291350945830345\nBatch loss: 0.19341660663485527\nBatch loss: 0.24224519729614258\nBatch loss: 0.07840156555175781\nBatch loss: 0.35046007484197617\nBatch loss: 0.23991107940673828\nBatch loss: 0.1718900166451931\nBatch loss: 0.681983008980751\nBatch loss: 0.17722083255648613\nBatch loss: 0.3193855285644531\nBatch loss: 0.8087368309497833\nBatch loss: 0.3054475784301758\nBatch loss: 0.7239861786365509\nBatch loss: 0.3292214870452881\nBatch loss: 0.19780684262514114\nBatch loss: 0.18886758014559746\nBatch loss: 0.24230672046542168\nBatch loss: 0.35141564905643463\nBatch loss: 0.3699016571044922\nBatch loss: 0.12334251776337624\nBatch loss: 0.2884106710553169\nBatch loss: 0.13773727230727673\nBatch loss: 0.13693046756088734\nBatch loss: 0.47945883125066757\nBatch loss: 0.32425500452518463\nBatch loss: 0.14973342418670654\nBatch loss: 0.6468887627124786\nBatch loss: 0.4211864620447159\nBatch loss: 0.140091422945261\nBatch loss: 0.1616382598876953\nBatch loss: 0.21231859922409058\nBatch loss: 0.46140480786561966\nBatch loss: 0.2388458326458931\nBatch loss: 0.2907276153564453\nBatch loss: 0.9851894527673721\nBatch loss: 0.8328399807214737\nBatch loss: 0.2581825293600559\nBatch loss: 0.07962035946547985\nBatch loss: 0.10115957818925381\nBatch loss: 0.3263873979449272\nBatch loss: 0.110878711566329\nBatch loss: 0.18947411328554153\nBatch loss: 0.4249095916748047\nBatch loss: 0.10735535994172096\nBatch loss: 0.1855774037539959\nBatch loss: 0.46372413635253906\nBatch loss: 0.3079347684979439\nBatch loss: 0.12886357493698597\nBatch loss: 0.5363316461443901\nBatch loss: 0.35296250134706497\nBatch loss: 0.08496093563735485\nBatch loss: 0.11251449584960938\nBatch loss: 0.27333617210388184\nBatch loss: 0.21364593878388405\nBatch loss: 0.6217346340417862\nBatch loss: 0.05195498466491699\nBatch loss: 0.261733066290617\nBatch loss: 0.22235488519072533\nBatch loss: 0.5790453031659126\nBatch loss: 0.279738437384367\nBatch loss: 0.09953737258911133\nBatch loss: 0.30823899433016777\nBatch loss: 0.04680347628891468\nBatch loss: 0.45413780957460403\nBatch loss: 0.5629992485046387\nBatch loss: 0.26246070861816406\nBatch loss: 0.137481689453125\nBatch loss: 0.3977012634277344\nBatch loss: 0.0840606726706028\nBatch loss: 0.12921715155243874\nBatch loss: 0.43141555041074753\nBatch loss: 0.31659986823797226\nBatch loss: 0.13451481238007545\nBatch loss: 0.3932628780603409\nBatch loss: 0.14523888006806374\nBatch loss: 0.6471262127161026\nBatch loss: 0.38233377039432526\nBatch loss: 0.1539211254566908\nBatch loss: 0.15970611944794655\nBatch loss: 0.4844093322753906\nBatch loss: 0.4341564327478409\nBatch loss: 0.40039636194705963\nBatch loss: 0.1461706217378378\nBatch loss: 0.771678015589714\nBatch loss: 0.40251828730106354\nBatch loss: 0.08403206244111061\nBatch loss: 0.25904083624482155\nBatch loss: 0.3215007856488228\nBatch loss: 0.12495827861130238\nBatch loss: 0.15962600708007812\nBatch loss: 0.2269892767071724\nBatch loss: 0.36524582654237747\nBatch loss: 0.09489774703979492\nBatch loss: 0.608583465218544\nBatch loss: 0.43216850608587265\nBatch loss: 0.08881664834916592\nBatch loss: 0.06136226933449507\nBatch loss: 0.14900827780365944\nBatch loss: 0.08600997738540173\nBatch loss: 0.5988254770636559\nBatch loss: 0.2900419197976589\nBatch loss: 0.4779024049639702\nBatch loss: 0.05275249481201172\nBatch loss: 0.6129799038171768\nBatch loss: 0.08276271633803844\nBatch loss: 0.06830406375229359\nBatch loss: 0.14297104440629482\nBatch loss: 0.08558941073715687\nBatch loss: 0.6400337070226669\nBatch loss: 0.3482823446393013\nBatch loss: 0.04899406339973211\nBatch loss: 0.08959961123764515\nBatch loss: 0.28903866186738014\nBatch loss: 0.6128102540969849\nBatch loss: 0.1342077273875475\nBatch loss: 0.32761286944150925\nBatch loss: 0.27767205610871315\nBatch loss: 0.1594228856265545\nBatch loss: 0.05025005433708429\nBatch loss: 0.21401597186923027\nBatch loss: 0.2869296073913574\nBatch loss: 0.578254722058773\nBatch loss: 0.8810120075941086\nBatch loss: 0.3310871124267578\nBatch loss: 0.814361572265625\nBatch loss: 0.07536983583122492\nBatch loss: 0.4230060800909996\nBatch loss: 0.048748492263257504\nBatch loss: 0.6344642490148544\nBatch loss: 0.26209449395537376\nBatch loss: 0.13261819258332253\nBatch loss: 0.32544612884521484\nBatch loss: 0.8363284915685654\nBatch loss: 0.09861230850219727\nBatch loss: 0.17660712823271751\nBatch loss: 0.9181862324476242\nBatch loss: 0.5692267417907715\nBatch loss: 0.5147933959960938\nBatch loss: 0.33135034143924713\nBatch loss: 0.3373746946454048\nBatch loss: 0.3097515180706978\nBatch loss: 0.8776398003101349\nBatch loss: 0.3231847286224365\nBatch loss: 0.2638711966574192\nBatch loss: 0.03800296923145652\nBatch loss: 0.18564224243164062\nBatch loss: 0.26595115661621094\nBatch loss: 0.6125068664550781\nBatch loss: 0.2307271957397461\nBatch loss: 0.06319713778793812\nBatch loss: 0.10890674777328968\nBatch loss: 0.11700821109116077\nBatch loss: 0.6430435180664062\nBatch loss: 0.36382056772708893\nBatch loss: 0.3307991102337837\nBatch loss: 0.34238051623106003\nBatch loss: 0.5960312113165855\nBatch loss: 0.3423452377319336\nBatch loss: 0.13700676150619984\nBatch loss: 0.26183128356933594\nBatch loss: 0.10382032953202724\nBatch loss: 0.33339500427246094\nBatch loss: 0.21322453394532204\nBatch loss: 0.06134510040283203\nBatch loss: 0.2651500701904297\nBatch loss: 0.46616651117801666\nBatch loss: 0.12616634368896484\nBatch loss: 0.6416144222021103\nBatch loss: 0.13731861487030983\nBatch loss: 0.16182422637939453\nBatch loss: 0.6445445865392685\nBatch loss: 0.2564067952334881\nBatch loss: 0.11659813113510609\nBatch loss: 0.1499786414206028\nBatch loss: 0.3758068010210991\nBatch loss: 0.9261627495288849\nBatch loss: 0.7629690319299698\nBatch loss: 0.23459149524569511\nBatch loss: 0.1419353485107422\nBatch loss: 0.11071729473769665\nBatch loss: 0.1674661599099636\nBatch loss: 0.16252612695097923\nBatch loss: 0.29786014929413795\nBatch loss: 0.28369713574647903\nBatch loss: 0.6387767940759659\nBatch loss: 0.05436372943222523\nBatch loss: 0.1646418683230877\nBatch loss: 0.08787584491074085\nBatch loss: 0.20647812634706497\nBatch loss: 0.6387863308191299\nBatch loss: 0.06071948911994696\nBatch loss: 0.15015698038041592\nBatch loss: 0.307209026068449\nBatch loss: 0.31516551971435547\nBatch loss: 0.3426036983728409\nBatch loss: 0.5136089399456978\nBatch loss: 0.44483091682195663\nBatch loss: 0.7674255222082138\nBatch loss: 0.09425449185073376\nBatch loss: 0.08407420478761196\nBatch loss: 0.09173202328383923\nBatch loss: 0.4841804504394531\nBatch loss: 0.13156604953110218\nBatch loss: 0.10751343332231045\nBatch loss: 0.6114001199603081\nBatch loss: 0.16995524987578392\nBatch loss: 0.15469837002456188\nBatch loss: 0.08235645480453968\nBatch loss: 0.8845663070678711\nBatch loss: 0.23256158456206322\nBatch loss: 0.43541431427001953\nBatch loss: 0.23708820343017578\nBatch loss: 0.10132980532944202\nBatch loss: 0.11554241180419922\nBatch loss: 0.30095672234892845\nBatch loss: 0.29119957238435745\nBatch loss: 0.16298485919833183\nBatch loss: 0.5074386671185493\nBatch loss: 0.478484146296978\nBatch loss: 0.19630242139101028\nBatch loss: 0.24540424346923828\nBatch loss: 0.08645057678222656\nBatch loss: 0.24070119485259056\nBatch loss: 0.14727878384292126\nBatch loss: 0.14689254574477673\nBatch loss: 0.07742381189018488\nBatch loss: 0.3835306316614151\nBatch loss: 0.5794696882367134\nBatch loss: 0.10554266162216663\nBatch loss: 0.16627265140414238\nBatch loss: 0.4136068746447563\nBatch loss: 0.12939834967255592\nBatch loss: 0.2028179168701172\nBatch loss: 0.5284042283892632\nBatch loss: 0.3092031553387642\nBatch loss: 0.22530937567353249\nBatch loss: 0.1516127586364746\nBatch loss: 0.21390903741121292\nBatch loss: 0.10147285647690296\nBatch loss: 0.13606262393295765\nBatch loss: 0.2764883078634739\nBatch loss: 0.3466830402612686\nBatch loss: 0.27886008843779564\nBatch loss: 0.31720351427793503\nBatch loss: 0.10825253091752529\nBatch loss: 0.8886414021253586\nBatch loss: 0.12913656421005726\nBatch loss: 0.48287104815244675\nBatch loss: 0.5346641689538956\nBatch loss: 0.17142677679657936\nBatch loss: 0.2802734449505806\nBatch loss: 0.24327611550688744\nBatch loss: 0.3296785429120064\nBatch loss: 0.37597324699163437\nBatch loss: 0.11407852172851562\nBatch loss: 0.7245407253503799\nBatch loss: 0.22241687402129173\nBatch loss: 0.12955665588378906\nBatch loss: 0.36829184740781784\nBatch loss: 0.22006893530488014\nBatch loss: 0.0671186437830329\nBatch loss: 0.5349235609173775\nBatch loss: 0.10252666659653187\nBatch loss: 0.2136707305908203\nBatch loss: 0.2694106101989746\nBatch loss: 0.1371326483786106\nBatch loss: 0.3629417344927788\nBatch loss: 0.08802509866654873\nBatch loss: 0.22870564833283424\nBatch loss: 0.08380222134292126\nBatch loss: 0.3435711935162544\nBatch loss: 0.030542374588549137\nBatch loss: 0.3133239969611168\nBatch loss: 0.08070754818618298\nBatch loss: 0.26903916150331497\nBatch loss: 0.08070182986557484\nBatch loss: 0.20302915945649147\nBatch loss: 0.15492248348891735\nBatch loss: 0.06882429122924805\nBatch loss: 0.08844089694321156\nBatch loss: 0.5132722854614258\nBatch loss: 0.5694618448615074\nBatch loss: 0.11043548583984375\nBatch loss: 0.09751605801284313\nBatch loss: 0.26554061099886894\nBatch loss: 0.029849291313439608\nBatch loss: 0.12562084011733532\nBatch loss: 0.09404349140822887\nBatch loss: 0.6654920428991318\nBatch loss: 0.098503353074193\nBatch loss: 0.41414644569158554\nBatch loss: 0.5583925172686577\nBatch loss: 1.1798324435949326\nBatch loss: 0.2607135847210884\nBatch loss: 0.0442924490198493\nBatch loss: 0.08002042770385742\nBatch loss: 0.25786733254790306\nBatch loss: 0.3515825420618057\nBatch loss: 0.17488647252321243\nBatch loss: 0.24425316601991653\nBatch loss: 0.9662475436925888\nBatch loss: 0.6400241702795029\nBatch loss: 0.1832432858645916\nBatch loss: 0.5223617702722549\nBatch loss: 0.03700613975524902\nBatch loss: 0.21205902099609375\nBatch loss: 0.3922748565673828\nBatch loss: 0.11997985653579235\nBatch loss: 0.24984551593661308\nBatch loss: 0.1553192175924778\nBatch loss: 0.22695446386933327\nBatch loss: 0.2653522603213787\nBatch loss: 0.06539058871567249\nBatch loss: 0.22415924817323685\nBatch loss: 0.14931822195649147\nBatch loss: 0.09329509921371937\nBatch loss: 0.5856561660766602\nBatch loss: 0.36391258239746094\nBatch loss: 0.12083577923476696\nBatch loss: 0.11191415600478649\nBatch loss: 0.43675996363162994\nBatch loss: 0.47689247876405716\nBatch loss: 0.20392989739775658\nBatch loss: 0.4060196876525879\nBatch loss: 0.40031861513853073\nBatch loss: 0.18198205158114433\nBatch loss: 0.298687219619751\nBatch loss: 0.32301902770996094\nBatch loss: 0.045203687623143196\nBatch loss: 0.3361673280596733\nBatch loss: 0.3971738740801811\nBatch loss: 0.4048042371869087\nBatch loss: 0.05554247181862593\nBatch loss: 0.29825974255800247\nBatch loss: 0.4439130052924156\nBatch loss: 0.09121894836425781\nBatch loss: 0.014829159481450915\nBatch loss: 0.09523249231278896\nBatch loss: 0.2820606343448162\nBatch loss: 0.30573511496186256\nBatch loss: 0.32606493681669235\nBatch loss: 0.9975367039442062\nBatch loss: 0.2521095238626003\nBatch loss: 0.028319598641246557\nBatch loss: 0.1439356803894043\nBatch loss: 0.1972217671573162\nBatch loss: 0.04711675923317671\nBatch loss: 0.7468336075544357\nBatch loss: 0.6067142635583878\nBatch loss: 0.45508909970521927\nBatch loss: 0.4103121906518936\nBatch loss: 0.19776677712798119\nBatch loss: 0.3365440294146538\nBatch loss: 0.24430036544799805\nBatch loss: 0.37867166101932526\nBatch loss: 0.4400463029742241\nBatch loss: 0.4649829864501953\nBatch loss: 0.10824823752045631\nBatch loss: 0.415617935359478\nBatch loss: 0.12879562564194202\nBatch loss: 0.05309009458869696\nBatch loss: 0.4521522670984268\nBatch loss: 0.22313786670565605\nBatch loss: 0.6266745179891586\nBatch loss: 0.18150711432099342\nBatch loss: 0.21889686584472656\nBatch loss: 0.051975250244140625\nBatch loss: 0.23273944854736328\nBatch loss: 0.18320847302675247\nBatch loss: 0.08058167062699795\nBatch loss: 0.5136963352560997\nBatch loss: 0.27568722143769264\nBatch loss: 0.3312849998474121\nBatch loss: 0.24078894406557083\nBatch loss: 0.16511822119355202\nBatch loss: 0.07980919443070889\nBatch loss: 0.26445532217621803\nBatch loss: 0.11702108196914196\nBatch loss: 0.11742806993424892\nBatch loss: 0.8902702480554581\nBatch loss: 0.5000581964850426\nBatch loss: 0.16198111698031425\nBatch loss: 0.043105604127049446\nBatch loss: 0.2889583073556423\nBatch loss: 0.28219079598784447\nBatch loss: 0.5705032497644424\nBatch loss: 0.44249631464481354\nBatch loss: 0.1925048790872097\nBatch loss: 0.6554434448480606\nBatch loss: 0.4485654830932617\nBatch loss: 0.532379150390625\nBatch loss: 0.19952727481722832\nBatch loss: 0.2213592641055584\nBatch loss: 0.46536829322576523\nBatch loss: 0.1465606689453125\nBatch loss: 0.060591697692871094\nBatch loss: 0.10799789801239967\nBatch loss: 0.2044682577252388\nBatch loss: 0.06679534912109375\nBatch loss: 0.7195796817541122\nBatch loss: 0.11449361220002174\nBatch loss: 0.36304857581853867\nBatch loss: 0.049706222489476204\nBatch loss: 0.17032815143465996\nBatch loss: 0.08505821228027344\nBatch loss: 0.16743039712309837\nBatch loss: 0.2726163901388645\nBatch loss: 0.10100889019668102\nBatch loss: 0.10546875186264515\nBatch loss: 0.36377619951963425\nBatch loss: 0.15671683475375175\nBatch loss: 0.15689803287386894\nBatch loss: 0.23305702954530716\nBatch loss: 0.527074821293354\nBatch loss: 0.2744293212890625\nBatch loss: 0.5372524261474609\nBatch loss: 0.29907798394560814\nBatch loss: 0.35663988441228867\nBatch loss: 0.26076316833496094\nBatch loss: 0.2827758900821209\nBatch loss: 0.07731628604233265\nBatch loss: 0.3798060491681099\nBatch loss: 0.3862924501299858\nBatch loss: 0.34607648849487305\nBatch loss: 0.08559036068618298\nBatch loss: 0.05805587861686945\nBatch loss: 0.27947235852479935\nBatch loss: 0.0813679676502943\nBatch loss: 0.26998162269592285\nBatch loss: 0.6907768547534943\nBatch loss: 0.25835825130343437\nBatch loss: 0.1273356657475233\nBatch loss: 0.09150124154984951\nBatch loss: 0.48266123980283737\nBatch loss: 0.19185353070497513\nBatch loss: 0.0819101370871067\nBatch loss: 0.5485055595636368\nBatch loss: 0.18762541934847832\nBatch loss: 0.10224438272416592\nBatch loss: 0.12257552705705166\nBatch loss: 0.3390960767865181\nBatch loss: 0.23194743320345879\nBatch loss: 0.07780075073242188\nBatch loss: 0.19485807046294212\nBatch loss: 0.20121384412050247\nBatch loss: 0.1555571611970663\nBatch loss: 0.35915564745664597\nBatch loss: 0.052830218337476254\nBatch loss: 0.06546306889504194\nBatch loss: 0.18228817731142044\nBatch loss: 0.12424278073012829\nBatch loss: 0.19672488793730736\nBatch loss: 0.0800867099314928\nBatch loss: 0.27980614453554153\nBatch loss: 0.23334408178925514\nBatch loss: 0.12650871649384499\nBatch loss: 0.6483020633459091\nBatch loss: 0.3968486934900284\nBatch loss: 0.07847118191421032\nBatch loss: 0.1997067965567112\nBatch loss: 0.15681743621826172\nBatch loss: 0.16823386773467064\nBatch loss: 0.7897567749023438\nBatch loss: 0.22882556542754173\nBatch loss: 0.3563671186566353\nBatch loss: 0.21390844136476517\nBatch loss: 0.24065732955932617\nBatch loss: 0.09743022732436657\nBatch loss: 0.11194610968232155\nBatch loss: 0.14173507690429688\nBatch loss: 0.22888517007231712\nBatch loss: 0.20826101303100586\nBatch loss: 0.18587112426757812\nBatch loss: 0.14259529300034046\nBatch loss: 0.12753677554428577\nBatch loss: 0.33536147326231003\nBatch loss: 0.2837686613202095\nBatch loss: 0.2109229564666748\nBatch loss: 0.1731257513165474\nBatch loss: 0.5857753753662109\nBatch loss: 0.0773434666916728\nBatch loss: 0.11158871464431286\nBatch loss: 0.3543863445520401\nBatch loss: 0.28751278296113014\nBatch loss: 0.22887898609042168\nBatch loss: 0.5947942659258842\nBatch loss: 0.11687231250107288\nBatch loss: 0.4533958435058594\nBatch loss: 0.3267708048224449\nBatch loss: 0.3416109085083008\nBatch loss: 0.40183544158935547\nBatch loss: 0.32682348042726517\nBatch loss: 0.4071035608649254\nBatch loss: 0.2378559112548828\nBatch loss: 0.7072372734546661\nBatch loss: 0.37211038172245026\nBatch loss: 0.10351181030273438\nBatch loss: 0.7528219372034073\nBatch loss: 0.27338219806551933\nBatch loss: 0.047279358841478825\nBatch loss: 0.13443565927445889\nBatch loss: 0.4884624481201172\nBatch loss: 0.04922223277390003\nBatch loss: 0.20766450092196465\nBatch loss: 0.14085006900131702\nBatch loss: 0.6135411188006401\nBatch loss: 0.05371475126594305\nBatch loss: 0.2422499656677246\nBatch loss: 0.34310292452573776\nBatch loss: 0.3251681476831436\nBatch loss: 0.10569286532700062\nBatch loss: 0.1491146069020033\nBatch loss: 0.5897197872400284\nBatch loss: 0.18273163586854935\nBatch loss: 0.23759841918945312\nBatch loss: 0.11393452063202858\nBatch loss: 0.9711151570081711\nBatch loss: 0.09584903717041016\nBatch loss: 0.24364281445741653\nBatch loss: 0.21586036309599876\nBatch loss: 0.17283057793974876\nBatch loss: 0.0440638093277812\nBatch loss: 0.11263466440141201\nBatch loss: 0.14046001248061657\nBatch loss: 0.34482669085264206\nBatch loss: 0.3780822828412056\nBatch loss: 1.0051088780164719\nBatch loss: 0.28293682262301445\nBatch loss: 0.3228144720196724\nBatch loss: 0.25809526443481445\nBatch loss: 0.7109846919775009\nBatch loss: 0.27756262570619583\nBatch loss: 0.3809395059943199\nBatch loss: 0.710570365190506\nBatch loss: 0.0756106385961175\nBatch loss: 0.5417513847351074\nBatch loss: 0.5727530643343925\nBatch loss: 0.09686899371445179\nBatch loss: 0.022400857415050268\nBatch loss: 0.10446715168654919\nBatch loss: 0.23383235558867455\nBatch loss: 0.0914840679615736\nBatch loss: 0.6193361431360245\nBatch loss: 0.12313509359955788\nBatch loss: 0.4653802141547203\nBatch loss: 0.19641071557998657\nBatch loss: 0.13743233866989613\nBatch loss: 0.05737042520195246\nBatch loss: 0.1876993291079998\nBatch loss: 1.0685410350561142\nBatch loss: 0.3678922727704048\nBatch loss: 0.25861000642180443\nBatch loss: 0.17374038696289062\nBatch loss: 0.18497753888368607\nBatch loss: 0.13623214326798916\nBatch loss: 0.07911133579909801\nBatch loss: 0.06658792495727539\nBatch loss: 0.09996605105698109\nBatch loss: 0.09589577093720436\nBatch loss: 0.032025338150560856\nBatch loss: 0.14384842477738857\nBatch loss: 0.2604041062295437\nBatch loss: 0.35274840891361237\nBatch loss: 0.2547264099121094\nBatch loss: 0.04736185073852539\nBatch loss: 0.2535710297524929\nBatch loss: 0.13848829083144665\nBatch loss: 0.46707630157470703\nBatch loss: 0.8774270862340927\nBatch loss: 0.014212608803063631\nBatch loss: 0.08543968200683594\nBatch loss: 0.461515448987484\nBatch loss: 0.05308699794113636\nBatch loss: 0.44994737952947617\nBatch loss: 0.13786506839096546\nBatch loss: 0.5548667907714844\nBatch loss: 0.2814660035073757\nBatch loss: 0.11834717355668545\nBatch loss: 0.177871473133564\nBatch loss: 0.04920578096061945\nBatch loss: 0.09750748053193092\nBatch loss: 0.3039994277060032\nBatch loss: 0.3618307039141655\nBatch loss: 0.28464555740356445\nBatch loss: 0.10127735324203968\nBatch loss: 0.31149351969361305\nBatch loss: 0.4470644146203995\nBatch loss: 0.14795995317399502\nBatch loss: 0.12521839700639248\nBatch loss: 0.08251381106674671\nBatch loss: 0.7037963718175888\nBatch loss: 0.3532838821411133\nBatch loss: 0.04777860827744007\nBatch loss: 0.15563249588012695\nBatch loss: 0.04265403840690851\nBatch loss: 0.024019242264330387\nBatch loss: 0.020766498055309057\nBatch loss: 0.08195877075195312\nBatch loss: 0.06476211827248335\nBatch loss: 0.4729042202234268\nBatch loss: 0.17642676830291748\nBatch loss: 0.3120565414428711\nBatch loss: 0.3853893280029297\nBatch loss: 0.2521677128970623\nBatch loss: 0.36556292325258255\nBatch loss: 0.7698951661586761\nBatch loss: 0.042550801299512386\nBatch loss: 0.3621978685259819\nBatch loss: 0.07924914360046387\nBatch loss: 0.3505382686853409\nBatch loss: 0.24644732475280762\nBatch loss: 0.29190635308623314\nBatch loss: 0.22569751366972923\nBatch loss: 0.044266702607274055\nBatch loss: 0.35877417773008347\nBatch loss: 0.047342777252197266\nBatch loss: 0.41257765144109726\nBatch loss: 0.054769041016697884\nBatch loss: 0.10644698515534401\nBatch loss: 0.5255303531885147\nBatch loss: 0.40430881083011627\nBatch loss: 0.043007852509617805\nBatch loss: 0.08632945828139782\nBatch loss: 0.28752362355589867\nBatch loss: 0.8594484627246857\nBatch loss: 0.1878361776471138\nBatch loss: 0.5633299425244331\nBatch loss: 0.2593388594686985\nBatch loss: 0.6206998974084854\nBatch loss: 0.08012557402253151\nBatch loss: 0.4938621446490288\nBatch loss: 0.25591230019927025\nBatch loss: 0.3131277486681938\nBatch loss: 0.796176940202713\nBatch loss: 0.5711059644818306\nBatch loss: 0.1173868216574192\nBatch loss: 0.6445827335119247\nBatch loss: 0.509759895503521\nBatch loss: 0.45027829706668854\nBatch loss: 0.031599283684045076\nBatch loss: 0.21975064650177956\nBatch loss: 0.07794165518134832\nBatch loss: 0.26370812207460403\nBatch loss: 0.24015570059418678\nBatch loss: 0.6656413525342941\nBatch loss: 0.3619227558374405\nBatch loss: 0.11685705743730068\nBatch loss: 0.4163479804992676\nBatch loss: 0.07657361216843128\nBatch loss: 0.19252777099609375\nBatch loss: 0.21801305934786797\nBatch loss: 0.4411029815673828\nBatch loss: 0.03914952278137207\nBatch loss: 0.13896942138671875\nBatch loss: 0.05944013595581055\nBatch loss: 0.03701281500980258\nBatch loss: 0.273419376462698\nBatch loss: 0.46373989433050156\nBatch loss: 0.14155126176774502\nBatch loss: 0.040954588912427425\nBatch loss: 0.19011545926332474\nBatch loss: 0.9775982052087784\nBatch loss: 0.47493912279605865\nBatch loss: 0.03312182379886508\nBatch loss: 0.06132889073342085\nBatch loss: 0.12605166994035244\nBatch loss: 0.22279739379882812\nBatch loss: 0.2828412130475044\nBatch loss: 0.29160797595977783\nBatch loss: 0.28508711606264114\nBatch loss: 0.04546308424323797\nBatch loss: 0.110564474016428\nBatch loss: 0.2582397498190403\nBatch loss: 0.17010152339935303\nBatch loss: 0.5905896425247192\nBatch loss: 0.34148503094911575\nBatch loss: 0.310547836124897\nBatch loss: 0.19489645957946777\nBatch loss: 0.09957695379853249\nBatch loss: 0.22244835272431374\nBatch loss: 0.07225966546684504\nBatch loss: 0.09431267157196999\nBatch loss: 0.22636890411376953\nBatch loss: 0.1811547391116619\nBatch loss: 0.051668169908225536\nBatch loss: 0.2614365890622139\nBatch loss: 0.6732139736413956\nBatch loss: 0.384831540286541\nBatch loss: 0.09233719669282436\nBatch loss: 0.10527277365326881\nBatch loss: 0.06699204444885254\nBatch loss: 0.30449677258729935\nBatch loss: 0.06861114408820868\nBatch loss: 0.4985504224896431\nBatch loss: 0.11238765902817249\nBatch loss: 0.31599950045347214\nBatch loss: 0.3675375133752823\nBatch loss: 0.03453517099842429\nBatch loss: 0.037191868759691715\nBatch loss: 0.11651706881821156\nBatch loss: 0.1373767852783203\nBatch loss: 0.39673473685979843\nBatch loss: 0.19617771729826927\nBatch loss: 0.7518761605024338\nBatch loss: 0.2929210662841797\nBatch loss: 0.39285358041524887\nBatch loss: 0.037868262734264135\nBatch loss: 0.1932220533490181\nBatch loss: 0.28069688007235527\nBatch loss: 0.08333444595336914\nBatch loss: 0.0739853410050273\nBatch loss: 0.17974186688661575\nBatch loss: 0.6135702133178711\nBatch loss: 0.16129827126860619\nBatch loss: 0.4391803964972496\nBatch loss: 0.14226245693862438\nBatch loss: 0.18104005604982376\nBatch loss: 0.2098994329571724\nBatch loss: 0.34809302538633347\nBatch loss: 0.27587080374360085\nBatch loss: 0.2077167108654976\nBatch loss: 0.07136869709938765\nBatch loss: 0.1680288277566433\nBatch loss: 0.06920242216438055\nBatch loss: 0.20165635272860527\nBatch loss: 0.29488373547792435\nBatch loss: 0.23298025131225586\nBatch loss: 0.04772854037582874\nBatch loss: 0.45960046350955963\nBatch loss: 0.24202346801757812\nBatch loss: 0.3511686250567436\nBatch loss: 0.056764367036521435\nBatch loss: 0.36420751363039017\nBatch loss: 0.5215897783637047\nBatch loss: 0.1524200476706028\nBatch loss: 0.18699383363127708\nBatch loss: 0.4964284971356392\nBatch loss: 0.3595004230737686\nBatch loss: 0.15354442410171032\nBatch loss: 0.10286427102982998\nBatch loss: 0.5087509378790855\nBatch loss: 0.12571907602250576\nBatch loss: 0.11296368204057217\nBatch loss: 0.13921785168349743\nBatch loss: 0.1738576963543892\nBatch loss: 0.09274697862565517\nBatch loss: 0.4458475112915039\nBatch loss: 0.28899718075990677\nBatch loss: 0.1233520545065403\nBatch loss: 0.12621975503861904\nBatch loss: 0.39788056164979935\nBatch loss: 0.3615589067339897\nBatch loss: 0.11158323846757412\nBatch loss: 0.25851083919405937\nBatch loss: 0.12563240714371204\nBatch loss: 0.38637686520814896\nBatch loss: 0.25014305487275124\nBatch loss: 0.44292688369750977\nBatch loss: 0.21963311359286308\nBatch loss: 0.18605351448059082\nBatch loss: 0.19671201705932617\nBatch loss: 0.42391445487737656\nBatch loss: 0.44467784464359283\nBatch loss: 0.19957734271883965\nBatch loss: 0.2681375853717327\nBatch loss: 0.09355700574815273\nBatch loss: 0.07747638504952192\nBatch loss: 0.21074866876006126\nBatch loss: 0.2294149436056614\nBatch loss: 0.09570193476974964\nBatch loss: 0.24077797308564186\nBatch loss: 0.180178415030241\nBatch loss: 0.3589353710412979\nBatch loss: 0.07778835482895374\nBatch loss: 0.2446289174258709\nBatch loss: 0.04810446407645941\nBatch loss: 0.2995314635336399\nBatch loss: 0.09480095468461514\nBatch loss: 0.1848926581442356\nBatch loss: 0.08155727759003639\nBatch loss: 0.23592030629515648\nBatch loss: 0.347319133579731\nBatch loss: 0.22054482251405716\nBatch loss: 0.22990847006440163\nBatch loss: 0.15733063220977783\nBatch loss: 0.08691072463989258\nBatch loss: 0.3493623808026314\nBatch loss: 0.40504932403564453\nBatch loss: 0.1860804669559002\nBatch loss: 0.3811084106564522\nBatch loss: 0.09302044287323952\nBatch loss: 0.18193436786532402\nBatch loss: 0.13938761316239834\nBatch loss: 0.19179536029696465\nBatch loss: 0.2645847760140896\nBatch loss: 0.6858434528112411\nBatch loss: 0.4878345876932144\nBatch loss: 0.39531420916318893\nBatch loss: 0.16743207350373268\nBatch loss: 0.48781145364046097\nBatch loss: 0.05143511574715376\nBatch loss: 0.5127282068133354\nBatch loss: 0.12510073371231556\nBatch loss: 0.08640157990157604\nBatch loss: 0.3735686466097832\nBatch loss: 0.26645660400390625\nBatch loss: 0.06886386778205633\nBatch loss: 0.609249584376812\nBatch loss: 0.07760429289191961\nBatch loss: 0.3662627935409546\nBatch loss: 0.26650333777070045\nBatch loss: 0.20421195775270462\nBatch loss: 0.3953910991549492\nBatch loss: 0.5669097974896431\nBatch loss: 0.3656056150794029\nBatch loss: 0.3998744487762451\nBatch loss: 0.441664457321167\nBatch loss: 0.26016760617494583\nBatch loss: 0.13917261734604836\nBatch loss: 0.11810875497758389\nBatch loss: 0.16823410987854004\nBatch loss: 0.1241383608430624\nBatch loss: 0.06178617477416992\nBatch loss: 0.4522109031677246\nBatch loss: 0.2953813783824444\nBatch loss: 0.22550582885742188\nBatch loss: 0.2642669714987278\nBatch loss: 0.13851619325578213\nBatch loss: 0.4091690853238106\nBatch loss: 0.17524337396025658\nBatch loss: 0.3884868696331978\nBatch loss: 0.20873451605439186\nBatch loss: 0.11093258857727051\nBatch loss: 0.1222529448568821\nBatch loss: 0.5053112655878067\nBatch loss: 0.4314393922686577\nBatch loss: 0.1618807390332222\nBatch loss: 0.06715965457260609\nBatch loss: 0.14650220051407814\nBatch loss: 0.06542790215462446\nBatch loss: 0.17623519524931908\nBatch loss: 0.21469105035066605\nBatch loss: 0.3129011392593384\nBatch loss: 0.2800617180764675\nBatch loss: 0.24238109588623047\nBatch loss: 0.6621026992797852\nBatch loss: 0.45992471277713776\nBatch loss: 0.16488730907440186\nBatch loss: 0.8593640476465225\nBatch loss: 0.7618045806884766\nBatch loss: 0.20266568288207054\nBatch loss: 0.06817627232521772\nBatch loss: 0.6673827022314072\nBatch loss: 0.36430981010198593\nBatch loss: 0.06780469324439764\nBatch loss: 0.26102781295776367\nBatch loss: 0.08058330975472927\nBatch loss: 0.45269232243299484\nBatch loss: 0.39362668991088867\nBatch loss: 0.3993765264749527\nBatch loss: 0.12188839726150036\nBatch loss: 0.3973717615008354\nBatch loss: 0.09549331851303577\nBatch loss: 0.0714673986658454\nBatch loss: 0.1365289743989706\nBatch loss: 0.044864388182759285\nBatch loss: 0.14105821028351784\nBatch loss: 0.4358568415045738\nBatch loss: 0.38162875920534134\nBatch loss: 0.5799686908721924\nBatch loss: 0.021538734436035156\nBatch loss: 0.25296473875641823\nBatch loss: 0.06698328536003828\nBatch loss: 0.11976814828813076\nBatch loss: 0.20684434100985527\nBatch loss: 0.07225799839943647\nBatch loss: 0.5278196558356285\nBatch loss: 0.34025099128484726\nBatch loss: 0.143929747864604\nBatch loss: 0.013528823619708419\nBatch loss: 0.0756042031571269\nBatch loss: 0.0601615896448493\nBatch loss: 0.23581696674227715\nBatch loss: 0.13549727387726307\nBatch loss: 0.031566142570227385\nBatch loss: 0.24231553077697754\nBatch loss: 0.1981215551495552\nBatch loss: 0.2576105669140816\nBatch loss: 0.1263027172535658\nBatch loss: 0.04728985019028187\nBatch loss: 0.4332871362566948\nBatch loss: 0.5863585695624352\nBatch loss: 0.24023056030273438\nBatch loss: 0.0422139186412096\nBatch loss: 0.2001972310245037\nBatch loss: 0.2652280405163765\nBatch loss: 0.08566237054765224\nBatch loss: 0.07718229200690985\nBatch loss: 0.29740143567323685\nBatch loss: 0.07291936781257391\nBatch loss: 0.324607752263546\nBatch loss: 0.2556269243359566\nBatch loss: 0.08774900808930397\nBatch loss: 0.07838058285415173\nBatch loss: 0.13967943377792835\nBatch loss: 0.04676616284996271\nBatch loss: 0.1105046272277832\nBatch loss: 0.4556990787386894\nBatch loss: 0.2572357654571533\nBatch loss: 0.02903080079704523\nBatch loss: 0.03637433052062988\nBatch loss: 0.10017300024628639\nBatch loss: 0.4350602626800537\nBatch loss: 0.3179195150732994\nBatch loss: 0.09440243244171143\nBatch loss: 0.434257872402668\nBatch loss: 0.06219935603439808\nBatch loss: 0.08173364214599133\nBatch loss: 0.024568678345531225\nBatch loss: 0.18714338541030884\nBatch loss: 0.19783258438110352\nBatch loss: 0.1784532144665718\nBatch loss: 0.10393762961030006\nBatch loss: 0.06923711393028498\nBatch loss: 0.03544211387634277\nBatch loss: 0.11069536209106445\nBatch loss: 0.05193281453102827\nBatch loss: 0.3243391588330269\nBatch loss: 0.15787316486239433\nBatch loss: 0.016982912784442306\nBatch loss: 0.3422870859503746\nBatch loss: 0.10358775034546852\nBatch loss: 0.04105139058083296\nBatch loss: 0.3414139896631241\nBatch loss: 0.12353754602372646\nBatch loss: 0.042833806946873665\nBatch loss: 0.11768102645874023\nBatch loss: 0.1236261148005724\nBatch loss: 0.24263430386781693\nBatch loss: 0.6392937153577805\nBatch loss: 0.12190962210297585\nBatch loss: 0.03878212068229914\nBatch loss: 0.22974824532866478\nBatch loss: 0.03236842108890414\nBatch loss: 0.517134927213192\nBatch loss: 0.17937183380126953\nBatch loss: 0.31250037252902985\nBatch loss: 0.12820529751479626\nBatch loss: 0.7216224819421768\nBatch loss: 0.018714905017986894\nBatch loss: 0.42174816131591797\nBatch loss: 0.24504268541932106\nBatch loss: 0.44954922050237656\nBatch loss: 0.17253970727324486\nBatch loss: 0.061881779693067074\nBatch loss: 0.047609806060791016\nBatch loss: 0.09707236662507057\nBatch loss: 0.5110464245080948\nBatch loss: 0.06462651770561934\nBatch loss: 0.027282715309411287\nBatch loss: 0.13019919395446777\nBatch loss: 0.6022505834698677\nBatch loss: 0.6438002735376358\nBatch loss: 0.10093403048813343\nBatch loss: 0.01854980015195906\nBatch loss: 0.475279800593853\nBatch loss: 0.02382040023803711\nBatch loss: 0.7045860588550568\nBatch loss: 0.07853102870285511\nBatch loss: 0.28105735778808594\nBatch loss: 0.38820933550596237\nBatch loss: 0.40774550288915634\nBatch loss: 0.029876232147216797\nBatch loss: 0.1483631134033203\nBatch loss: 0.39073754101991653\nBatch loss: 0.4486246034502983\nBatch loss: 0.07698357105255127\nBatch loss: 0.19533896818757057\nBatch loss: 0.36847736686468124\nBatch loss: 0.29472922906279564\nBatch loss: 0.38554001599550247\nEpoch 5/7\nBatch loss: 0.11140704154968262\nBatch loss: 0.44277120381593704\nBatch loss: 0.21640682592988014\nBatch loss: 0.12442469596862793\nBatch loss: 0.45542337000370026\nBatch loss: 0.4406724125146866\nBatch loss: 1.2926511466503143\nBatch loss: 0.6176884099841118\nBatch loss: 0.8162813633680344\nBatch loss: 0.09596014395356178\nBatch loss: 0.38449954241514206\nBatch loss: 0.30308688059449196\nBatch loss: 0.033423423301428556\nBatch loss: 0.2728548087179661\nBatch loss: 0.1372208632528782\nBatch loss: 0.020880221854895353\nBatch loss: 0.16643429175019264\nBatch loss: 0.06501126568764448\nBatch loss: 0.327058807015419\nBatch loss: 0.018920422298833728\nBatch loss: 0.07021880242973566\nBatch loss: 0.1636028289794922\nBatch loss: 0.23642921820282936\nBatch loss: 0.06123638246208429\nBatch loss: 0.605841763317585\nBatch loss: 0.04513275809586048\nBatch loss: 0.09209346957504749\nBatch loss: 0.35157348960638046\nBatch loss: 0.17138957977294922\nBatch loss: 0.5582575872540474\nBatch loss: 0.03917503636330366\nBatch loss: 0.34682657569646835\nBatch loss: 0.22676659747958183\nBatch loss: 0.8140275627374649\nBatch loss: 0.3148745372891426\nBatch loss: 0.17546653747558594\nBatch loss: 0.0896985549479723\nBatch loss: 0.021828652825206518\nBatch loss: 0.6716346740722656\nBatch loss: 0.2706744708120823\nBatch loss: 0.3566126897931099\nBatch loss: 0.0854182243347168\nBatch loss: 0.2069571055471897\nBatch loss: 0.3544476255774498\nBatch loss: 0.11532807722687721\nBatch loss: 0.552486889064312\nBatch loss: 0.7300815731287003\nBatch loss: 0.0847470760345459\nBatch loss: 0.03692269325256348\nBatch loss: 0.17701053991913795\nBatch loss: 0.3092942200601101\nBatch loss: 0.32820701599121094\nBatch loss: 0.03258514450863004\nBatch loss: 0.18378471955657005\nBatch loss: 0.11788702569901943\nBatch loss: 0.07860136218369007\nBatch loss: 0.5517114326357841\nBatch loss: 0.2188565768301487\nBatch loss: 0.0250399112701416\nBatch loss: 0.18340302631258965\nBatch loss: 0.11529278941452503\nBatch loss: 0.10799980722367764\nBatch loss: 0.2084503136575222\nBatch loss: 0.1310057658702135\nBatch loss: 0.13840890489518642\nBatch loss: 0.5220335721969604\nBatch loss: 0.07161521818488836\nBatch loss: 0.0487291207537055\nBatch loss: 0.5489861965179443\nBatch loss: 0.17314363270998\nBatch loss: 0.22067977115511894\nBatch loss: 0.43011557310819626\nBatch loss: 0.3150523826479912\nBatch loss: 0.07218265440315008\nBatch loss: 0.07986831478774548\nBatch loss: 0.06335997488349676\nBatch loss: 0.6078186258673668\nBatch loss: 0.2802734449505806\nBatch loss: 0.542176254093647\nBatch loss: 0.045106890611350536\nBatch loss: 0.124969482421875\nBatch loss: 0.10604453273117542\nBatch loss: 0.20749522373080254\nBatch loss: 0.1181910652667284\nBatch loss: 0.10332822799682617\nBatch loss: 0.05362296011298895\nBatch loss: 0.08460569195449352\nBatch loss: 0.02990245819091797\nBatch loss: 0.8008346706628799\nBatch loss: 0.07781744003295898\nBatch loss: 0.25983238592743874\nBatch loss: 0.133735416457057\nBatch loss: 0.05299472715705633\nBatch loss: 0.07626146078109741\nBatch loss: 0.06759107112884521\nBatch loss: 0.07457637693732977\nBatch loss: 0.16776204109191895\nBatch loss: 0.4951193556189537\nBatch loss: 0.5479917675256729\nBatch loss: 0.21778656169772148\nBatch loss: 0.023618699051439762\nBatch loss: 0.2805204503238201\nBatch loss: 0.054604532197117805\nBatch loss: 0.05435753148049116\nBatch loss: 0.4225349426269531\nBatch loss: 0.1272223051637411\nBatch loss: 0.13583803549408913\nBatch loss: 0.18815422430634499\nBatch loss: 0.31649578362703323\nBatch loss: 0.07963228039443493\nBatch loss: 0.2205958403646946\nBatch loss: 0.623868964612484\nBatch loss: 0.11217975988984108\nBatch loss: 0.0658118724822998\nBatch loss: 0.059499265626072884\nBatch loss: 0.24924039840698242\nBatch loss: 0.21601486951112747\nBatch loss: 0.044868472032248974\nBatch loss: 0.15976238995790482\nBatch loss: 0.06876063533127308\nBatch loss: 0.14586853794753551\nBatch loss: 0.15133810229599476\nBatch loss: 0.7471159100532532\nBatch loss: 0.2534055709838867\nBatch loss: 0.6472492218017578\nBatch loss: 0.18265414983034134\nBatch loss: 0.12943172827363014\nBatch loss: 0.19563082605600357\nBatch loss: 0.3713374212384224\nBatch loss: 0.3238043934106827\nBatch loss: 0.07935464382171631\nBatch loss: 0.6473255157470703\nBatch loss: 0.4611968994140625\nBatch loss: 0.26533056050539017\nBatch loss: 0.04427134990692139\nBatch loss: 0.03202366875484586\nBatch loss: 0.2111196517944336\nBatch loss: 0.16594721004366875\nBatch loss: 0.2805156819522381\nBatch loss: 0.06433188915252686\nBatch loss: 0.171645637601614\nBatch loss: 0.03963518422096968\nBatch loss: 0.5664439126849174\nBatch loss: 0.0648343563079834\nBatch loss: 0.20174741744995117\nBatch loss: 0.5500557646155357\nBatch loss: 0.697992593050003\nBatch loss: 0.0344240665435791\nBatch loss: 0.03135418985038996\nBatch loss: 0.10331940837204456\nBatch loss: 0.08452988229691982\nBatch loss: 0.4355046898126602\nBatch loss: 0.2799069881439209\nBatch loss: 0.05311840679496527\nBatch loss: 0.016010880935937166\nBatch loss: 0.14254760928452015\nBatch loss: 0.05328082945197821\nBatch loss: 0.027698040939867496\nBatch loss: 0.2973785437643528\nBatch loss: 0.08900070562958717\nBatch loss: 0.05168330855667591\nBatch loss: 0.13622522354125977\nBatch loss: 0.24429036304354668\nBatch loss: 0.22133802995085716\nBatch loss: 0.21174145862460136\nBatch loss: 0.04506540484726429\nBatch loss: 0.27344871312379837\nBatch loss: 0.14103484340012074\nBatch loss: 0.10978984646499157\nBatch loss: 0.21883249282836914\nBatch loss: 0.3026287630200386\nBatch loss: 0.22358370944857597\nBatch loss: 0.15807438641786575\nBatch loss: 0.20056819543242455\nBatch loss: 0.34023046493530273\nBatch loss: 0.09909707121551037\nBatch loss: 0.1693481206893921\nBatch loss: 0.22064018994569778\nBatch loss: 0.2564551867544651\nBatch loss: 0.1909288763999939\nBatch loss: 0.3699558973312378\nBatch loss: 0.10733223520219326\nBatch loss: 0.029451013542711735\nBatch loss: 0.10373449884355068\nBatch loss: 0.11006265878677368\nBatch loss: 0.19167138263583183\nBatch loss: 0.06357765290886164\nBatch loss: 1.1406486481428146\nBatch loss: 0.022176147904247046\nBatch loss: 0.08819389156997204\nBatch loss: 0.3274836763739586\nBatch loss: 0.16707992181181908\nBatch loss: 0.437077060341835\nBatch loss: 0.0688858050853014\nBatch loss: 0.20750045776367188\nBatch loss: 0.19959140568971634\nBatch loss: 0.16968011856079102\nBatch loss: 0.27915215119719505\nBatch loss: 0.05870711989700794\nBatch loss: 0.21154820919036865\nBatch loss: 0.011317491298541427\nBatch loss: 0.1488338690251112\nBatch loss: 0.2082538604736328\nBatch loss: 0.4011540487408638\nBatch loss: 0.16730977222323418\nBatch loss: 0.29444683343172073\nBatch loss: 0.16946274787187576\nBatch loss: 0.18705744296312332\nBatch loss: 0.07222581189125776\nBatch loss: 0.2272598259150982\nBatch loss: 0.10346484370529652\nBatch loss: 0.4071993753314018\nBatch loss: 0.049706222489476204\nBatch loss: 0.031822563614696264\nBatch loss: 0.31241441145539284\nBatch loss: 0.06539035122841597\nBatch loss: 0.2542738988995552\nBatch loss: 0.020037502981722355\nBatch loss: 0.17638517543673515\nBatch loss: 0.40193237364292145\nBatch loss: 0.03131914185360074\nBatch loss: 0.625244602560997\nBatch loss: 0.08381676860153675\nBatch loss: 0.2144947089254856\nBatch loss: 0.3634662553668022\nBatch loss: 0.05716359708458185\nBatch loss: 0.2265012264251709\nBatch loss: 0.09295391850173473\nBatch loss: 0.0698509206995368\nBatch loss: 0.2139451541006565\nBatch loss: 0.8885598182678223\nBatch loss: 0.05530500318855047\nBatch loss: 0.13666379265487194\nBatch loss: 0.1014788169413805\nBatch loss: 0.10729980655014515\nBatch loss: 0.05141997244209051\nBatch loss: 0.051755430176854134\nBatch loss: 0.2626800537109375\nBatch loss: 0.13414478860795498\nBatch loss: 0.12113774195313454\nBatch loss: 0.19236791878938675\nBatch loss: 0.20864678546786308\nBatch loss: 0.10660029016435146\nBatch loss: 0.3275008127093315\nBatch loss: 0.23997116833925247\nBatch loss: 0.38263536989688873\nBatch loss: 0.017241478199139237\nBatch loss: 0.6495551019906998\nBatch loss: 0.3433961793780327\nBatch loss: 0.33135034143924713\nBatch loss: 0.015141487820073962\nBatch loss: 0.5601515993475914\nBatch loss: 0.11250019073486328\nBatch loss: 0.14206481166183949\nBatch loss: 0.09062600322067738\nBatch loss: 0.014234305126592517\nBatch loss: 0.5624282360076904\nBatch loss: 0.05179786588996649\nBatch loss: 0.03199434373527765\nBatch loss: 0.6020500883460045\nBatch loss: 0.06365108769387007\nBatch loss: 0.9971191734075546\nBatch loss: 0.19400406628847122\nBatch loss: 0.45192789286375046\nBatch loss: 0.6654035300016403\nBatch loss: 0.8393316715955734\nBatch loss: 0.02857637358829379\nBatch loss: 0.1821610890328884\nBatch loss: 0.28874803334474564\nBatch loss: 0.34170519560575485\nBatch loss: 0.025077105965465307\nBatch loss: 0.2144891582429409\nBatch loss: 0.2004241943359375\nBatch loss: 0.18004966899752617\nBatch loss: 0.21816348657011986\nBatch loss: 0.4708299785852432\nBatch loss: 0.3960144519805908\nBatch loss: 0.15604818239808083\nBatch loss: 0.032232762314379215\nBatch loss: 0.05817913915961981\nBatch loss: 0.03607749938964844\nBatch loss: 0.11901402845978737\nBatch loss: 0.39664365351200104\nBatch loss: 0.21427756175398827\nBatch loss: 0.1444320660084486\nBatch loss: 0.45914579182863235\nBatch loss: 0.27034807950258255\nBatch loss: 0.12520886026322842\nBatch loss: 0.49066033214330673\nBatch loss: 0.02696955343708396\nBatch loss: 0.09972977451980114\nBatch loss: 0.232611782848835\nBatch loss: 0.10644900612533092\nBatch loss: 0.4375159740447998\nBatch loss: 0.4785957559943199\nBatch loss: 0.11058593168854713\nBatch loss: 0.03354239510372281\nBatch loss: 0.06510710809379816\nBatch loss: 0.10739060118794441\nBatch loss: 0.1763669215142727\nBatch loss: 0.24343598634004593\nBatch loss: 0.27810145169496536\nBatch loss: 0.05420184228569269\nBatch loss: 0.2746143378317356\nBatch loss: 0.09484863840043545\nBatch loss: 0.02921080682426691\nBatch loss: 0.47457434237003326\nBatch loss: 0.10208511725068092\nBatch loss: 0.04408979322761297\nBatch loss: 0.24838536977767944\nBatch loss: 0.42086925357580185\nBatch loss: 0.09475446306169033\nBatch loss: 0.2074441872537136\nBatch loss: 0.09126299992203712\nBatch loss: 0.3163471445441246\nBatch loss: 0.049608233384788036\nBatch loss: 0.027377367950975895\nBatch loss: 0.129462955519557\nBatch loss: 0.3277754783630371\nBatch loss: 0.4294166713953018\nBatch loss: 0.5951216444373131\nBatch loss: 0.020255804993212223\nBatch loss: 0.3779958561062813\nBatch loss: 0.014396578771993518\nBatch loss: 0.1397781353443861\nBatch loss: 0.7302212715148926\nBatch loss: 0.18120909109711647\nBatch loss: 0.49833059310913086\nBatch loss: 0.15117264352738857\nBatch loss: 0.11548264883458614\nBatch loss: 0.22581672295928001\nBatch loss: 0.048041343688964844\nBatch loss: 0.40473174303770065\nBatch loss: 0.13226890936493874\nBatch loss: 0.11251878924667835\nBatch loss: 0.602373480796814\nBatch loss: 0.10926199145615101\nBatch loss: 0.2291279472410679\nBatch loss: 0.2860410325229168\nBatch loss: 0.070696952752769\nBatch loss: 0.5805733427405357\nBatch loss: 0.16976697370409966\nBatch loss: 0.02254629274830222\nBatch loss: 0.1412057876586914\nBatch loss: 0.18361080437898636\nBatch loss: 0.12560940347611904\nBatch loss: 0.10308361612260342\nBatch loss: 0.033393860794603825\nBatch loss: 0.16486262902617455\nBatch loss: 0.4509136825799942\nBatch loss: 0.43173737823963165\nBatch loss: 0.20026301965117455\nBatch loss: 0.594613328576088\nBatch loss: 0.21076012402772903\nBatch loss: 0.04147482104599476\nBatch loss: 0.36104824393987656\nBatch loss: 0.6080141291022301\nBatch loss: 0.2600431442260742\nBatch loss: 0.08115994744002819\nBatch loss: 0.038003684021532536\nBatch loss: 0.04977965261787176\nBatch loss: 0.09559119120240211\nBatch loss: 0.15581632032990456\nBatch loss: 0.2909250371158123\nBatch loss: 0.10570776648819447\nBatch loss: 0.42131807655096054\nBatch loss: 0.14044314622879028\nBatch loss: 0.08999633602797985\nBatch loss: 0.1571338251233101\nBatch loss: 0.3891110420227051\nBatch loss: 0.11908936314284801\nBatch loss: 0.2641911618411541\nBatch loss: 0.1071014441549778\nBatch loss: 0.5283899232745171\nBatch loss: 1.1852963268756866\nBatch loss: 0.5326957628130913\nBatch loss: 0.24464035406708717\nBatch loss: 0.5946526676416397\nBatch loss: 0.4919455200433731\nBatch loss: 0.04000043962150812\nBatch loss: 0.2870340459048748\nBatch loss: 0.044274539686739445\nBatch loss: 0.45473169535398483\nBatch loss: 0.1657559908926487\nBatch loss: 0.16130734235048294\nBatch loss: 0.11416584253311157\nBatch loss: 0.12153577990829945\nBatch loss: 0.06607294082641602\nBatch loss: 0.063946726731956\nBatch loss: 0.02017152262851596\nBatch loss: 0.3188002109527588\nBatch loss: 0.3210340812802315\nBatch loss: 0.04175209905952215\nBatch loss: 0.254946481436491\nBatch loss: 0.42757607996463776\nBatch loss: 0.36549951881170273\nBatch loss: 0.0985265988856554\nBatch loss: 0.029708624351769686\nBatch loss: 0.0423541059717536\nBatch loss: 0.16768265515565872\nBatch loss: 0.020597220864146948\nBatch loss: 0.058016302064061165\nBatch loss: 0.20608901977539062\nBatch loss: 0.06939965765923262\nBatch loss: 0.22575760260224342\nBatch loss: 0.0558129558339715\nBatch loss: 0.29738331213593483\nBatch loss: 0.1477603893727064\nBatch loss: 0.04916334059089422\nBatch loss: 0.22453665733337402\nBatch loss: 0.3380432352423668\nBatch loss: 0.020056248176842928\nBatch loss: 0.09093356318771839\nBatch loss: 0.04700005054473877\nBatch loss: 0.05724621005356312\nBatch loss: 0.04261970520019531\nBatch loss: 0.1710495911538601\nBatch loss: 0.08214938454329967\nBatch loss: 0.06714809220284224\nBatch loss: 0.15796875581145287\nBatch loss: 0.18789101392030716\nBatch loss: 0.06786179728806019\nBatch loss: 1.0506706684827805\nBatch loss: 0.003855556424241513\nBatch loss: 0.02472663065418601\nBatch loss: 0.07262131664901972\nBatch loss: 0.05550813861191273\nBatch loss: 0.115671930834651\nBatch loss: 0.1511149387806654\nBatch loss: 0.09085274301469326\nBatch loss: 0.0619740504771471\nBatch loss: 0.15599608421325684\nBatch loss: 0.5363097414374352\nBatch loss: 0.3834012895822525\nBatch loss: 0.13358903117477894\nBatch loss: 0.4383384808897972\nBatch loss: 0.07455432321876287\nBatch loss: 0.08282042108476162\nBatch loss: 0.5213193967938423\nBatch loss: 0.04732155706733465\nBatch loss: 0.0782854575663805\nBatch loss: 0.3159001097083092\nBatch loss: 0.13313925825059414\nBatch loss: 0.023934722412377596\nBatch loss: 0.23979723453521729\nBatch loss: 0.40450572967529297\nBatch loss: 0.507817268371582\nBatch loss: 0.13963013887405396\nBatch loss: 0.5122995376586914\nBatch loss: 0.3043009713292122\nBatch loss: 0.029021145310252905\nBatch loss: 0.14137357473373413\nBatch loss: 0.010866880184039474\nBatch loss: 0.39267491549253464\nBatch loss: 0.79311303794384\nBatch loss: 0.016739427810534835\nBatch loss: 0.22335875779390335\nBatch loss: 0.015130162937566638\nBatch loss: 0.6508579105138779\nBatch loss: 0.022176981437951326\nBatch loss: 0.23575210943818092\nBatch loss: 0.021174312569200993\nBatch loss: 0.053383051417768\nBatch loss: 0.6496810913085938\nBatch loss: 0.6366401165723801\nBatch loss: 0.03910732455551624\nBatch loss: 0.020570873748511076\nBatch loss: 0.17966557294130325\nBatch loss: 0.7357974350452423\nBatch loss: 0.10821294970810413\nBatch loss: 0.030972480308264494\nBatch loss: 0.24980545043945312\nBatch loss: 0.041880845092236996\nBatch loss: 0.4549771547317505\nBatch loss: 0.3099883906543255\nBatch loss: 0.5756191164255142\nBatch loss: 0.08284389972686768\nBatch loss: 0.5177602916955948\nBatch loss: 0.019247293239459395\nBatch loss: 0.6688567996025085\nBatch loss: 0.23848628625273705\nBatch loss: 0.13020670972764492\nBatch loss: 0.02268505049869418\nBatch loss: 0.036703587975353\nBatch loss: 0.11711359024047852\nBatch loss: 0.07353115361183882\nBatch loss: 0.19624615088105202\nBatch loss: 0.2738838270306587\nBatch loss: 0.012388468021526933\nBatch loss: 0.4033532366156578\nBatch loss: 0.11322331614792347\nBatch loss: 0.14041388407349586\nBatch loss: 0.04608517978340387\nBatch loss: 0.08100462146103382\nBatch loss: 0.026814937591552734\nBatch loss: 0.03702497575432062\nBatch loss: 0.025291205383837223\nBatch loss: 0.37717103958129883\nBatch loss: 0.033662617206573486\nBatch loss: 0.03241205355152488\nBatch loss: 0.2890458144247532\nBatch loss: 0.1650451496243477\nBatch loss: 0.970357283949852\nBatch loss: 0.22707080468535423\nBatch loss: 0.2894486300647259\nBatch loss: 0.06873345468193293\nBatch loss: 0.4790031909942627\nBatch loss: 0.16540825366973877\nBatch loss: 0.5726270750164986\nBatch loss: 0.03792667528614402\nBatch loss: 0.09490824304521084\nBatch loss: 0.4282226786017418\nBatch loss: 0.44367551803588867\nBatch loss: 0.6383746117353439\nBatch loss: 0.06565690040588379\nBatch loss: 0.6011190637946129\nBatch loss: 0.01650476478971541\nBatch loss: 0.5738123878836632\nBatch loss: 0.43585706502199173\nBatch loss: 0.28057193383574486\nBatch loss: 0.10664070025086403\nBatch loss: 0.5575511604547501\nBatch loss: 0.47278475016355515\nBatch loss: 0.06164908409118652\nBatch loss: 0.39284802973270416\nBatch loss: 0.12148380279541016\nBatch loss: 0.03877520561218262\nBatch loss: 0.05570626351982355\nBatch loss: 0.4380262643098831\nBatch loss: 0.006687164423055947\nBatch loss: 0.3606843948364258\nBatch loss: 0.2106163464486599\nBatch loss: 0.032484771218150854\nBatch loss: 0.1021125353872776\nBatch loss: 0.16652792692184448\nBatch loss: 0.12347841635346413\nBatch loss: 0.39896585047245026\nBatch loss: 0.47526001930236816\nBatch loss: 0.4568951204419136\nBatch loss: 0.058141290210187435\nBatch loss: 0.426524393260479\nBatch loss: 0.11700845323503017\nBatch loss: 0.06041526794433594\nBatch loss: 0.18500471487641335\nBatch loss: 0.14735865406692028\nBatch loss: 0.04444205667823553\nBatch loss: 0.2235150896012783\nBatch loss: 0.3282494470477104\nBatch loss: 0.3872547298669815\nBatch loss: 0.052565098740160465\nBatch loss: 0.33634014427661896\nBatch loss: 0.12164878658950329\nBatch loss: 0.10201073251664639\nBatch loss: 0.16951942816376686\nBatch loss: 0.5032234266400337\nBatch loss: 0.21375179290771484\nBatch loss: 0.07662391755729914\nBatch loss: 0.07713508792221546\nBatch loss: 0.04069185350090265\nBatch loss: 0.09341144934296608\nBatch loss: 0.23457635194063187\nBatch loss: 0.23352241143584251\nBatch loss: 0.2885122410953045\nBatch loss: 0.015802144771441817\nBatch loss: 0.04103326704353094\nBatch loss: 0.02089297864586115\nBatch loss: 0.2539733983576298\nBatch loss: 0.13167167082428932\nBatch loss: 0.4280843958258629\nBatch loss: 0.2101130597293377\nBatch loss: 0.06379580590873957\nBatch loss: 0.2635069005191326\nBatch loss: 0.1762714423239231\nBatch loss: 0.10276067070662975\nBatch loss: 0.10123503394424915\nBatch loss: 0.627070888876915\nBatch loss: 0.5593996122479439\nBatch loss: 0.11179471388459206\nBatch loss: 0.4288773611187935\nBatch loss: 0.2809181250631809\nBatch loss: 0.6243934854865074\nBatch loss: 0.11368799023330212\nBatch loss: 0.3448505699634552\nBatch loss: 0.05952000617980957\nBatch loss: 0.41524317115545273\nBatch loss: 0.07290744688361883\nBatch loss: 0.5368254333734512\nBatch loss: 0.09351730346679688\nBatch loss: 0.20139742642641068\nBatch loss: 0.23985052481293678\nBatch loss: 0.17424775287508965\nBatch loss: 0.15542912296950817\nBatch loss: 0.04488516133278608\nBatch loss: 0.3383345529437065\nBatch loss: 0.11897373013198376\nBatch loss: 0.044024111703038216\nBatch loss: 0.3187413141131401\nBatch loss: 0.40206432342529297\nBatch loss: 0.51250409334898\nBatch loss: 0.058346809819340706\nBatch loss: 0.12788409367203712\nBatch loss: 0.40866564959287643\nBatch loss: 0.9911184757947922\nBatch loss: 0.07455599494278431\nBatch loss: 0.16458511352539062\nBatch loss: 0.37926264107227325\nBatch loss: 0.8989949524402618\nBatch loss: 0.3635730966925621\nBatch loss: 0.45868225395679474\nBatch loss: 0.3035886399447918\nBatch loss: 0.31828880310058594\nBatch loss: 0.10695076547563076\nBatch loss: 0.04374408628791571\nBatch loss: 0.1900452934205532\nBatch loss: 0.31905461102724075\nBatch loss: 0.15397286973893642\nBatch loss: 0.14454436488449574\nBatch loss: 0.12058568187057972\nBatch loss: 0.21200085058808327\nBatch loss: 0.02262592315673828\nBatch loss: 0.47860492020845413\nBatch loss: 0.0387877831235528\nBatch loss: 0.06650257389992476\nBatch loss: 0.7687006145715714\nBatch loss: 0.04212904255837202\nBatch loss: 0.061321379616856575\nBatch loss: 0.3027689456939697\nBatch loss: 0.1521005667746067\nBatch loss: 0.053846868686378\nBatch loss: 0.1697690598666668\nBatch loss: 0.10507774539291859\nBatch loss: 0.03342193318530917\nBatch loss: 0.16740156337618828\nBatch loss: 0.38527119904756546\nBatch loss: 0.1269056834280491\nBatch loss: 0.04403162281960249\nBatch loss: 0.07122796960175037\nBatch loss: 0.33179450780153275\nBatch loss: 0.44658899307250977\nBatch loss: 0.1383585948497057\nBatch loss: 0.0442349910736084\nBatch loss: 0.1892104186117649\nBatch loss: 0.07110625505447388\nBatch loss: 0.25261832401156425\nBatch loss: 0.04378020763397217\nBatch loss: 0.029161334969103336\nBatch loss: 0.23769522085785866\nBatch loss: 0.3014194965362549\nBatch loss: 0.12288475409150124\nBatch loss: 0.0708345789462328\nBatch loss: 0.06674194242805243\nBatch loss: 0.1159139908850193\nBatch loss: 0.12674713507294655\nBatch loss: 0.23301124572753906\nBatch loss: 0.7244439423084259\nBatch loss: 0.020572661887854338\nBatch loss: 0.6592724472284317\nBatch loss: 0.06358671467751265\nBatch loss: 0.04646778106689453\nBatch loss: 0.5060267448425293\nBatch loss: 0.14136457815766335\nBatch loss: 0.44222544878721237\nBatch loss: 0.052519324235618114\nBatch loss: 0.20989704877138138\nBatch loss: 0.20732594653964043\nBatch loss: 0.13068961910903454\nBatch loss: 0.24122048169374466\nBatch loss: 0.3893321380019188\nBatch loss: 0.08728432469069958\nBatch loss: 0.06322979927062988\nBatch loss: 0.315336249768734\nBatch loss: 0.226547010242939\nBatch loss: 0.05060649011284113\nBatch loss: 0.05163431167602539\nBatch loss: 0.29336972162127495\nBatch loss: 0.1410121936351061\nBatch loss: 0.19820762798190117\nBatch loss: 0.08568418212234974\nBatch loss: 0.045285942032933235\nBatch loss: 0.07570818066596985\nBatch loss: 0.060270787216722965\nBatch loss: 0.29866551980376244\nBatch loss: 0.08601409383118153\nBatch loss: 0.22059131413698196\nBatch loss: 0.02845430513843894\nBatch loss: 0.7123231887817383\nBatch loss: 0.02361297607421875\nBatch loss: 0.17389727756381035\nBatch loss: 0.3299316391348839\nBatch loss: 0.2917938306927681\nBatch loss: 0.23489190265536308\nBatch loss: 0.23064613342285156\nBatch loss: 0.09978580288589001\nBatch loss: 0.09551334194839001\nBatch loss: 0.14260626398026943\nBatch loss: 0.35274337977170944\nBatch loss: 0.047554969787597656\nBatch loss: 0.05845403764396906\nBatch loss: 0.6436977535486221\nBatch loss: 0.3329644352197647\nBatch loss: 0.0287380232475698\nBatch loss: 0.07805276196449995\nBatch loss: 0.06119251251220703\nBatch loss: 0.3636360168457031\nBatch loss: 0.40060974657535553\nBatch loss: 0.22428488358855247\nBatch loss: 0.21097039803862572\nBatch loss: 0.020706176292151213\nBatch loss: 0.15789270401000977\nBatch loss: 0.5313238129019737\nBatch loss: 0.07880496792495251\nBatch loss: 0.2495136298239231\nBatch loss: 0.12807107530534267\nBatch loss: 0.21967673674225807\nBatch loss: 0.04643553402274847\nBatch loss: 0.21500993520021439\nBatch loss: 0.39118122309446335\nBatch loss: 0.43677162379026413\nBatch loss: 0.21031737327575684\nBatch loss: 0.13771414756774902\nBatch loss: 0.6368608772754669\nBatch loss: 0.26954127475619316\nBatch loss: 0.4342179372906685\nBatch loss: 0.31857240945100784\nBatch loss: 0.18899131566286087\nBatch loss: 0.0909271277487278\nBatch loss: 0.5217409133911133\nBatch loss: 0.1153421401977539\nBatch loss: 0.04168582148849964\nBatch loss: 0.04205894656479359\nBatch loss: 0.04266476724296808\nBatch loss: 0.16170455142855644\nBatch loss: 0.493009090423584\nBatch loss: 0.20509351044893265\nBatch loss: 0.24144530296325684\nBatch loss: 0.02965044928714633\nBatch loss: 0.4603055864572525\nBatch loss: 0.28623247519135475\nBatch loss: 0.16787052154541016\nBatch loss: 0.2995704486966133\nBatch loss: 0.6628646701574326\nBatch loss: 0.22995967417955399\nBatch loss: 0.2616503834724426\nBatch loss: 0.16891002655029297\nBatch loss: 0.36804892122745514\nBatch loss: 0.22399259731173515\nBatch loss: 0.43306972831487656\nBatch loss: 0.12847900390625\nBatch loss: 0.3401322290301323\nBatch loss: 0.170761588960886\nBatch loss: 0.4838987812399864\nBatch loss: 0.08467770181596279\nBatch loss: 0.5474987253546715\nBatch loss: 0.5583667755126953\nBatch loss: 0.5002651363611221\nBatch loss: 0.5214257165789604\nBatch loss: 0.34476280212402344\nBatch loss: 0.14770793728530407\nBatch loss: 0.6335391849279404\nBatch loss: 0.35351086407899857\nBatch loss: 0.4758225753903389\nBatch loss: 0.33176708966493607\nBatch loss: 0.3979816660284996\nBatch loss: 0.2565464936196804\nBatch loss: 0.21186042577028275\nBatch loss: 0.14468478970229626\nBatch loss: 0.12900590896606445\nBatch loss: 0.1609497144818306\nBatch loss: 0.10783255100250244\nBatch loss: 0.010439157485961914\nBatch loss: 0.12029039673507214\nBatch loss: 0.1038889866322279\nBatch loss: 0.5874128267168999\nBatch loss: 0.4522893950343132\nBatch loss: 0.0373420724645257\nBatch loss: 0.2096397988498211\nBatch loss: 0.04628634545952082\nBatch loss: 0.0979769229888916\nBatch loss: 0.1891757920384407\nBatch loss: 0.07338118739426136\nBatch loss: 0.050960066728293896\nBatch loss: 0.09269440546631813\nBatch loss: 0.5625572428107262\nBatch loss: 0.08556151762604713\nBatch loss: 0.03839164972305298\nBatch loss: 0.12982106767594814\nBatch loss: 0.0888983067125082\nBatch loss: 0.1683330535888672\nBatch loss: 0.11597729288041592\nBatch loss: 0.1559591293334961\nBatch loss: 0.13252235017716885\nBatch loss: 0.5082302168011665\nBatch loss: 0.046351910568773746\nBatch loss: 0.24371886625885963\nBatch loss: 0.23886924609541893\nBatch loss: 0.11680841445922852\nBatch loss: 0.1347286719828844\nBatch loss: 0.026111602783203125\nBatch loss: 0.18032336607575417\nBatch loss: 0.06838989444077015\nBatch loss: 0.26517821475863457\nBatch loss: 0.08078194223344326\nBatch loss: 0.5654535442590714\nBatch loss: 0.1377804297953844\nBatch loss: 0.04850625991821289\nBatch loss: 0.4014401510357857\nBatch loss: 0.05908966064453125\nBatch loss: 0.18833518028259277\nBatch loss: 0.22004390135407448\nBatch loss: 0.49771785736083984\nBatch loss: 0.1355192717164755\nBatch loss: 0.26489926502108574\nBatch loss: 0.028260708786547184\nBatch loss: 0.11843300424516201\nBatch loss: 0.3159584105014801\nBatch loss: 0.15585136599838734\nBatch loss: 0.42109157890081406\nBatch loss: 0.15233421698212624\nBatch loss: 0.049069407396018505\nBatch loss: 0.3280630335211754\nBatch loss: 0.3799062967300415\nBatch loss: 0.24483656510710716\nBatch loss: 0.4799354076385498\nBatch loss: 0.044759511947631836\nBatch loss: 0.06179237272590399\nBatch loss: 0.23669922724366188\nBatch loss: 0.09896433912217617\nBatch loss: 0.056778909638524055\nBatch loss: 0.3112649917602539\nBatch loss: 0.4286212846636772\nBatch loss: 0.2870704047381878\nBatch loss: 0.4050862789154053\nBatch loss: 0.04363811109215021\nBatch loss: 0.23192310705780983\nBatch loss: 0.327054038643837\nBatch loss: 0.18919086083769798\nBatch loss: 0.016913563013076782\nBatch loss: 1.0619087517261505\nBatch loss: 0.235536340624094\nBatch loss: 0.0700192479416728\nBatch loss: 0.19021987915039062\nBatch loss: 0.08192396722733974\nBatch loss: 0.17126893624663353\nBatch loss: 0.09217167273163795\nBatch loss: 0.1330957468599081\nBatch loss: 0.05451893899589777\nBatch loss: 0.5090193822979927\nBatch loss: 0.5934543535113335\nBatch loss: 0.2986343018710613\nBatch loss: 0.05227732937783003\nBatch loss: 0.22281790152192116\nBatch loss: 0.17715275287628174\nBatch loss: 1.1292610317468643\nBatch loss: 0.6837590038776398\nBatch loss: 0.11629295535385609\nBatch loss: 0.1662115566432476\nBatch loss: 0.33346083015203476\nBatch loss: 0.19290070980787277\nBatch loss: 0.33223628997802734\nBatch loss: 0.16141045838594437\nBatch loss: 0.021369934547692537\nBatch loss: 0.08534681983292103\nBatch loss: 0.1957988739013672\nBatch loss: 0.05983471870422363\nBatch loss: 0.2181565761566162\nBatch loss: 0.03931212704628706\nBatch loss: 0.14196300879120827\nBatch loss: 0.4796915128827095\nBatch loss: 0.25379812344908714\nBatch loss: 0.10383486747741699\nBatch loss: 0.5561981350183487\nBatch loss: 0.0750170974060893\nBatch loss: 0.0588037958368659\nBatch loss: 0.4299955442547798\nBatch loss: 0.07384419441223145\nBatch loss: 0.05807566922158003\nBatch loss: 0.06425503175705671\nBatch loss: 0.2852497063577175\nBatch loss: 0.19397974014282227\nBatch loss: 0.36577939987182617\nBatch loss: 0.08220131509006023\nBatch loss: 0.18589306622743607\nBatch loss: 0.23672455921769142\nBatch loss: 0.2389388158917427\nBatch loss: 0.219467431306839\nBatch loss: 0.09601175785064697\nBatch loss: 0.1548237819224596\nBatch loss: 0.16502881422638893\nBatch loss: 0.21135425195097923\nBatch loss: 0.5858705192804337\nBatch loss: 0.17180580645799637\nBatch loss: 0.27098942548036575\nBatch loss: 0.019831419922411442\nBatch loss: 0.20927263423800468\nBatch loss: 0.2307911030948162\nBatch loss: 0.3801131248474121\nBatch loss: 0.24149393662810326\nBatch loss: 0.5154571682214737\nBatch loss: 0.19357038661837578\nBatch loss: 0.25731801986694336\nBatch loss: 0.2971227280795574\nBatch loss: 0.35005975514650345\nBatch loss: 0.13940763659775257\nBatch loss: 0.18729722127318382\nBatch loss: 0.1304184179753065\nBatch loss: 0.023694992996752262\nBatch loss: 0.15135002322494984\nBatch loss: 0.6290192902088165\nBatch loss: 0.10014915838837624\nBatch loss: 0.148801626637578\nBatch loss: 0.11536932550370693\nBatch loss: 0.14178466983139515\nBatch loss: 0.4774193838238716\nBatch loss: 0.7682271301746368\nBatch loss: 0.20408917218446732\nBatch loss: 0.1547486800700426\nBatch loss: 0.37606295198202133\nBatch loss: 0.639745220541954\nBatch loss: 0.10442209430038929\nBatch loss: 0.04583215806633234\nBatch loss: 0.17458820715546608\nBatch loss: 0.09496820159256458\nBatch loss: 0.01967573305591941\nBatch loss: 0.30126428231596947\nBatch loss: 0.2885887585580349\nBatch loss: 0.2535622753202915\nBatch loss: 0.20969344303011894\nBatch loss: 0.22852255031466484\nBatch loss: 0.46713925898075104\nBatch loss: 0.0546335568651557\nBatch loss: 0.08888554759323597\nBatch loss: 0.09395170025527477\nBatch loss: 0.05327272694557905\nBatch loss: 0.01212436007335782\nBatch loss: 0.2939586713910103\nBatch loss: 0.7458501309156418\nBatch loss: 0.261866208165884\nBatch loss: 0.11323213577270508\nBatch loss: 0.07284927647560835\nBatch loss: 0.13528728857636452\nBatch loss: 0.06912946701049805\nBatch loss: 0.2981041744351387\nBatch loss: 0.3445262089371681\nBatch loss: 0.12925529852509499\nBatch loss: 0.13189143501222134\nBatch loss: 0.4869816452264786\nBatch loss: 0.17977464944124222\nBatch loss: 0.036614418495446444\nBatch loss: 0.20962953567504883\nBatch loss: 0.203904639929533\nBatch loss: 0.047375322319567204\nBatch loss: 0.11593246832489967\nBatch loss: 0.21440600976347923\nBatch loss: 0.07154548075050116\nBatch loss: 0.16437603160738945\nBatch loss: 0.20117128267884254\nBatch loss: 0.08084935136139393\nBatch loss: 0.489753894507885\nBatch loss: 0.10427760891616344\nBatch loss: 0.2013024128973484\nBatch loss: 0.30966823920607567\nBatch loss: 0.20625997334718704\nBatch loss: 0.05106210708618164\nBatch loss: 0.017583429580554366\nBatch loss: 0.14354228973388672\nBatch loss: 0.2704796753823757\nBatch loss: 0.10103094391524792\nBatch loss: 0.1319589652121067\nBatch loss: 0.05906605627387762\nBatch loss: 0.08936452679336071\nBatch loss: 0.5090687423944473\nBatch loss: 0.3894874081015587\nBatch loss: 0.033230900298804045\nBatch loss: 0.3025937080383301\nBatch loss: 0.1284637488424778\nBatch loss: 0.30669545754790306\nBatch loss: 0.29549671337008476\nBatch loss: 0.22919511422514915\nBatch loss: 0.08863866329193115\nBatch loss: 0.21614624187350273\nBatch loss: 0.08381510153412819\nBatch loss: 0.28733421117067337\nBatch loss: 0.0407564640045166\nBatch loss: 0.9999918937683105\nBatch loss: 0.2765798568725586\nBatch loss: 0.12311697006225586\nBatch loss: 0.29558027163147926\nBatch loss: 0.027844251599162817\nBatch loss: 0.1014325674623251\nBatch loss: 0.13145113363862038\nBatch loss: 0.12354659847915173\nBatch loss: 0.036770820152014494\nBatch loss: 0.1513531245291233\nBatch loss: 0.009151220438070595\nBatch loss: 0.13927883468568325\nBatch loss: 0.03240385791286826\nBatch loss: 0.055576800368726254\nBatch loss: 0.0446132430806756\nBatch loss: 0.014307498931884766\nBatch loss: 0.1012046355754137\nBatch loss: 0.3296709805727005\nEpoch 6/7\nBatch loss: 0.6637879461050034\nBatch loss: 0.06456518080085516\nBatch loss: 0.04595214035362005\nBatch loss: 0.29133081436157227\nBatch loss: 0.07039410062134266\nBatch loss: 0.024819374084472656\nBatch loss: 0.008779526106081903\nBatch loss: 0.08186054416000843\nBatch loss: 0.008177399868145585\nBatch loss: 0.15697836875915527\nBatch loss: 0.6555182486772537\nBatch loss: 0.039080861024558544\nBatch loss: 0.3018077649176121\nBatch loss: 0.013247013557702303\nBatch loss: 0.09067905135452747\nBatch loss: 0.17211783677339554\nBatch loss: 0.2431086264550686\nBatch loss: 0.03166961716488004\nBatch loss: 0.5291693285107613\nBatch loss: 0.09314454160630703\nBatch loss: 0.02904111286625266\nBatch loss: 0.6967677921056747\nBatch loss: 0.10303163900971413\nBatch loss: 0.020602585282176733\nBatch loss: 0.027462958823889494\nBatch loss: 1.0597798973321915\nBatch loss: 0.1553888339549303\nBatch loss: 0.029041767120361328\nBatch loss: 0.6567610800266266\nBatch loss: 0.09094524197280407\nBatch loss: 0.14974474906921387\nBatch loss: 0.1006317138671875\nBatch loss: 0.22789467126131058\nBatch loss: 0.9012496471405029\nBatch loss: 0.7303748279809952\nBatch loss: 0.09322143159806728\nBatch loss: 0.33144641667604446\nBatch loss: 0.41287362575531006\nBatch loss: 0.4027951508760452\nBatch loss: 0.6048481538891792\nBatch loss: 0.38787364959716797\nBatch loss: 0.04722118377685547\nBatch loss: 0.0062295200768858194\nBatch loss: 0.013186216820031404\nBatch loss: 0.14870858751237392\nBatch loss: 0.32037485390901566\nBatch loss: 0.02205920172855258\nBatch loss: 0.6146049499511719\nBatch loss: 0.15609062276780605\nBatch loss: 0.08233738131821156\nBatch loss: 0.25052690878510475\nBatch loss: 0.5898637697100639\nBatch loss: 0.09911191649734974\nBatch loss: 0.1763988845050335\nBatch loss: 0.5371463298797607\nBatch loss: 0.03372275969013572\nBatch loss: 0.23611759766936302\nBatch loss: 0.13614296913146973\nBatch loss: 0.14019275084137917\nBatch loss: 0.3187788650393486\nBatch loss: 0.5858228355646133\nBatch loss: 0.047554285265505314\nBatch loss: 0.10001110844314098\nBatch loss: 0.19420433789491653\nBatch loss: 0.11782306246459484\nBatch loss: 0.13457584194839\nBatch loss: 0.13546275906264782\nBatch loss: 0.016224145656451583\nBatch loss: 0.5575945600867271\nBatch loss: 0.45535992830991745\nBatch loss: 0.8135300129652023\nBatch loss: 0.11147218756377697\nBatch loss: 0.4475729539990425\nBatch loss: 0.0425145635381341\nBatch loss: 0.10693800635635853\nBatch loss: 0.053659859113395214\nBatch loss: 0.1915949583053589\nBatch loss: 0.015532731777057052\nBatch loss: 0.05493489094078541\nBatch loss: 0.7927031815052032\nBatch loss: 0.31591977924108505\nBatch loss: 0.12939262203872204\nBatch loss: 0.24796409532427788\nBatch loss: 0.04414164926856756\nBatch loss: 0.5103693157434464\nBatch loss: 0.14137998223304749\nBatch loss: 0.17628991976380348\nBatch loss: 0.5771822854876518\nBatch loss: 0.47205962240695953\nBatch loss: 0.22919489070773125\nBatch loss: 0.042731077410280704\nBatch loss: 0.26201820001006126\nBatch loss: 0.3414876386523247\nBatch loss: 0.42436789721250534\nBatch loss: 0.15679145231842995\nBatch loss: 0.34949880093336105\nBatch loss: 0.021081448066979647\nBatch loss: 0.9595402330160141\nBatch loss: 0.018736958736553788\nBatch loss: 0.07779273670166731\nBatch loss: 0.015465737087652087\nBatch loss: 0.3184821829199791\nBatch loss: 0.2909970283508301\nBatch loss: 0.19744277000427246\nBatch loss: 0.30566930770874023\nBatch loss: 0.4486861452460289\nBatch loss: 0.25066256523132324\nBatch loss: 0.3403956815600395\nBatch loss: 0.559932254254818\nBatch loss: 0.03415226936340332\nBatch loss: 0.45539427548646927\nBatch loss: 0.09953928180038929\nBatch loss: 1.290510892868042\nBatch loss: 0.37088360637426376\nBatch loss: 0.11942666955292225\nBatch loss: 0.013422012561932206\nBatch loss: 0.03568029496818781\nBatch loss: 0.12192285619676113\nBatch loss: 0.08971357718110085\nBatch loss: 0.39005257189273834\nBatch loss: 0.045440676622092724\nBatch loss: 0.6062337383627892\nBatch loss: 0.07241666316986084\nBatch loss: 0.0697206286713481\nBatch loss: 0.38495272397994995\nBatch loss: 0.6136214733123779\nBatch loss: 0.09282874874770641\nBatch loss: 0.6427422910928726\nBatch loss: 0.3889913484454155\nBatch loss: 0.26291394606232643\nBatch loss: 0.07720899768173695\nBatch loss: 0.0854390300810337\nBatch loss: 0.11227014474570751\nBatch loss: 0.5161097273230553\nBatch loss: 0.1160728931427002\nBatch loss: 0.009454727405682206\nBatch loss: 0.020084024872630835\nBatch loss: 0.15322578139603138\nBatch loss: 0.09944135323166847\nBatch loss: 0.0509798526763916\nBatch loss: 0.041000694036483765\nBatch loss: 0.4986138269305229\nBatch loss: 0.26930857449769974\nBatch loss: 0.06707740016281605\nBatch loss: 0.3399703651666641\nBatch loss: 0.4292795807123184\nBatch loss: 0.4871523380279541\nBatch loss: 0.4741332679986954\nBatch loss: 0.0954294204711914\nBatch loss: 0.31619250774383545\nBatch loss: 0.14146852307021618\nBatch loss: 0.01745200133882463\nBatch loss: 0.4212620481848717\nBatch loss: 0.32106876373291016\nBatch loss: 0.6041665002703667\nBatch loss: 0.1137309055775404\nBatch loss: 0.3244302421808243\nBatch loss: 0.11058426462113857\nBatch loss: 0.5195365101099014\nBatch loss: 0.17696959897875786\nBatch loss: 0.3222832828760147\nBatch loss: 0.04343664739280939\nBatch loss: 0.1160894613713026\nBatch loss: 0.3201712295413017\nBatch loss: 0.08324933238327503\nBatch loss: 0.01150798867456615\nBatch loss: 0.39456315338611603\nBatch loss: 0.5850018188357353\nBatch loss: 0.17067039385437965\nBatch loss: 0.49307476729154587\nBatch loss: 0.2605853043496609\nBatch loss: 0.034078359603881836\nBatch loss: 0.12318039312958717\nBatch loss: 0.025619089137762785\nBatch loss: 0.46107251197099686\nBatch loss: 0.3771383687853813\nBatch loss: 0.1578540913760662\nBatch loss: 0.210600383579731\nBatch loss: 0.20714664831757545\nBatch loss: 0.6597087532281876\nBatch loss: 0.11960173025727272\nBatch loss: 0.12889432720839977\nBatch loss: 0.11178731918334961\nBatch loss: 0.014191180234774947\nBatch loss: 0.01423290348611772\nBatch loss: 0.13391727581620216\nBatch loss: 0.3867912292480469\nBatch loss: 0.2433236874639988\nBatch loss: 0.10242319665849209\nBatch loss: 0.11990166269242764\nBatch loss: 0.21996354684233665\nBatch loss: 0.4478902742266655\nBatch loss: 0.09294790215790272\nBatch loss: 0.10857383720576763\nBatch loss: 0.21038485690951347\nBatch loss: 0.14605212025344372\nBatch loss: 0.005271434783935547\nBatch loss: 0.6660512834787369\nBatch loss: 0.5153267458081245\nBatch loss: 0.02535152481868863\nBatch loss: 0.3543062135577202\nBatch loss: 0.18065381795167923\nBatch loss: 0.37116318941116333\nBatch loss: 0.08575797080993652\nBatch loss: 0.10354191064834595\nBatch loss: 0.49903012812137604\nBatch loss: 0.15665387734770775\nBatch loss: 0.17471600323915482\nBatch loss: 0.8409871906042099\nBatch loss: 0.2355273999273777\nBatch loss: 0.3118220530450344\nBatch loss: 0.16181934624910355\nBatch loss: 0.084465267136693\nBatch loss: 0.07927894592285156\nBatch loss: 0.0100108387414366\nBatch loss: 0.577068105340004\nBatch loss: 0.27623463422060013\nBatch loss: 0.6582927703857422\nBatch loss: 0.20602917298674583\nBatch loss: 0.017120838165283203\nBatch loss: 0.5789460614323616\nBatch loss: 0.506933219730854\nBatch loss: 0.7340202480554581\nBatch loss: 0.010279298294335604\nBatch loss: 0.39611995220184326\nBatch loss: 0.6824952363967896\nBatch loss: 0.24669576436281204\nBatch loss: 0.18474949523806572\nBatch loss: 0.10542082600295544\nBatch loss: 0.24185944348573685\nBatch loss: 0.24178504943847656\nBatch loss: 0.02249647630378604\nBatch loss: 0.07410538382828236\nBatch loss: 0.24679755792021751\nBatch loss: 0.061558159068226814\nBatch loss: 0.7312619686126709\nBatch loss: 0.3285966068506241\nBatch loss: 0.10612201876938343\nBatch loss: 0.13178992085158825\nBatch loss: 0.4964010789990425\nBatch loss: 0.24327224120497704\nBatch loss: 0.3936590626835823\nBatch loss: 0.03566521452739835\nBatch loss: 0.16014397144317627\nBatch loss: 0.25719309225678444\nBatch loss: 0.2929574251174927\nBatch loss: 0.24616455659270287\nBatch loss: 0.18541742116212845\nBatch loss: 0.13176441192626953\nBatch loss: 0.18394052982330322\nBatch loss: 0.1909484900534153\nBatch loss: 0.3899693489074707\nBatch loss: 0.1803579367697239\nBatch loss: 0.10348009876906872\nBatch loss: 0.29056524857878685\nBatch loss: 0.22791290655732155\nBatch loss: 0.09002894163131714\nBatch loss: 0.05255860276520252\nBatch loss: 0.0758473901078105\nBatch loss: 0.6385445594787598\nBatch loss: 0.12610054574906826\nBatch loss: 0.4629622772336006\nBatch loss: 0.2028369903564453\nBatch loss: 0.16804886981844902\nBatch loss: 0.06640625186264515\nBatch loss: 0.3706458956003189\nBatch loss: 0.007724106544628739\nBatch loss: 0.10426527820527554\nBatch loss: 0.03027850529178977\nBatch loss: 0.036680460907518864\nBatch loss: 0.2954239957034588\nBatch loss: 0.14877855777740479\nBatch loss: 0.019212246406823397\nBatch loss: 0.09243965148925781\nBatch loss: 0.01298284507356584\nBatch loss: 0.06972140166908503\nBatch loss: 0.12960719875991344\nBatch loss: 0.1386247854679823\nBatch loss: 0.12141287326812744\nBatch loss: 0.1335753221064806\nBatch loss: 0.04629194736480713\nBatch loss: 0.4329831525683403\nBatch loss: 0.45640017837285995\nBatch loss: 0.10485219769179821\nBatch loss: 0.13704180717468262\nBatch loss: 0.022069632541388273\nBatch loss: 0.008050561300478876\nBatch loss: 0.08829200640320778\nBatch loss: 0.05493068601936102\nBatch loss: 0.3623546287417412\nBatch loss: 0.27393246069550514\nBatch loss: 0.021954774856567383\nBatch loss: 0.4816165193915367\nBatch loss: 0.27010058984160423\nBatch loss: 0.7436122745275497\nBatch loss: 0.27384812012314796\nBatch loss: 0.06419706624001265\nBatch loss: 0.221694465726614\nBatch loss: 0.16129208728671074\nBatch loss: 0.2561154402792454\nBatch loss: 0.02068090485408902\nBatch loss: 0.5831803008913994\nBatch loss: 0.8616512268781662\nBatch loss: 0.05007732193917036\nBatch loss: 0.13830924406647682\nBatch loss: 0.1605607382953167\nBatch loss: 0.08594012819230556\nBatch loss: 0.057637691497802734\nBatch loss: 0.17838818952441216\nBatch loss: 0.22964192554354668\nBatch loss: 0.06605720613151789\nBatch loss: 0.3600405529141426\nBatch loss: 0.16097784042358398\nBatch loss: 0.056801559403538704\nBatch loss: 0.2959033288061619\nBatch loss: 0.1265127118676901\nBatch loss: 0.07892164401710033\nBatch loss: 0.13946187682449818\nBatch loss: 0.05602115299552679\nBatch loss: 0.17778445035219193\nBatch loss: 0.1595095358788967\nBatch loss: 0.11648070998489857\nBatch loss: 0.32280873507261276\nBatch loss: 0.06334805395454168\nBatch loss: 0.1483453530818224\nBatch loss: 0.39289772510528564\nBatch loss: 0.5794356018304825\nBatch loss: 0.0422061700373888\nBatch loss: 0.05555951502174139\nBatch loss: 0.3288372978568077\nBatch loss: 0.08592188358306885\nBatch loss: 0.24432701990008354\nBatch loss: 0.04533314611762762\nBatch loss: 0.01750731491483748\nBatch loss: 0.03129196120426059\nBatch loss: 0.040872576646506786\nBatch loss: 0.372224822640419\nBatch loss: 0.15064594335854053\nBatch loss: 0.04013919737190008\nBatch loss: 0.03872275352478027\nBatch loss: 0.017685070633888245\nBatch loss: 0.13717628084123135\nBatch loss: 0.12944579124450684\nBatch loss: 0.42157329618930817\nBatch loss: 0.017842650413513184\nBatch loss: 0.18506282940506935\nBatch loss: 0.08404052816331387\nBatch loss: 0.09870469570159912\nBatch loss: 0.06845152471214533\nBatch loss: 0.008585751056671143\nBatch loss: 0.023212910164147615\nBatch loss: 0.4760916158556938\nBatch loss: 0.20143430680036545\nBatch loss: 0.46364378184080124\nBatch loss: 0.016484558582305908\nBatch loss: 0.1690562441945076\nBatch loss: 0.04066702909767628\nBatch loss: 0.33206652849912643\nBatch loss: 0.12551533989608288\nBatch loss: 0.07843446917831898\nBatch loss: 0.02078670309856534\nBatch loss: 0.1470144372433424\nBatch loss: 0.09748793207108974\nBatch loss: 0.06832313723862171\nBatch loss: 0.041787861846387386\nBatch loss: 0.18999112769961357\nBatch loss: 0.1955178938806057\nBatch loss: 0.1837308518588543\nBatch loss: 0.20949536934494972\nBatch loss: 0.26076650246977806\nBatch loss: 0.0978629570454359\nBatch loss: 0.3134572505950928\nBatch loss: 0.5070192739367485\nBatch loss: 0.061510950326919556\nBatch loss: 0.03485107561573386\nBatch loss: 0.03881967160850763\nBatch loss: 0.009688377613201737\nBatch loss: 0.02166962716728449\nBatch loss: 0.3199316933751106\nBatch loss: 0.06083720829337835\nBatch loss: 0.06169963162392378\nBatch loss: 0.3419463708996773\nBatch loss: 0.13992547988891602\nBatch loss: 0.29254866763949394\nBatch loss: 0.006612419965676963\nBatch loss: 0.2974292077124119\nBatch loss: 0.2868991531431675\nBatch loss: 0.5835726484656334\nBatch loss: 0.14106154441833496\nBatch loss: 0.4704973101615906\nBatch loss: 0.050093173049390316\nBatch loss: 0.20195186138153076\nBatch loss: 0.13856482692062855\nBatch loss: 0.23248886689543724\nBatch loss: 0.261460542678833\nBatch loss: 0.5751751735806465\nBatch loss: 0.14462209306657314\nBatch loss: 0.18201900646090508\nBatch loss: 0.33602070063352585\nBatch loss: 0.11706501245498657\nBatch loss: 0.01554513000883162\nBatch loss: 0.5542870983481407\nBatch loss: 0.5075099691748619\nBatch loss: 0.020737589802592993\nBatch loss: 0.0031452180701307952\nBatch loss: 0.1563839428126812\nBatch loss: 0.19682049751281738\nBatch loss: 0.022687434684485197\nBatch loss: 0.021753788460046053\nBatch loss: 0.11144131422042847\nBatch loss: 0.11732143349945545\nBatch loss: 0.24963974952697754\nBatch loss: 0.18414905294775963\nBatch loss: 0.04028296563774347\nBatch loss: 0.31476516276597977\nBatch loss: 0.030647634994238615\nBatch loss: 0.12338399887084961\nBatch loss: 0.020609856583178043\nBatch loss: 0.11368483304977417\nBatch loss: 0.06794835906475782\nBatch loss: 0.7422938942909241\nBatch loss: 0.130904084071517\nBatch loss: 0.19151270389556885\nBatch loss: 0.700671449303627\nBatch loss: 0.18977178260684013\nBatch loss: 0.08711934089660645\nBatch loss: 0.041574956849217415\nBatch loss: 0.39875220507383347\nBatch loss: 0.039042949210852385\nBatch loss: 0.0636867294088006\nBatch loss: 0.2552192285656929\nBatch loss: 0.1283044833689928\nBatch loss: 0.18935823813080788\nBatch loss: 0.05299496930092573\nBatch loss: 0.22880816832184792\nBatch loss: 0.06814441177994013\nBatch loss: 0.47852229326963425\nBatch loss: 0.22443044930696487\nBatch loss: 0.06913113873451948\nBatch loss: 0.18625056371092796\nBatch loss: 0.7791180908679962\nBatch loss: 0.20731234923005104\nBatch loss: 0.014675379497930408\nBatch loss: 0.06676340010017157\nBatch loss: 0.018069864017888904\nBatch loss: 0.3489666059613228\nBatch loss: 0.12138045392930508\nBatch loss: 0.54828692227602\nBatch loss: 0.1580815389752388\nBatch loss: 0.27750611305236816\nBatch loss: 0.024636387825012207\nBatch loss: 0.12195062823593616\nBatch loss: 0.3982473537325859\nBatch loss: 0.12310600839555264\nBatch loss: 0.19288331270217896\nBatch loss: 0.16643930226564407\nBatch loss: 0.10680270381271839\nBatch loss: 0.21062446758151054\nBatch loss: 0.3818204998970032\nBatch loss: 0.4029477760195732\nBatch loss: 0.09137475863099098\nBatch loss: 0.07839846424758434\nBatch loss: 0.03072452498599887\nBatch loss: 0.23680388927459717\nBatch loss: 0.13280210085213184\nBatch loss: 0.02556747291237116\nBatch loss: 0.12060344219207764\nBatch loss: 0.014858723152428865\nBatch loss: 0.24736914783716202\nBatch loss: 0.46676643192768097\nBatch loss: 0.18965436145663261\nBatch loss: 0.28727317228913307\nBatch loss: 0.06551957223564386\nBatch loss: 0.10147309862077236\nBatch loss: 0.08478320203721523\nBatch loss: 0.1368570327758789\nBatch loss: 0.09752201847732067\nBatch loss: 0.36671675741672516\nBatch loss: 0.43343938887119293\nBatch loss: 0.3178269788622856\nBatch loss: 0.006950974348001182\nBatch loss: 0.005363941309042275\nBatch loss: 0.027733088936656713\nBatch loss: 0.09519219398498535\nBatch loss: 0.02452802611514926\nBatch loss: 0.08296799845993519\nBatch loss: 0.38595475256443024\nBatch loss: 0.5968239530920982\nBatch loss: 0.12352597899734974\nBatch loss: 0.5154361948370934\nBatch loss: 0.06713247392326593\nBatch loss: 0.07789743132889271\nBatch loss: 0.8881836384534836\nBatch loss: 0.2399010770022869\nBatch loss: 0.17036175355315208\nBatch loss: 0.46788908541202545\nBatch loss: 0.5077611282467842\nBatch loss: 0.4402592405676842\nBatch loss: 0.27022218331694603\nBatch loss: 0.2837972715497017\nBatch loss: 0.07246613502502441\nBatch loss: 0.3833630308508873\nBatch loss: 0.14757633209228516\nBatch loss: 0.3179113194346428\nBatch loss: 0.1148843765258789\nBatch loss: 0.3250124678015709\nBatch loss: 0.03690445562824607\nBatch loss: 0.17264986410737038\nBatch loss: 0.23768896237015724\nBatch loss: 0.3572862222790718\nBatch loss: 0.42483117431402206\nBatch loss: 0.09721874259412289\nBatch loss: 0.24903154000639915\nBatch loss: 0.07599580567330122\nBatch loss: 0.11968756094574928\nBatch loss: 0.11818850412964821\nBatch loss: 0.2029665745794773\nBatch loss: 0.08604335598647594\nBatch loss: 0.01813089824281633\nBatch loss: 0.14461109414696693\nBatch loss: 0.44224441051483154\nBatch loss: 0.17366576939821243\nBatch loss: 0.03442180110141635\nBatch loss: 0.14272570610046387\nBatch loss: 0.2144831418991089\nBatch loss: 0.19729584455490112\nBatch loss: 0.594455860555172\nBatch loss: 0.21522259339690208\nBatch loss: 0.5401012301445007\nBatch loss: 0.0054491759510710835\nBatch loss: 0.1971919648349285\nBatch loss: 0.04696703050285578\nBatch loss: 0.08795976638793945\nBatch loss: 0.28525209054350853\nBatch loss: 0.3390166908502579\nBatch loss: 0.040886285714805126\nBatch loss: 0.011626363266259432\nBatch loss: 0.13370776548981667\nBatch loss: 0.10759211145341396\nBatch loss: 0.4420948028564453\nBatch loss: 0.10940933600068092\nBatch loss: 0.08210492320358753\nBatch loss: 0.019840002059936523\nBatch loss: 0.11201691813766956\nBatch loss: 0.06972003262490034\nBatch loss: 0.22141087800264359\nBatch loss: 0.4991917684674263\nBatch loss: 0.40420081466436386\nBatch loss: 0.03957116510719061\nBatch loss: 0.015059352153912187\nBatch loss: 0.11840963736176491\nBatch loss: 0.3571615368127823\nBatch loss: 0.20373869687318802\nBatch loss: 0.31341221183538437\nBatch loss: 0.18412018194794655\nBatch loss: 0.20078230649232864\nBatch loss: 0.1818537712097168\nBatch loss: 0.06248089950531721\nBatch loss: 0.0396116403862834\nBatch loss: 0.04124057479202747\nBatch loss: 0.17951106652617455\nBatch loss: 0.49059975892305374\nBatch loss: 0.47223281115293503\nBatch loss: 0.30380619689822197\nBatch loss: 0.3562638908624649\nBatch loss: 0.028613419272005558\nBatch loss: 0.41494037955999374\nBatch loss: 0.3077383153140545\nBatch loss: 0.31045055016875267\nBatch loss: 0.16554046422243118\nBatch loss: 0.4175354167819023\nBatch loss: 0.13166618533432484\nBatch loss: 0.5629408359527588\nBatch loss: 0.4547467455267906\nBatch loss: 0.12807250022888184\nBatch loss: 0.15086865983903408\nBatch loss: 0.013026654487475753\nBatch loss: 0.0866012554615736\nBatch loss: 0.05569434259086847\nBatch loss: 0.21883010864257812\nBatch loss: 0.01791966031305492\nBatch loss: 0.13621318154037\nBatch loss: 0.07301759906113148\nBatch loss: 0.29189109802246094\nBatch loss: 0.20917141810059547\nBatch loss: 0.221771951764822\nBatch loss: 0.2431194856762886\nBatch loss: 0.148743512108922\nBatch loss: 0.09393728338181973\nBatch loss: 0.1032490748912096\nBatch loss: 0.3705120086669922\nBatch loss: 0.5381506681442261\nBatch loss: 0.025545598473399878\nBatch loss: 0.06696391385048628\nBatch loss: 0.4826030880212784\nBatch loss: 0.37790562957525253\nBatch loss: 0.2019619382917881\nBatch loss: 0.020716190338134766\nBatch loss: 0.23742986842989922\nBatch loss: 0.31507372856140137\nBatch loss: 0.5171081051230431\nBatch loss: 0.35762619227170944\nBatch loss: 0.13980555348098278\nBatch loss: 0.24246275424957275\nBatch loss: 0.22698163986206055\nBatch loss: 0.10296988300979137\nBatch loss: 0.11238194070756435\nBatch loss: 0.5287010595202446\nBatch loss: 0.194710623472929\nBatch loss: 0.23267829790711403\nBatch loss: 0.4428282007575035\nBatch loss: 0.09285521693527699\nBatch loss: 0.5392603948712349\nBatch loss: 0.3736419603228569\nBatch loss: 0.2756332792341709\nBatch loss: 0.1507433969527483\nBatch loss: 0.009068548679351807\nBatch loss: 0.06340000312775373\nBatch loss: 0.03254234790802002\nBatch loss: 0.14000368304550648\nBatch loss: 0.1448061503469944\nBatch loss: 0.047616721130907536\nBatch loss: 0.08095532655715942\nBatch loss: 0.08420849218964577\nBatch loss: 0.04977071192115545\nBatch loss: 0.1886594295501709\nBatch loss: 0.17303777858614922\nBatch loss: 0.046072485856711864\nBatch loss: 0.1169662456959486\nBatch loss: 0.37868719547986984\nBatch loss: 0.036732316948473454\nBatch loss: 0.15870308503508568\nBatch loss: 0.11015760712325573\nBatch loss: 0.28737807646393776\nBatch loss: 0.2631394937634468\nBatch loss: 0.15753090381622314\nBatch loss: 0.09619474411010742\nBatch loss: 0.2800685167312622\nBatch loss: 0.32138295471668243\nBatch loss: 0.1419182401150465\nBatch loss: 0.261195357888937\nBatch loss: 0.6766178458929062\nBatch loss: 0.028728009201586246\nBatch loss: 0.32805632799863815\nBatch loss: 0.2786356210708618\nBatch loss: 0.14604926109313965\nBatch loss: 0.4914703592658043\nBatch loss: 0.27043914422392845\nBatch loss: 0.13109922409057617\nBatch loss: 0.2854938618838787\nBatch loss: 0.03971791360527277\nBatch loss: 0.11734104715287685\nBatch loss: 0.2846590243279934\nBatch loss: 0.022675395011901855\nBatch loss: 0.6703567504882812\nBatch loss: 0.13696334324777126\nBatch loss: 0.28754962608218193\nBatch loss: 0.40441930294036865\nBatch loss: 0.24278093129396439\nBatch loss: 0.10197735391557217\nBatch loss: 0.14224099926650524\nBatch loss: 0.4522993788123131\nBatch loss: 0.2071293629705906\nBatch loss: 0.49981068819761276\nBatch loss: 0.03458952996879816\nBatch loss: 0.33811837434768677\nBatch loss: 0.17634475603699684\nBatch loss: 0.2195444144308567\nBatch loss: 0.23042172193527222\nBatch loss: 0.09391880594193935\nBatch loss: 0.2317209355533123\nBatch loss: 0.19491052255034447\nBatch loss: 0.14934889040887356\nBatch loss: 0.20711874589323997\nBatch loss: 0.0608484772965312\nBatch loss: 0.18497968092560768\nBatch loss: 0.13269699178636074\nBatch loss: 0.4093772917985916\nBatch loss: 0.1749420166015625\nBatch loss: 0.38616131991147995\nBatch loss: 0.18410814926028252\nBatch loss: 0.2059778943657875\nBatch loss: 0.12900722213089466\nBatch loss: 0.04773855209350586\nBatch loss: 0.15167051926255226\nBatch loss: 0.15911294147372246\nBatch loss: 0.008157253614626825\nBatch loss: 0.2357618324458599\nBatch loss: 0.5706286430358887\nBatch loss: 0.1838698424398899\nBatch loss: 0.10727047920227051\nBatch loss: 0.19744349643588066\nBatch loss: 0.11115479283034801\nBatch loss: 0.11532223783433437\nBatch loss: 0.023192407097667456\nBatch loss: 0.09630441665649414\nBatch loss: 0.026511014439165592\nBatch loss: 0.16245508566498756\nBatch loss: 0.5183152109384537\nBatch loss: 0.20931316539645195\nBatch loss: 0.04360771272331476\nBatch loss: 0.37238407880067825\nBatch loss: 0.1589806191623211\nBatch loss: 0.059938966296613216\nBatch loss: 0.1682150922715664\nBatch loss: 0.11882257647812366\nBatch loss: 0.18394995480775833\nBatch loss: 0.2584247663617134\nBatch loss: 0.5118824169039726\nBatch loss: 0.04263439681380987\nBatch loss: 0.13217992149293423\nBatch loss: 0.04446470644325018\nBatch loss: 0.17285514622926712\nBatch loss: 0.15699196606874466\nBatch loss: 0.09986475110054016\nBatch loss: 0.06604909896850586\nBatch loss: 0.22131586447358131\nBatch loss: 0.13356494717299938\nBatch loss: 0.030419589020311832\nBatch loss: 0.02894330071285367\nBatch loss: 0.2915935590863228\nBatch loss: 0.20003868266940117\nBatch loss: 0.20535826683044434\nBatch loss: 0.30721116811037064\nBatch loss: 0.42700059711933136\nBatch loss: 0.12154988013207912\nBatch loss: 0.14009934850037098\nBatch loss: 0.018088341457769275\nBatch loss: 0.1603982411324978\nBatch loss: 0.40397703647613525\nBatch loss: 0.18106698989868164\nBatch loss: 0.19295692443847656\nBatch loss: 0.19861210137605667\nBatch loss: 0.36414124071598053\nBatch loss: 0.07608962245285511\nBatch loss: 0.0818338431417942\nBatch loss: 0.1428017672151327\nBatch loss: 0.029896260239183903\nBatch loss: 0.10496663860976696\nBatch loss: 0.03502035280689597\nBatch loss: 0.5710981041193008\nBatch loss: 0.09553289972245693\nBatch loss: 0.14879298396408558\nBatch loss: 0.256655216217041\nBatch loss: 0.2511425130069256\nBatch loss: 0.23490255698561668\nBatch loss: 0.02457308815792203\nBatch loss: 0.3876956179738045\nBatch loss: 0.3567395359277725\nBatch loss: 0.37667490541934967\nBatch loss: 0.0401725759729743\nBatch loss: 0.11797499842941761\nBatch loss: 0.1646963320672512\nBatch loss: 0.24126455187797546\nBatch loss: 0.25922972708940506\nBatch loss: 0.11067605577409267\nBatch loss: 0.0749251851812005\nBatch loss: 0.11407464742660522\nBatch loss: 0.24176979437470436\nBatch loss: 0.064709666185081\nBatch loss: 0.0914144515991211\nBatch loss: 0.54412592202425\nBatch loss: 0.0996100902557373\nBatch loss: 0.07611393928527832\nBatch loss: 0.10156488977372646\nBatch loss: 0.1430146675556898\nBatch loss: 0.17041444778442383\nBatch loss: 0.3481297567486763\nBatch loss: 0.12084639631211758\nBatch loss: 0.3052718751132488\nBatch loss: 0.09119010530412197\nBatch loss: 0.4140157625079155\nBatch loss: 0.08505952544510365\nBatch loss: 0.27686769142746925\nBatch loss: 0.0076923222513869405\nBatch loss: 0.0669474620372057\nBatch loss: 1.134946122765541\nBatch loss: 0.3391404077410698\nBatch loss: 0.18317580223083496\nBatch loss: 0.09153342805802822\nBatch loss: 0.06635403726249933\nBatch loss: 0.28037548065185547\nBatch loss: 0.33268552273511887\nBatch loss: 0.06052887532860041\nBatch loss: 0.20283937454223633\nBatch loss: 0.09524441324174404\nBatch loss: 0.12443721294403076\nBatch loss: 0.7017675787210464\nBatch loss: 0.17360830679535866\nBatch loss: 0.1458199042826891\nBatch loss: 0.3483785316348076\nBatch loss: 0.30794907361268997\nBatch loss: 0.09577560238540173\nBatch loss: 0.3659301996231079\nBatch loss: 0.39348792284727097\nBatch loss: 0.184000376611948\nBatch loss: 0.025928616523742676\nBatch loss: 0.20409410819411278\nBatch loss: 0.2370607852935791\nBatch loss: 0.09368026629090309\nBatch loss: 0.11819208040833473\nBatch loss: 0.3385517746210098\nBatch loss: 0.0651828059926629\nBatch loss: 0.027525187470018864\nBatch loss: 0.3662329167127609\nBatch loss: 0.2649000659584999\nBatch loss: 0.18462372943758965\nBatch loss: 0.09654849767684937\nBatch loss: 0.10636645369231701\nBatch loss: 0.25630122050642967\nBatch loss: 0.07697212975472212\nBatch loss: 0.12768435291945934\nBatch loss: 0.01933136605657637\nBatch loss: 0.0485016405582428\nBatch loss: 0.06946963258087635\nBatch loss: 0.07029962725937366\nBatch loss: 0.0819475669413805\nBatch loss: 0.19981343299150467\nBatch loss: 0.06731991656124592\nBatch loss: 0.43110109865665436\nBatch loss: 0.24703716859221458\nBatch loss: 0.12480097822844982\nBatch loss: 0.1938195899128914\nBatch loss: 0.052728294394910336\nBatch loss: 0.04012275021523237\nBatch loss: 0.04026174545288086\nBatch loss: 0.4773084446787834\nBatch loss: 0.48084642738103867\nBatch loss: 0.1393115520477295\nBatch loss: 0.5761580541729927\nBatch loss: 0.0726352958008647\nBatch loss: 0.7241642475128174\nBatch loss: 0.005644261837005615\nBatch loss: 0.04620975349098444\nBatch loss: 0.40909506380558014\nBatch loss: 0.5242934450507164\nBatch loss: 0.01225328422151506\nBatch loss: 0.5269203335046768\nBatch loss: 0.07679206319153309\nBatch loss: 0.25213489308953285\nBatch loss: 0.1566782034933567\nBatch loss: 0.1806400530040264\nBatch loss: 0.33133793622255325\nBatch loss: 0.9034127742052078\nBatch loss: 0.10599899105727673\nBatch loss: 0.1661778800189495\nBatch loss: 0.0965416431427002\nBatch loss: 0.4651547595858574\nBatch loss: 0.050573828630149364\nBatch loss: 0.19964218139648438\nBatch loss: 0.10781980119645596\nBatch loss: 0.10806846432387829\nBatch loss: 0.4100913554430008\nBatch loss: 0.28802836313843727\nBatch loss: 0.017832756275311112\nBatch loss: 0.5964317545294762\nBatch loss: 0.2402663230895996\nBatch loss: 0.08340263739228249\nBatch loss: 0.41326045989990234\nBatch loss: 0.05264196079224348\nBatch loss: 0.20247099921107292\nBatch loss: 0.019546032417565584\nBatch loss: 0.10785532183945179\nBatch loss: 0.30133021995425224\nBatch loss: 0.16805721446871758\nBatch loss: 0.3747188672423363\nBatch loss: 0.08007717318832874\nBatch loss: 0.22401047870516777\nBatch loss: 0.04454899113625288\nBatch loss: 0.39303161203861237\nBatch loss: 0.5327079445123672\nBatch loss: 0.381564162671566\nBatch loss: 0.15859974548220634\nBatch loss: 0.04683643579483032\nBatch loss: 0.29384613037109375\nBatch loss: 0.10575986467301846\nBatch loss: 0.3727954626083374\nBatch loss: 0.050843837670981884\nBatch loss: 0.1423346996307373\nBatch loss: 0.24289220571517944\nBatch loss: 0.4178003966808319\nBatch loss: 0.04224276635795832\nBatch loss: 0.11908531188964844\nBatch loss: 0.17101073637604713\nBatch loss: 0.14437079429626465\nBatch loss: 0.3868594765663147\nBatch loss: 0.07429790683090687\nBatch loss: 0.6670611351728439\nBatch loss: 0.0586590776219964\nBatch loss: 0.0204622745513916\nBatch loss: 0.08357733488082886\nBatch loss: 0.0225565442815423\nBatch loss: 0.21967435255646706\nBatch loss: 0.06871736142784357\nBatch loss: 0.12313759885728359\nBatch loss: 0.5215368419885635\nBatch loss: 0.17277730628848076\nBatch loss: 0.24730956181883812\nBatch loss: 0.018701255321502686\nBatch loss: 0.21053314208984375\nBatch loss: 0.23418003693223\nBatch loss: 0.1111307181417942\nBatch loss: 0.08025589399039745\nBatch loss: 0.05930447485297918\nBatch loss: 0.5200233682990074\nBatch loss: 0.45252371579408646\nBatch loss: 0.32725755125284195\nBatch loss: 0.4331011697649956\nBatch loss: 0.10534333996474743\nBatch loss: 0.4490094259381294\nBatch loss: 0.08257770910859108\nBatch loss: 0.03408962627872825\nBatch loss: 0.10222530923783779\nBatch loss: 0.4459301382303238\nBatch loss: 0.22612405940890312\nBatch loss: 0.19858738407492638\nBatch loss: 0.10574698448181152\nBatch loss: 0.007078170892782509\nBatch loss: 0.6403257697820663\nBatch loss: 0.026120305992662907\nBatch loss: 0.10851181112229824\nBatch loss: 0.5989501625299454\nBatch loss: 0.14603197574615479\nBatch loss: 0.0777094392105937\nBatch loss: 0.007117509958334267\nBatch loss: 0.04662817809730768\nBatch loss: 0.039442540146410465\nBatch loss: 0.39121415466070175\nBatch loss: 0.1409319695085287\nBatch loss: 0.05411982536315918\nBatch loss: 0.047240792773664\nBatch loss: 0.1356099545955658\nBatch loss: 0.04232344217598438\nBatch loss: 0.6464385986328125\nBatch loss: 0.152629679068923\nBatch loss: 0.035713077522814274\nBatch loss: 0.128420889377594\nBatch loss: 0.010300845606252551\nBatch loss: 0.04687231965363026\nBatch loss: 0.23551201447844505\nBatch loss: 0.007842541090212762\nBatch loss: 0.04511392209678888\nBatch loss: 0.03854906652122736\nBatch loss: 0.2084546722471714\nBatch loss: 0.017778754699975252\nBatch loss: 0.08247340098023415\nBatch loss: 0.016722738509997725\nBatch loss: 0.17293501645326614\nBatch loss: 0.008699774625711143\nBatch loss: 0.249877218157053\nBatch loss: 0.12897062115371227\nBatch loss: 0.20714199170470238\nBatch loss: 0.30301809310913086\nBatch loss: 0.06888842675834894\nBatch loss: 0.01226186752319336\nBatch loss: 0.04016530700027943\nBatch loss: 0.15117711387574673\nBatch loss: 0.18945811316370964\nBatch loss: 0.59853196144104\nBatch loss: 0.17182333394885063\nBatch loss: 0.26000356301665306\nBatch loss: 0.25874704122543335\nBatch loss: 0.15448546968400478\nBatch loss: 0.10790742002427578\nBatch loss: 0.15630656853318214\nBatch loss: 0.04671430680900812\nBatch loss: 0.025850057136267424\nBatch loss: 0.31551383435726166\nBatch loss: 0.02355217933654785\nBatch loss: 0.07966602221131325\nBatch loss: 0.24668747559189796\nBatch loss: 0.3330318257212639\nBatch loss: 0.3244064003229141\nBatch loss: 0.4295489937067032\nBatch loss: 0.10438809171319008\nBatch loss: 0.03868627594783902\nBatch loss: 0.16969114542007446\nBatch loss: 0.40285754948854446\nBatch loss: 0.07086700294166803\nBatch loss: 0.1462838053703308\nBatch loss: 0.007818699232302606\nBatch loss: 0.023407936096191406\nBatch loss: 0.07704329676926136\nBatch loss: 0.19377458840608597\nBatch loss: 0.16847563907504082\nBatch loss: 0.03428077790886164\nBatch loss: 0.10606884956359863\nBatch loss: 0.23648912087082863\nBatch loss: 0.10924643836915493\nBatch loss: 0.06612443830817938\nBatch loss: 0.294890645891428\nBatch loss: 0.2423321083188057\nBatch loss: 0.017711938126012683\nBatch loss: 0.07439774461090565\nBatch loss: 0.14361840672791004\nBatch loss: 0.14584279619157314\nBatch loss: 0.03221190068870783\nBatch loss: 0.02531415317207575\nBatch loss: 0.0738260755315423\nBatch loss: 0.37216223776340485\nBatch loss: 0.27855997905135155\nBatch loss: 0.20560240373015404\nEpoch 7/7\nBatch loss: 0.09735501371324062\nBatch loss: 0.4446566104888916\nBatch loss: 0.10116028599441051\nBatch loss: 0.05125754978507757\nBatch loss: 0.32059401273727417\nBatch loss: 0.20355194807052612\nBatch loss: 0.09691953659057617\nBatch loss: 0.2900812588632107\nBatch loss: 0.14493191614747047\nBatch loss: 0.26558656245470047\nBatch loss: 0.09336802177131176\nBatch loss: 0.1709732972085476\nBatch loss: 0.06733221001923084\nBatch loss: 0.39808858186006546\nBatch loss: 0.5076440051198006\nBatch loss: 0.16482580453157425\nBatch loss: 0.05685639567673206\nBatch loss: 0.17554044723510742\nBatch loss: 0.032465935219079256\nBatch loss: 0.12089872732758522\nBatch loss: 0.03866422222927213\nBatch loss: 0.16753697767853737\nBatch loss: 1.0327988117933273\nBatch loss: 0.25290394201874733\nBatch loss: 0.04357573576271534\nBatch loss: 0.08339190855622292\nBatch loss: 0.34939195960760117\nBatch loss: 0.05320477765053511\nBatch loss: 0.1541993673890829\nBatch loss: 0.18053054809570312\nBatch loss: 0.11121511459350586\nBatch loss: 0.3773920610547066\nBatch loss: 0.25439584627747536\nBatch loss: 0.23128777742385864\nBatch loss: 0.03369188401848078\nBatch loss: 0.2705760672688484\nBatch loss: 0.09355160407721996\nBatch loss: 0.03958582878112793\nBatch loss: 0.031307816971093416\nBatch loss: 0.36714445799589157\nBatch loss: 0.5125397071242332\nBatch loss: 0.07416820619255304\nBatch loss: 0.5601538345217705\nBatch loss: 0.11320752091705799\nBatch loss: 0.7584839314222336\nBatch loss: 0.10104918852448463\nBatch loss: 0.0077448488445952535\nBatch loss: 0.24310607463121414\nBatch loss: 0.0745772710070014\nBatch loss: 0.025257111992686987\nBatch loss: 0.23256057873368263\nBatch loss: 0.07133555598556995\nBatch loss: 0.46305179595947266\nBatch loss: 0.0841300468891859\nBatch loss: 0.10423230938613415\nBatch loss: 0.030805051792412996\nBatch loss: 0.010427475208416581\nBatch loss: 0.4639226198196411\nBatch loss: 0.018110752571374178\nBatch loss: 0.16634155064821243\nBatch loss: 0.6188011914491653\nBatch loss: 0.03162190318107605\nBatch loss: 0.0190579891204834\nBatch loss: 0.12955295853316784\nBatch loss: 0.014630347723141313\nBatch loss: 0.05872573237866163\nBatch loss: 0.8006548881530762\nBatch loss: 0.03687429474666715\nBatch loss: 0.10492801666259766\nBatch loss: 0.05926203913986683\nBatch loss: 1.2251565605401993\nBatch loss: 0.2322760783135891\nBatch loss: 0.006302684778347611\nBatch loss: 0.2994281053543091\nBatch loss: 0.007199823739938438\nBatch loss: 0.18530024215579033\nBatch loss: 0.4920123890042305\nBatch loss: 0.325392447412014\nBatch loss: 0.06384921260178089\nBatch loss: 0.04912275355309248\nBatch loss: 0.5823707580566406\nBatch loss: 0.03576773451641202\nBatch loss: 0.5354733392596245\nBatch loss: 0.403498075902462\nBatch loss: 0.08869707584381104\nBatch loss: 0.10581875219941139\nBatch loss: 0.02383756684139371\nBatch loss: 0.169657114893198\nBatch loss: 0.03248441265895963\nBatch loss: 0.15921199694275856\nBatch loss: 0.4603291302919388\nBatch loss: 0.46175040304660797\nBatch loss: 0.23891521617770195\nBatch loss: 0.0825972855091095\nBatch loss: 0.3273525834083557\nBatch loss: 0.2542424201965332\nBatch loss: 0.7152833789587021\nBatch loss: 0.8464433252811432\nBatch loss: 0.1552295684814453\nBatch loss: 0.1341510098427534\nBatch loss: 0.05890494678169489\nBatch loss: 0.11527840979397297\nBatch loss: 0.10392415337264538\nBatch loss: 0.012867868645116687\nBatch loss: 0.5356619507074356\nBatch loss: 0.04168081562966108\nBatch loss: 0.03697007894515991\nBatch loss: 0.04384275060147047\nBatch loss: 0.5757584795355797\nBatch loss: 0.042492388747632504\nBatch loss: 0.0636148452758789\nBatch loss: 0.12505287304520607\nBatch loss: 0.010878145694732666\nBatch loss: 0.0431747455149889\nBatch loss: 0.16011333093047142\nBatch loss: 0.18083153292536736\nBatch loss: 0.33272840082645416\nBatch loss: 0.2740379609167576\nBatch loss: 0.040771723724901676\nBatch loss: 0.1141851581633091\nBatch loss: 0.3927592560648918\nBatch loss: 0.8466644585132599\nBatch loss: 0.6917017698287964\nBatch loss: 0.08535690605640411\nBatch loss: 0.06864273454993963\nBatch loss: 0.31617145985364914\nBatch loss: 0.20341485738754272\nBatch loss: 0.13673201203346252\nBatch loss: 0.1956222765147686\nBatch loss: 0.04717600531876087\nBatch loss: 0.21046161651611328\nBatch loss: 0.008740127086639404\nBatch loss: 0.18260562792420387\nBatch loss: 0.7419013977050781\nBatch loss: 0.03254187060520053\nBatch loss: 0.14508175663650036\nBatch loss: 0.09536238387227058\nBatch loss: 0.691172182559967\nBatch loss: 0.07218289654701948\nBatch loss: 0.10399973951280117\nBatch loss: 0.297890305519104\nBatch loss: 0.16508067026734352\nBatch loss: 0.011945307487621903\nBatch loss: 0.17799688503146172\nBatch loss: 0.16859948635101318\nBatch loss: 0.05229872651398182\nBatch loss: 0.014275908470153809\nBatch loss: 0.024319172371178865\nBatch loss: 0.03827747656032443\nBatch loss: 0.17917728051543236\nBatch loss: 0.5767977237701416\nBatch loss: 0.003752157208509743\nBatch loss: 0.011294455034658313\nBatch loss: 0.7353071123361588\nBatch loss: 0.026628493797034025\nBatch loss: 0.28416698798537254\nBatch loss: 0.07884847931563854\nBatch loss: 0.12695997953414917\nBatch loss: 0.1082755345851183\nBatch loss: 0.08535156957805157\nBatch loss: 0.00749993312638253\nBatch loss: 0.3328746557235718\nBatch loss: 0.2017970196902752\nBatch loss: 0.5879185721278191\nBatch loss: 0.0729072978720069\nBatch loss: 0.15456080436706543\nBatch loss: 0.02337199402973056\nBatch loss: 0.6884991377592087\nBatch loss: 0.22671019658446312\nBatch loss: 0.028048516251146793\nBatch loss: 0.03453588578850031\nBatch loss: 0.36446571350097656\nBatch loss: 0.14892465434968472\nBatch loss: 0.05819905083626509\nBatch loss: 0.14382347464561462\nBatch loss: 0.004177063819952309\nBatch loss: 0.05206966307014227\nBatch loss: 0.2157018519937992\nBatch loss: 0.17156535759568214\nBatch loss: 0.05002349615097046\nBatch loss: 0.24671411141753197\nBatch loss: 0.076657235622406\nBatch loss: 0.04797554109245539\nBatch loss: 0.10002791881561279\nBatch loss: 0.08461154066026211\nBatch loss: 0.4665049538016319\nBatch loss: 0.024935738183557987\nBatch loss: 0.02554836915805936\nBatch loss: 0.07796710822731256\nBatch loss: 0.1617651991546154\nBatch loss: 0.13461164198815823\nBatch loss: 0.20697999745607376\nBatch loss: 0.02763474127277732\nBatch loss: 0.11137249879539013\nBatch loss: 0.04484699573367834\nBatch loss: 0.17101658508181572\nBatch loss: 0.2863888815045357\nBatch loss: 0.15748430043458939\nBatch loss: 0.20844662562012672\nBatch loss: 0.843857154250145\nBatch loss: 0.16487037762999535\nBatch loss: 0.06267911288887262\nBatch loss: 0.354040265083313\nBatch loss: 0.17061347141861916\nBatch loss: 0.6397410482168198\nBatch loss: 0.45924704521894455\nBatch loss: 0.10776317678391933\nBatch loss: 0.1987210474908352\nBatch loss: 0.03933647181838751\nBatch loss: 0.1846233569085598\nBatch loss: 0.011845857370644808\nBatch loss: 0.05384487099945545\nBatch loss: 0.028161646332591772\nBatch loss: 0.14539742842316628\nBatch loss: 0.03529811045154929\nBatch loss: 0.6764914095401764\nBatch loss: 0.22072827443480492\nBatch loss: 0.12136447243392467\nBatch loss: 0.191287100315094\nBatch loss: 0.09970158338546753\nBatch loss: 0.28326183557510376\nBatch loss: 0.03021979471668601\nBatch loss: 0.41962098330259323\nBatch loss: 0.014133871300145984\nBatch loss: 0.03961145877838135\nBatch loss: 1.1084627360105515\nBatch loss: 0.4864818975329399\nBatch loss: 0.017598987324163318\nBatch loss: 0.019463628996163607\nBatch loss: 0.0513231148943305\nBatch loss: 0.12266826815903187\nBatch loss: 0.0918194092810154\nBatch loss: 0.26137834414839745\nBatch loss: 0.044670761562883854\nBatch loss: 0.3007245995104313\nBatch loss: 0.04488331265747547\nBatch loss: 0.10406804271042347\nBatch loss: 0.26804303750395775\nBatch loss: 0.07780456449836493\nBatch loss: 0.24924254044890404\nBatch loss: 0.0493046035990119\nBatch loss: 0.2610199525952339\nBatch loss: 0.015521318418905139\nBatch loss: 0.2979031763970852\nBatch loss: 0.03585350466892123\nBatch loss: 0.0662186136469245\nBatch loss: 0.14067232608795166\nBatch loss: 0.07709969766438007\nBatch loss: 0.08174574933946133\nBatch loss: 0.014020234812051058\nBatch loss: 0.03234934760257602\nBatch loss: 0.12826752848923206\nBatch loss: 0.061150314286351204\nBatch loss: 0.05230754613876343\nBatch loss: 0.07626199629157782\nBatch loss: 0.04015590529888868\nBatch loss: 0.11100721545517445\nBatch loss: 0.24750862270593643\nBatch loss: 0.06862880196422338\nBatch loss: 0.16360152512788773\nBatch loss: 0.05735150072723627\nBatch loss: 0.11447715573012829\nBatch loss: 0.25440121069550514\nBatch loss: 0.4821530729532242\nBatch loss: 0.41129112243652344\nBatch loss: 0.14838898554444313\nBatch loss: 0.06058958359062672\nBatch loss: 0.07881897501647472\nBatch loss: 0.2856610529124737\nBatch loss: 0.037433027755469084\nBatch loss: 0.38451146334409714\nBatch loss: 0.13880240730941296\nBatch loss: 0.589185357093811\nBatch loss: 0.020367861725389957\nBatch loss: 0.014838814968243241\nBatch loss: 0.03119313856586814\nBatch loss: 0.18260443583130836\nBatch loss: 0.18982529640197754\nBatch loss: 0.26740431785583496\nBatch loss: 0.691673681139946\nBatch loss: 0.24753814563155174\nBatch loss: 0.25717174634337425\nBatch loss: 0.0997376348823309\nBatch loss: 0.1497071422636509\nBatch loss: 0.039030194748193026\nBatch loss: 0.30755579471588135\nBatch loss: 0.23644262924790382\nBatch loss: 0.19462300464510918\nBatch loss: 0.06379272323101759\nBatch loss: 0.2902836911380291\nBatch loss: 0.3577404096722603\nBatch loss: 0.061966716311872005\nBatch loss: 0.034649253357201815\nBatch loss: 0.2440916746854782\nBatch loss: 0.41890736669301987\nBatch loss: 0.33648230135440826\nBatch loss: 0.004993886104784906\nBatch loss: 0.17599409446120262\nBatch loss: 0.0781796034425497\nBatch loss: 0.11697745881974697\nBatch loss: 0.23823369294404984\nBatch loss: 0.11947092600166798\nBatch loss: 0.05043697543442249\nBatch loss: 0.17461085692048073\nBatch loss: 0.13422274962067604\nBatch loss: 0.6321831047534943\nBatch loss: 0.016007513040676713\nBatch loss: 0.002109587221639231\nBatch loss: 0.00944480299949646\nBatch loss: 0.5899521335959435\nBatch loss: 0.2224496565759182\nBatch loss: 0.15468120574951172\nBatch loss: 0.020343393553048372\nBatch loss: 0.34088820219039917\nBatch loss: 0.015531688695773482\nBatch loss: 0.2708147466182709\nBatch loss: 0.0032773317070677876\nBatch loss: 0.3791341930627823\nBatch loss: 0.7788490504026413\nBatch loss: 0.1737469993531704\nBatch loss: 0.07920361123979092\nBatch loss: 1.1473307758569717\nBatch loss: 0.03797215176746249\nBatch loss: 0.019173742039129138\nBatch loss: 0.13248897157609463\nBatch loss: 0.06612134166061878\nBatch loss: 0.03232354065403342\nBatch loss: 0.2089986763894558\nBatch loss: 0.027385533321648836\nBatch loss: 0.1659393310546875\nBatch loss: 0.14891022816300392\nBatch loss: 0.02116632414981723\nBatch loss: 0.12374169193208218\nBatch loss: 0.1625521294772625\nBatch loss: 0.2153642289340496\nBatch loss: 0.3205832839012146\nBatch loss: 0.13907432556152344\nBatch loss: 0.38841523230075836\nBatch loss: 0.41688431054353714\nBatch loss: 0.3643188998103142\nBatch loss: 0.03133574267849326\nBatch loss: 0.04198305308818817\nBatch loss: 0.08485299535095692\nBatch loss: 0.28809940442442894\nBatch loss: 0.2227284573018551\nBatch loss: 0.007177770021371543\nBatch loss: 0.15530726872384548\nBatch loss: 0.03718289779499173\nBatch loss: 0.6173296645283699\nBatch loss: 0.28755057603120804\nBatch loss: 0.1223224401473999\nBatch loss: 0.28170322999358177\nBatch loss: 0.024652243591845036\nBatch loss: 0.20392252132296562\nBatch loss: 0.10632353834807873\nBatch loss: 0.6265483051538467\nBatch loss: 0.26404786854982376\nBatch loss: 0.38413871079683304\nBatch loss: 0.0566632766276598\nBatch loss: 0.14472794719040394\nBatch loss: 0.10972428135573864\nBatch loss: 0.29117023572325706\nBatch loss: 0.19303489476442337\nBatch loss: 0.24210428819060326\nBatch loss: 0.26376819238066673\nBatch loss: 0.4226670414209366\nBatch loss: 0.21344447508454323\nBatch loss: 0.2997075393795967\nBatch loss: 0.8378827571868896\nBatch loss: 0.35513918846845627\nBatch loss: 0.8351337164640427\nBatch loss: 0.20021963864564896\nBatch loss: 0.03541278885677457\nBatch loss: 0.017424345714971423\nBatch loss: 0.07343387696892023\nBatch loss: 0.660865306854248\nBatch loss: 0.011683375341817737\nBatch loss: 0.1292145997285843\nBatch loss: 0.25998353958129883\nBatch loss: 0.16466571018099785\nBatch loss: 0.09095805697143078\nBatch loss: 0.6856662034988403\nBatch loss: 0.14125955291092396\nBatch loss: 0.18204545602202415\nBatch loss: 0.37639107555150986\nBatch loss: 0.014165372122079134\nBatch loss: 0.3103707730770111\nBatch loss: 0.26346301659941673\nBatch loss: 0.030828416347503662\nBatch loss: 0.663907527923584\nBatch loss: 0.46436071395874023\nBatch loss: 0.17252564430236816\nBatch loss: 1.1515247076749802\nBatch loss: 0.12804645113646984\nBatch loss: 0.20178629085421562\nBatch loss: 0.1767299883067608\nBatch loss: 0.050348639488220215\nBatch loss: 0.055855573154985905\nBatch loss: 0.031372071243822575\nBatch loss: 0.7073011249303818\nBatch loss: 0.0442810682579875\nBatch loss: 0.48983480781316757\nBatch loss: 0.2995579317212105\nBatch loss: 0.021333605982363224\nBatch loss: 0.28009116649627686\nBatch loss: 0.019913315773010254\nBatch loss: 0.5998524650931358\nBatch loss: 0.06080246064811945\nBatch loss: 0.1144206803292036\nBatch loss: 0.033030391205102205\nBatch loss: 0.0945176463574171\nBatch loss: 0.06476664450019598\nBatch loss: 0.029765963554382324\nBatch loss: 0.32358039170503616\nBatch loss: 0.20120447501540184\nBatch loss: 0.059152841567993164\nBatch loss: 0.37627633661031723\nBatch loss: 0.2458498440682888\nBatch loss: 0.07547748275101185\nBatch loss: 0.08978831581771374\nBatch loss: 0.5241502076387405\nBatch loss: 0.11187505908310413\nBatch loss: 0.029636502731591463\nBatch loss: 0.07220530416816473\nBatch loss: 0.015374303329735994\nBatch loss: 0.0747754005715251\nBatch loss: 0.050637125968933105\nBatch loss: 0.9060957282781601\nBatch loss: 0.041841985657811165\nBatch loss: 0.3295576572418213\nBatch loss: 0.16988400369882584\nBatch loss: 0.5866997316479683\nBatch loss: 0.2850581519305706\nBatch loss: 0.024499117862433195\nBatch loss: 0.23787451907992363\nBatch loss: 0.09246614761650562\nBatch loss: 0.5546968057751656\nBatch loss: 0.04296338651329279\nBatch loss: 0.5832163244485855\nBatch loss: 0.08996772579848766\nBatch loss: 0.014127359027042985\nBatch loss: 0.10828337632119656\nBatch loss: 0.06825065705925226\nBatch loss: 0.012380123371258378\nBatch loss: 0.027296782936900854\nBatch loss: 0.8709134161472321\nBatch loss: 0.04719001241028309\nBatch loss: 0.5405305698513985\nBatch loss: 0.3735918924212456\nBatch loss: 0.0856237392872572\nBatch loss: 0.40801089257001877\nBatch loss: 0.04246450960636139\nBatch loss: 0.4193076118826866\nBatch loss: 0.06025511305779219\nBatch loss: 0.16728078946471214\nBatch loss: 0.03090047976002097\nBatch loss: 0.08876276202499866\nBatch loss: 0.08392858318984509\nBatch loss: 0.03329378319904208\nBatch loss: 0.04738259594887495\nBatch loss: 0.03215831471607089\nBatch loss: 0.0890120305120945\nBatch loss: 0.36761946976184845\nBatch loss: 0.07085895631462336\nBatch loss: 0.030124725308269262\nBatch loss: 0.05018112249672413\nBatch loss: 0.017385364044457674\nBatch loss: 0.004166305006947368\nBatch loss: 0.01098895096220076\nBatch loss: 0.39270590990781784\nBatch loss: 0.0241034640930593\nBatch loss: 0.18634332343935966\nBatch loss: 0.12066907249391079\nBatch loss: 0.07979214191436768\nBatch loss: 0.10141176171600819\nBatch loss: 0.015430450439453125\nBatch loss: 0.06309497635811567\nBatch loss: 0.43057240545749664\nBatch loss: 0.37258733063936234\nBatch loss: 0.03663563868030906\nBatch loss: 0.030171393882483244\nBatch loss: 0.12692379765212536\nBatch loss: 0.18587296828627586\nBatch loss: 0.3524308279156685\nBatch loss: 0.25714194402098656\nBatch loss: 0.1327073574066162\nBatch loss: 0.059194983914494514\nBatch loss: 0.061476677656173706\nBatch loss: 0.3777189180254936\nBatch loss: 0.26493875309824944\nBatch loss: 0.3021254576742649\nBatch loss: 0.03890812397003174\nBatch loss: 0.10966802015900612\nBatch loss: 0.011940359836444259\nBatch loss: 0.05585014820098877\nBatch loss: 0.7835054397583008\nBatch loss: 0.02104508923366666\nBatch loss: 0.01391422818414867\nBatch loss: 0.9841546416282654\nBatch loss: 0.14345074072480202\nBatch loss: 0.21093511953949928\nBatch loss: 0.017141640419140458\nBatch loss: 0.02404934260994196\nBatch loss: 0.07275414653122425\nBatch loss: 0.07569527719169855\nBatch loss: 0.012150406837463379\nBatch loss: 0.14793622307479382\nBatch loss: 0.08958197198808193\nBatch loss: 0.03686249256134033\nBatch loss: 0.021708966232836246\nBatch loss: 0.07935047149658203\nBatch loss: 0.06630539894104004\nBatch loss: 0.020925283897668123\nBatch loss: 0.05153439939022064\nBatch loss: 0.038492323365062475\nBatch loss: 0.37472691386938095\nBatch loss: 0.25251200422644615\nBatch loss: 0.08089505136013031\nBatch loss: 0.05619013216346502\nBatch loss: 0.037670612800866365\nBatch loss: 0.04744303412735462\nBatch loss: 0.08612639270722866\nBatch loss: 0.2917180769145489\nBatch loss: 0.44669009745121\nBatch loss: 0.008454740163870156\nBatch loss: 0.024694502353668213\nBatch loss: 0.2148042991757393\nBatch loss: 0.04015231039375067\nBatch loss: 0.11600864119827747\nBatch loss: 0.027722360100597143\nBatch loss: 0.01069313264451921\nBatch loss: 0.06006515119224787\nBatch loss: 0.2526695467531681\nBatch loss: 0.35638023167848587\nBatch loss: 0.09755677543580532\nBatch loss: 0.05539047997444868\nBatch loss: 0.11794174090027809\nBatch loss: 0.007858991739340127\nBatch loss: 0.09871328249573708\nBatch loss: 0.18674273043870926\nBatch loss: 0.154928807169199\nBatch loss: 0.23904705420136452\nBatch loss: 0.21851325407624245\nBatch loss: 0.1349161844700575\nBatch loss: 0.06266054697334766\nBatch loss: 0.2604089491069317\nBatch loss: 0.7842911034822464\nBatch loss: 0.03601121949031949\nBatch loss: 0.41442085057497025\nBatch loss: 0.04562092013657093\nBatch loss: 0.30056556686758995\nBatch loss: 0.16730505973100662\nBatch loss: 0.451577864587307\nBatch loss: 0.04806637763977051\nBatch loss: 0.192315224558115\nBatch loss: 0.06621298380196095\nBatch loss: 0.020658851135522127\nBatch loss: 0.18463348969817162\nBatch loss: 0.3776068612933159\nBatch loss: 0.05356562323868275\nBatch loss: 0.002583622990641743\nBatch loss: 0.04997432231903076\nBatch loss: 0.06892567966133356\nBatch loss: 0.022077502217143774\nBatch loss: 0.7075860351324081\nBatch loss: 0.1633017137646675\nBatch loss: 0.5004177242517471\nBatch loss: 0.002473175700288266\nBatch loss: 0.06725836079567671\nBatch loss: 0.010141134262084961\nBatch loss: 0.7058633863925934\nBatch loss: 0.5024171993136406\nBatch loss: 0.24545563384890556\nBatch loss: 0.018016636604443192\nBatch loss: 0.14497566036880016\nBatch loss: 0.27474701404571533\nBatch loss: 0.00436377536971122\nBatch loss: 0.05943679716438055\nBatch loss: 0.0464365491643548\nBatch loss: 0.4254162311553955\nBatch loss: 0.19143296405673027\nBatch loss: 0.333959236741066\nBatch loss: 0.02856174251064658\nBatch loss: 0.018315926427021623\nBatch loss: 0.15768198296427727\nBatch loss: 0.3405023366212845\nBatch loss: 0.3587627410888672\nBatch loss: 0.044559123925864697\nBatch loss: 0.7283160835504532\nBatch loss: 0.015140116447582841\nBatch loss: 0.07261443417519331\nBatch loss: 0.1693110540509224\nBatch loss: 0.23133981972932816\nBatch loss: 0.014548421604558825\nBatch loss: 0.12633931823074818\nBatch loss: 0.0739707937464118\nBatch loss: 0.043715834617614746\nBatch loss: 0.04171836655586958\nBatch loss: 0.6103173643350601\nBatch loss: 0.09915495291352272\nBatch loss: 0.02355052623897791\nBatch loss: 0.04649353213608265\nBatch loss: 0.1682691089808941\nBatch loss: 0.17466623336076736\nBatch loss: 0.31918715685606003\nBatch loss: 0.3082550875842571\nBatch loss: 0.028036118019372225\nBatch loss: 0.0412055104970932\nBatch loss: 0.07214286830276251\nBatch loss: 0.43045856058597565\nBatch loss: 0.056603848934173584\nBatch loss: 0.011835247278213501\nBatch loss: 0.034358680713921785\nBatch loss: 0.06470859050750732\nBatch loss: 0.0314900279045105\nBatch loss: 0.3698117658495903\nBatch loss: 0.11533808894455433\nBatch loss: 0.3064439073204994\nBatch loss: 0.10757732205092907\nBatch loss: 1.1085672676563263\nBatch loss: 0.304233618080616\nBatch loss: 0.2914766035974026\nBatch loss: 0.3633322939276695\nBatch loss: 0.015351296169683337\nBatch loss: 0.1340928114950657\nBatch loss: 0.39716243743896484\nBatch loss: 0.1053242664784193\nBatch loss: 0.15777213498950005\nBatch loss: 0.09870195761322975\nBatch loss: 0.3210940584540367\nBatch loss: 0.147286057472229\nBatch loss: 0.12033927254378796\nBatch loss: 0.1019070204347372\nBatch loss: 0.3145590052008629\nBatch loss: 0.8608820289373398\nBatch loss: 0.027475834358483553\nBatch loss: 0.03926767501980066\nBatch loss: 0.03947711084038019\nBatch loss: 0.09192705154418945\nBatch loss: 0.09597063064575195\nBatch loss: 0.050810487009584904\nBatch loss: 0.14261228032410145\nBatch loss: 0.49192775040864944\nBatch loss: 0.10302061215043068\nBatch loss: 0.0834202766418457\nBatch loss: 0.023176968097686768\nBatch loss: 0.3135182335972786\nBatch loss: 0.25194525718688965\nBatch loss: 0.04136466886848211\nBatch loss: 0.07661014795303345\nBatch loss: 0.16656506806612015\nBatch loss: 0.033523321617394686\nBatch loss: 0.5050546675920486\nBatch loss: 0.11600404977798462\nBatch loss: 0.4175279662013054\nBatch loss: 0.046098590828478336\nBatch loss: 0.06947970483452082\nBatch loss: 0.5664593726396561\nBatch loss: 0.02109767636284232\nBatch loss: 0.0751146674156189\nBatch loss: 0.015362590784206986\nBatch loss: 0.18661284819245338\nBatch loss: 0.0463828444480896\nBatch loss: 0.40085863322019577\nBatch loss: 0.4394939914345741\nBatch loss: 0.01315343426540494\nBatch loss: 0.10029638186097145\nBatch loss: 0.4318244382739067\nBatch loss: 0.0733410706743598\nBatch loss: 0.04319894593209028\nBatch loss: 0.025386603083461523\nBatch loss: 0.16424035653471947\nBatch loss: 0.40535297244787216\nBatch loss: 0.3420190140604973\nBatch loss: 0.06281965877860785\nBatch loss: 0.34931566566228867\nBatch loss: 0.064334855414927\nBatch loss: 0.12386304326355457\nBatch loss: 0.19452814012765884\nBatch loss: 0.18023598939180374\nBatch loss: 0.3644637018442154\nBatch loss: 0.07817220874130726\nBatch loss: 0.08463454432785511\nBatch loss: 0.12283395044505596\nBatch loss: 0.26854610070586205\nBatch loss: 0.36155302077531815\nBatch loss: 0.5134842544794083\nBatch loss: 0.3581361845135689\nBatch loss: 0.15486872754991055\nBatch loss: 0.04801055882126093\nBatch loss: 0.11472845450043678\nBatch loss: 0.4871826246380806\nBatch loss: 0.08490330539643764\nBatch loss: 0.6877537816762924\nBatch loss: 0.06166237872093916\nBatch loss: 0.37561845034360886\nBatch loss: 0.39433836936950684\nBatch loss: 0.27741873636841774\nBatch loss: 0.009519815794192255\nBatch loss: 0.2101423777639866\nBatch loss: 0.015912235248833895\nBatch loss: 0.04003975074738264\nBatch loss: 0.07143634837120771\nBatch loss: 0.28250638395547867\nBatch loss: 0.27373170480132103\nBatch loss: 0.4768926650285721\nBatch loss: 0.11582920327782631\nBatch loss: 0.10560775175690651\nBatch loss: 0.32959800213575363\nBatch loss: 0.24058526381850243\nBatch loss: 0.010533392196521163\nBatch loss: 0.190871711820364\nBatch loss: 0.2496335469186306\nBatch loss: 0.03165674163028598\nBatch loss: 0.2807137928903103\nBatch loss: 0.1352744735777378\nBatch loss: 0.12180889025330544\nBatch loss: 0.37282198667526245\nBatch loss: 0.022297741379588842\nBatch loss: 0.1046832837164402\nBatch loss: 0.5286730453372002\nBatch loss: 0.32191336154937744\nBatch loss: 0.07984406314790249\nBatch loss: 0.4347574710845947\nBatch loss: 0.2670755796134472\nBatch loss: 0.7100894302129745\nBatch loss: 0.6154220178723335\nBatch loss: 0.07618966978043318\nBatch loss: 0.2838406525552273\nBatch loss: 0.4666248708963394\nBatch loss: 0.27264535427093506\nBatch loss: 0.3951040655374527\nBatch loss: 0.2154042385518551\nBatch loss: 0.048191994428634644\nBatch loss: 0.1321966666728258\nBatch loss: 0.06149220746010542\nBatch loss: 0.2539649046957493\nBatch loss: 0.7244852185249329\nBatch loss: 0.19122431054711342\nBatch loss: 0.02928517758846283\nBatch loss: 0.16539132222533226\nBatch loss: 0.0978631991893053\nBatch loss: 0.40815260261297226\nBatch loss: 0.5726292356848717\nBatch loss: 0.3977767378091812\nBatch loss: 0.11353871785104275\nBatch loss: 0.12496259994804859\nBatch loss: 0.3240940719842911\nBatch loss: 0.08908534422516823\nBatch loss: 0.2321823313832283\nBatch loss: 0.47863151878118515\nBatch loss: 0.4565468057990074\nBatch loss: 0.05154457874596119\nBatch loss: 0.24070549756288528\nBatch loss: 0.11085730977356434\nBatch loss: 0.13996613211929798\nBatch loss: 0.1613488793373108\nBatch loss: 0.013611257309094071\nBatch loss: 0.3527567908167839\nBatch loss: 0.4209798574447632\nBatch loss: 0.3356682136654854\nBatch loss: 0.15236938372254372\nBatch loss: 0.04169559571892023\nBatch loss: 0.23015690967440605\nBatch loss: 0.31379740685224533\nBatch loss: 0.3705477714538574\nBatch loss: 0.22367151454091072\nBatch loss: 0.009329483145847917\nBatch loss: 0.4213995859026909\nBatch loss: 0.19085247069597244\nBatch loss: 0.013855159049853683\nBatch loss: 0.3550019860267639\nBatch loss: 0.3765420988202095\nBatch loss: 0.16055041924118996\nBatch loss: 0.13275617733597755\nBatch loss: 0.10384631343185902\nBatch loss: 0.4287717491388321\nBatch loss: 0.06307101342827082\nBatch loss: 0.5129431560635567\nBatch loss: 0.8354965597391129\nBatch loss: 0.38183070719242096\nBatch loss: 0.03547884523868561\nBatch loss: 0.455278642475605\nBatch loss: 0.27401644736528397\nBatch loss: 0.06948256399482489\nBatch loss: 0.1690840721130371\nBatch loss: 0.4779789596796036\nBatch loss: 0.21588901057839394\nBatch loss: 0.4719613119959831\nBatch loss: 0.13320088386535645\nBatch loss: 0.03662261413410306\nBatch loss: 0.07715284824371338\nBatch loss: 0.9271200746297836\nBatch loss: 0.1775066927075386\nBatch loss: 0.24433864280581474\nBatch loss: 0.4402273893356323\nBatch loss: 0.4385727643966675\nBatch loss: 0.2503986470401287\nBatch loss: 0.029423385858535767\nBatch loss: 0.008492946508340538\nBatch loss: 0.13526118360459805\nBatch loss: 0.1243733149021864\nBatch loss: 0.2219725213944912\nBatch loss: 0.017406254773959517\nBatch loss: 0.04029321949928999\nBatch loss: 0.034390806686133146\nBatch loss: 0.10823654942214489\nBatch loss: 0.11809182353317738\nBatch loss: 0.40512870997190475\nBatch loss: 0.10990905575454235\nBatch loss: 0.4691355302929878\nBatch loss: 0.07786639034748077\nBatch loss: 0.20497364923357964\nBatch loss: 0.20282184705138206\nBatch loss: 0.12737298384308815\nBatch loss: 0.4009610414505005\nBatch loss: 0.1632903330028057\nBatch loss: 0.1105356216430664\nBatch loss: 0.11485690250992775\nBatch loss: 0.01611596322618425\nBatch loss: 0.2060997486114502\nBatch loss: 0.24879395961761475\nBatch loss: 0.6584195047616959\nBatch loss: 0.004065901157446206\nBatch loss: 0.42406607419252396\nBatch loss: 0.21633049473166466\nBatch loss: 0.43950144201517105\nBatch loss: 0.14115489087998867\nBatch loss: 0.016376794083043933\nBatch loss: 0.04399466793984175\nBatch loss: 0.25576818734407425\nBatch loss: 0.09503317065536976\nBatch loss: 0.25506392121315\nBatch loss: 0.24475861340761185\nBatch loss: 0.12472355738282204\nBatch loss: 0.11321902275085449\nBatch loss: 0.0752899656072259\nBatch loss: 0.32286424189805984\nBatch loss: 0.04930284805595875\nBatch loss: 0.6333665549755096\nBatch loss: 0.04929936025291681\nBatch loss: 0.3087143041193485\nBatch loss: 0.5591843277215958\nBatch loss: 0.48630453646183014\nBatch loss: 0.13388407416641712\nBatch loss: 0.2101387456059456\nBatch loss: 0.5368482694029808\nBatch loss: 0.4830418527126312\nBatch loss: 0.07057231850922108\nBatch loss: 0.22605884820222855\nBatch loss: 0.3030895069241524\nBatch loss: 0.37953685969114304\nBatch loss: 0.728665217757225\nBatch loss: 0.10681855492293835\nBatch loss: 0.012132168048992753\nBatch loss: 0.07328939624130726\nBatch loss: 1.046334058046341\nBatch loss: 0.28953302651643753\nBatch loss: 0.12119913473725319\nBatch loss: 0.11752021498978138\nBatch loss: 0.06777638103812933\nBatch loss: 0.3184392675757408\nBatch loss: 0.035316317807883024\nBatch loss: 0.1865929178893566\nBatch loss: 0.14439010992646217\nBatch loss: 0.21892977878451347\nBatch loss: 0.01018524169921875\nBatch loss: 0.03654113505035639\nBatch loss: 0.16874754801392555\nBatch loss: 0.41498612612485886\nBatch loss: 0.2668139897286892\nBatch loss: 0.2895590476691723\nBatch loss: 0.0966945756226778\nBatch loss: 0.03205385757610202\nBatch loss: 0.09768902324140072\nBatch loss: 0.01137900399044156\nBatch loss: 0.21309375762939453\nBatch loss: 0.08885401301085949\nBatch loss: 0.07320177741348743\nBatch loss: 0.055305599234998226\nBatch loss: 0.35337604582309723\nBatch loss: 0.26021236553788185\nBatch loss: 0.44123362749814987\nBatch loss: 0.2202465571463108\nBatch loss: 0.1740717887878418\nBatch loss: 0.11821562424302101\nBatch loss: 0.3677697107195854\nBatch loss: 0.25329941883683205\nBatch loss: 0.08988047018647194\nBatch loss: 0.06278824992477894\nBatch loss: 0.05972433369606733\nBatch loss: 0.012365132570266724\nBatch loss: 0.05114315543323755\nBatch loss: 0.10489935055375099\nBatch loss: 0.07377623114734888\nBatch loss: 0.5228817090392113\nBatch loss: 0.02450656844303012\nBatch loss: 0.19933968782424927\nBatch loss: 0.23370305076241493\nBatch loss: 0.24699652567505836\nBatch loss: 0.2798997238278389\nBatch loss: 0.13696384616196156\nBatch loss: 0.24675661697983742\nBatch loss: 0.45764800161123276\nBatch loss: 0.024838268291205168\nBatch loss: 0.05206799600273371\nBatch loss: 0.1931104063987732\nBatch loss: 0.13156622648239136\nBatch loss: 0.5834486708045006\nBatch loss: 0.21296292543411255\nBatch loss: 0.17409492284059525\nBatch loss: 0.13903397135436535\nBatch loss: 0.16719436272978783\nBatch loss: 0.0984578114002943\nBatch loss: 0.2645714394748211\nBatch loss: 0.1132669486105442\nBatch loss: 0.19977297633886337\nBatch loss: 0.10613328777253628\nBatch loss: 0.3220854327082634\nBatch loss: 0.27673864737153053\nBatch loss: 0.2054440788924694\nBatch loss: 0.41950203478336334\nBatch loss: 1.1067316681146622\nBatch loss: 0.11718076653778553\nBatch loss: 0.35449691116809845\nBatch loss: 0.43345727026462555\nBatch loss: 0.13854670338332653\nBatch loss: 0.22325480356812477\nBatch loss: 0.07782554719597101\nBatch loss: 0.12847447767853737\nBatch loss: 0.1744384877383709\nBatch loss: 0.21606458351016045\nBatch loss: 0.06566023919731379\nBatch loss: 0.20486950874328613\nBatch loss: 0.17271805554628372\nBatch loss: 0.061529637314379215\nBatch loss: 0.052418592385947704\nBatch loss: 0.19562071189284325\nBatch loss: 0.5365076288580894\nBatch loss: 0.4057624563574791\nBatch loss: 0.07710182573646307\nBatch loss: 0.44407714158296585\nBatch loss: 0.02999687334522605\nBatch loss: 0.012141227489337325\nBatch loss: 0.5195420980453491\nBatch loss: 0.0161773560103029\nBatch loss: 0.13664639554917812\nBatch loss: 0.22351885214447975\nBatch loss: 0.8866456896066666\nBatch loss: 0.09475278668105602\nBatch loss: 0.3033638000488281\nBatch loss: 0.3202543407678604\nBatch loss: 0.4459715634584427\nBatch loss: 0.17131328582763672\nBatch loss: 0.16576742753386497\nBatch loss: 0.3168800100684166\nBatch loss: 0.1805751956999302\nBatch loss: 0.01700031803920865\nBatch loss: 0.06769347470253706\nBatch loss: 0.7374093681573868\nBatch loss: 0.07361072581261396\nBatch loss: 0.18051696941256523\nBatch loss: 0.04366457462310791\nBatch loss: 0.01857382128946483\nBatch loss: 0.2816184237599373\nBatch loss: 0.12665987014770508\nBatch loss: 0.06643984001129866\nBatch loss: 0.07226336281746626\nBatch loss: 0.2618107199668884\nBatch loss: 0.0752458581700921\nBatch loss: 0.41510559618473053\nBatch loss: 0.4721241816878319\nBatch loss: 0.2736654318869114\nBatch loss: 0.08234354667365551\nBatch loss: 0.21779274567961693\nBatch loss: 0.1960185170173645\nBatch loss: 0.27912139892578125\nBatch loss: 0.16628418117761612\nBatch loss: 0.04432201385498047\nBatch loss: 0.23841679096221924\nBatch loss: 0.03434848738834262\nBatch loss: 0.008371055009774864\nBatch loss: 0.4373207315802574\nBatch loss: 0.14462590217590332\nBatch loss: 0.46895477920770645\nBatch loss: 0.19503910094499588\nBatch loss: 0.05895256996154785\nBatch loss: 0.06542289163917303\nBatch loss: 0.22725796326994896\nBatch loss: 0.13542848639190197\nBatch loss: 0.00672930502332747\nBatch loss: 0.3567550331354141\nBatch loss: 0.022385239135473967\nBatch loss: 0.02168369246646762\nBatch loss: 0.5556262657046318\nBatch loss: 0.11127305217087269\nBatch loss: 0.021914541721343994\nBatch loss: 0.012283743126317859\nBatch loss: 0.36449480801820755\nBatch loss: 0.6243931129574776\nBatch loss: 0.3518446162343025\nBatch loss: 0.10307395830750465\nTraining completed.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install scikit-learn\nfrom sklearn.model_selection import train_test_split\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:12:43.460401Z","iopub.execute_input":"2024-10-13T09:12:43.460838Z","iopub.status.idle":"2024-10-13T09:12:57.164994Z","shell.execute_reply.started":"2024-10-13T09:12:43.460800Z","shell.execute_reply":"2024-10-13T09:12:57.164084Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\ndef test(model, test_dataloader, device):\n    # Put the model in evaluation mode\n    model.eval()\n    \n    total_loss = 0\n    predictions = []\n    true_labels = []\n    \n    # No need to track gradients for evaluation\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch[0].to(device)\n            attention_mask = batch[1].to(device)\n            labels = batch[2].to(device)\n\n            # Forward pass (inference)\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            logits = outputs.logits\n            \n            # Accumulate total loss\n            total_loss += loss.item()\n            \n            # Get predictions (logits are the raw model outputs before activation)\n            preds = torch.argmax(logits, dim=1).cpu().numpy()\n            label_ids = labels.cpu().numpy()\n\n            # Collect predictions and true labels\n            predictions.extend(preds)\n            true_labels.extend(label_ids)\n    \n    # Calculate average loss\n    avg_loss = total_loss / len(test_dataloader)\n    \n    # Calculate accuracy, precision, recall, and F1-score\n    accuracy = accuracy_score(true_labels, predictions)\n    precision = precision_score(true_labels, predictions)\n    recall = recall_score(true_labels, predictions)\n    f1 = f1_score(true_labels, predictions)\n    \n    # Create confusion matrix\n    conf_matrix = confusion_matrix(true_labels, predictions)\n\n    # Print results\n    print(f\"Test Loss: {avg_loss:.4f}\")\n    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"Recall: {recall:.2f}\")\n    print(f\"F1-Score: {f1:.2f}\")\n    print(\"Confusion Matrix:\")\n    print(conf_matrix)\n\n    return avg_loss, accuracy, precision, recall, f1, conf_matrix\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T11:13:43.224431Z","iopub.execute_input":"2024-10-13T11:13:43.225098Z","iopub.status.idle":"2024-10-13T11:13:43.239466Z","shell.execute_reply.started":"2024-10-13T11:13:43.225054Z","shell.execute_reply":"2024-10-13T11:13:43.238148Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#Epoch=3\n\navg_loss, accuracy, precision, recall, f1, conf_matrix = test(model, test_dataloader, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:50:07.395107Z","iopub.execute_input":"2024-10-13T09:50:07.396018Z","iopub.status.idle":"2024-10-13T09:52:04.575797Z","shell.execute_reply.started":"2024-10-13T09:50:07.395974Z","shell.execute_reply":"2024-10-13T09:52:04.574908Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Test Loss: 0.2579\nAccuracy: 88.59%\nPrecision: 0.94\nRecall: 0.88\nF1-Score: 0.90\nConfusion Matrix:\n[[1188  128]\n [ 264 1856]]\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_confusion_matrix(conf_matrix):\n    plt.figure(figsize=(6,6))\n    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n                xticklabels=['Not Useful', 'Useful'], \n                yticklabels=['Not Useful', 'Useful'])\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\n# After testing, you can call the plot function\nplot_confusion_matrix(conf_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:27:53.825067Z","iopub.execute_input":"2024-10-13T09:27:53.825465Z","iopub.status.idle":"2024-10-13T09:27:54.577102Z","shell.execute_reply.started":"2024-10-13T09:27:53.825430Z","shell.execute_reply":"2024-10-13T09:27:54.575894Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 600x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"#Epoch=7\n\navg_loss, accuracy, precision, recall, f1, conf_matrix = test(model, test_dataloader, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T11:25:51.623107Z","iopub.execute_input":"2024-10-13T11:25:51.623529Z","iopub.status.idle":"2024-10-13T11:27:46.020395Z","shell.execute_reply.started":"2024-10-13T11:25:51.623494Z","shell.execute_reply":"2024-10-13T11:27:46.019486Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Test Loss: 0.1948\nAccuracy: 91.76%\nPrecision: 0.90\nRecall: 0.98\nF1-Score: 0.94\nConfusion Matrix:\n[[1081  235]\n [  48 2072]]\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_confusion_matrix(conf_matrix):\n    plt.figure(figsize=(6,6))\n    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n                xticklabels=['Not Useful', 'Useful'], \n                yticklabels=['Not Useful', 'Useful'])\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\n# After testing, you can call the plot function\nplot_confusion_matrix(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T11:29:12.047794Z","iopub.execute_input":"2024-10-13T11:29:12.048166Z","iopub.status.idle":"2024-10-13T11:29:12.171587Z","shell.execute_reply.started":"2024-10-13T11:29:12.048130Z","shell.execute_reply":"2024-10-13T11:29:12.170689Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 600x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# Load the fine-tuned model and tokenizer\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Example of a single input (comment + code context)\ncomment = \"#test 45\"\ncode_context = \"\"\n\n# Tokenize the input\ninput_text = f\"code: {code_context} comment: {comment}\"\ninputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512).to(device)\n\n# Perform inference (disable gradient calculations)\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Get the predicted class (logits to probabilities)\nlogits = outputs.logits\npredicted_class = torch.argmax(logits, dim=1).item()\n\n# Interpret the output\nclass_mapping = {0: \"Not Useful\", 1: \"Useful\"}  # Map the predicted label to the class name\nprint(f\"Predicted class: {class_mapping[predicted_class]}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:21:01.305538Z","iopub.execute_input":"2024-10-13T09:21:01.306469Z","iopub.status.idle":"2024-10-13T09:21:01.464493Z","shell.execute_reply.started":"2024-10-13T09:21:01.306425Z","shell.execute_reply":"2024-10-13T09:21:01.463614Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Predicted class: Useful\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T06:07:11.993019Z","iopub.execute_input":"2024-10-13T06:07:11.993770Z","iopub.status.idle":"2024-10-13T06:07:11.999114Z","shell.execute_reply.started":"2024-10-13T06:07:11.993718Z","shell.execute_reply":"2024-10-13T06:07:11.998067Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}